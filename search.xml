<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[blog从wordpress迁移到github pages&hexo]]></title>
      <url>%2Fwordpress-to-hexo%2F</url>
      <content type="text"><![CDATA[写在开头将博客从wordpress系统迁移到github pages+hexo了。 相比于wordpress之类的自己买主机建站，对于我个人来讲，采用github+hexo类的静态化方案有如下几个好处：a) 免费，不限流量b) git版本管理，方便追查修改记录c) 把精力用来写文章(像ci代码一样方便)，不用再做网管了d) 学习下markdown之类的写作语法e) 了解下无后端的前端解决方案 缺点：a) 折腾。除了markdown语法，各种各样的动态功能往往需要借助外部服务。不过这也是优点。刚好可以熟悉下业内对应的解决方案。b) 静态网页。动态功能需要借助外部服务。c) 需要遍历生成。多一步静态化生成的步骤。网站越大，生成时间也越长。 严格来讲，对于一般的博客，blog数量有限，c不是问题。a与b换个角度也是优点。 实践之路入门学习参考1)。一开始用的Jekyll。后来迁移到了hexo。githubpages+hexo迁移。参考2)。相比于Jekyll，hexo似乎更简单。 hexo日常使用命令url格式hexo的默认格式如下：1permalink: :year/:month/:day/:title/ 按照我以前wordpress的格式进行了调整：1permalink: :title/ 文章阅读数统计参考3)。存储用了leanloud。需要先注册。如果用的next主题，很简单。在leancloud注册后，将_config.xml中的开关打开(搜索leancloud)即可。如果其他主题，需要参考3)所示的链接加入脚本代码。对脚本代码进行了一些调整：将访问数存储至leancloud但不显示，对存储至leancloud的字段也进行了些许修改。但原博客的pv就没办法再保留了。 图片markdown的图片引入语法为:”![title](http://a.com/123.jpeg)&quot;。图片加速。参考4)。存储用的七牛云。可以用我的这个七牛邀请链接注册。注册后直接引用其生成的链接地址即可。批量工具可以用qshell。简单用法如下。12345678$ qshell account ak sk$ qshell qupload qshell.config$ cat qshell.config&#123; "src_dir" : "source/cdn", "bucket" : "blog", "rescan_local" : true&#125; 先设置ak/sk，然后用quload批量上传。其中src_dir为本地目录， bucket为qiniu上的bucket。 wordpress迁移如果顺利的话，似乎几个步骤就搞定了。123$ npm install hexo-migrator-wordpress --save$ --&gt; WordPress 仪表盘中导出数据(“Tools” → “Export” → “WordPress”)$ hexo migrate wordpress &lt;source&gt; 非常遗憾。我的blog几乎有大半导不成功。只能手工搞了。另外像评论、阅读数等等统计也就没有了，只能人工迁移了。 重定向与首页不显示部分链接：不想采用原来wordpress的链接展示格式，但也不希望原来的链接变成死链。因此需要重定向原来的链接。123cat source/leveldb/leveldb-8-snapshot.html&lt;meta http-equiv="refresh" content="0; url=http://www.petermao.com/leveldb-8-snapshot/"&gt;&lt;link rel="canonical" href="http://www.petermao.com/leveldb-8-snapshot/" /&gt; 原来的链接访问”/leveldb/leveldb-8-snapshot.html”会自动重定向到”/leveldb-8-snapshot/“如果该类转化较多，写个脚本自动化之。 如果不希望指定的post在首页显示，则需要修改index.swig。然后每篇blog添加对应的标签值。1234# vim themes/next/layout/index.swig &#123;% if post.visible !== 'hide' %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endif %&#125; 搜索重点了解下其搜索解决方案。local search:原理是通过hexo-generator-search插件在本地生成一个文件，搜索的时候检索这个文件。12345678$ npm install hexo-generator-search --save$ npm install hexo-generator-searchdb --save$ vim themes/next/_config.ymlsearch: path: search.xml field: post format: html limit: 10000 algolia: 先注册(非常简单，New Index即可)，然后按如下方式修改本地配置。version为5.1.0，基本只需要简单改下配置即可，但还是有个bug导致被坑了良久。1234567891011121314151617181920212223242526$ npm install hexo-algolia --save$ vim _config.ymlalgolia: applicationID: 'yours' apiKey: 'yours' adminApiKey: 'yours' indexName: 'blog' chunkSiz: 5000$ vim themes/next/_config.yml algolia_search: enable: true hits: per_page: 10 labels: input_placeholder: 输入关键词 hits_empty: "没有找到与 $&#123;query&#125; 相关的内容" hits_stats: "$&#123;hits&#125;条相关记录，共耗时 $&#123;time&#125; ms"$ vim themes/next/layout/_partials/header.swig (note:我这版本已经有了, 不需要改) &#123;% elseif config.search || theme.algolia_search.enable %&#125; &lt;a href="javascript:;" class="popup-trigger"&gt;$ vim node_modules/hexo-algolia/lib/command.js var storedPost = _.pick(data, ['title', 'date', 'slug', 'content', 'path', 'excerpt', 'permalink']);$ hexo algolia Swiftype、 微搜索 打赏现在支付宝与微信支付都通过扫码就能实现支付，所以原理就是将你的支付账号用图片加载进来。next现在已经集成了该功能。但需要自行打开相关开关。12345$ ls themes/next/layout/_macro/reward.swig$ tail -n 3 themes/next/_config.ymlreward_comment: Enjoy it ? Donate me ! 欣赏此文？求鼓励，求支持！alipay: http://static.ixirong.com/pic/donate/alipay1.webpwechatpay: http://static.ixirong.com/pic/donate/wechat.png TODO整理wordpres的blog时的一大发现就是草稿里累计了好多，需要加把劲完善完善正式发布出来。``` 致谢与参考1 搭建一个免费的，无限流量的Blog—-github Pages和Jekyll入门 2 Jekyll迁移到Hexo搭建个人博客 3 Hexo Next主题添加文章阅读量统计功能 4 图片存储使用七牛为Hexo存储图片qshell 5 搜索hexo+next添加algolia搜索next algolia bug 5 next打赏功能 6 next中文帮助文档 7 hexo中文帮助文档]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释8--snapshot机制]]></title>
      <url>%2Fleveldb-8-snapshot%2F</url>
      <content type="text"><![CDATA[snapshot，也就是快照机制的作用，就是使得读操作不受写操作影响。其巧妙的使用了SequenceNumber来实现这一机制。 上一节中，我们知道，SequenceNumber使得对写入的任何key在内部存储时都会附加一个递增的SequenceNumber。而在leveldb内部，一次snapshot也对应一个全局唯一的SequenceNumber。因此，在调用GetSnapshot获取snapshot时，leveldb所做的仅仅是生成一个SequenceNumber。db内部使用双向循环链表保存先后生成的snapshot。获取快照GetSnapshot会将新snapshot加入链表，而释放快照ReleaseSnapshot则将snapshot从链表中删除。DBImpl类中的snapshots_成员就是这个链表。这部分代码主要在snapshot.h与db_impl.cc/.h中。代码很简单，内部是一个双向循环链表的封装。 const Snapshot* DBImpl::GetSnapshot() { MutexLock l(&amp;mutex); return snapshots.New(versions_-&gt;LastSequence());}snapshot机制主要会影响指定的读操作与所有后续的压缩操作，直到该snapshot释放。 从前面的使用接口章节中，我们知道，当读操作Get函数指定snapshot选项时，从snapshot生成至Get函数调用的这段时间里的写入操作对该次读没有影响。当指定snapshot时，Get函数会使用这个snapshot的sequencenumber+输入的key来构造要查找的LookupKey（没有指定snapshot，则使用当前已使用的最大sequencenumber）。而对于要查找的数据结构memtable、immunable memtable与sstable来讲，其内部存储的”key”都是带sequencenumber的（上一章节的分析），其最终查找到的”key”会满足key == 输入的key，而sequencenumber&lt;=Get函数传入的sequencenumber。这样，对于那些key相同但sequencenumber比较大（也就是后续针对同一个key做写入操作）的slice，则查找不到，自动的被屏蔽掉了。关于读操作Get的详细细节，我们在后续章节进行分析。 Status DBImpl::Get(const ReadOptions&amp; options, const Slice&amp; key, std::string value) { Status s; MutexLock l(&amp;mutex_); SequenceNumber snapshot; if (options.snapshot != NULL) { snapshot = reinterpret_cast&lt;const SnapshotImpl&gt;(options.snapshot)-&gt;number; } else { snapshot = versions-&gt;LastSequence(); } MemTable mem = mem_; MemTable imm = imm; Version* current = versions-&gt;current(); mem-&gt;Ref(); if (imm != NULL) imm-&gt;Ref(); current-&gt;Ref(); { mutex_.Unlock(); // mem与imm是带sequence_number以保证不会查到新数据 // 查找顺序依次是：memtable、immutable memtable、sstable文件 // First look in the memtable, then in the immutable memtable (if any). LookupKey lkey(key, snapshot); if (mem-&gt;Get(lkey, value, &amp;s)) { // Done } else if (imm != NULL &amp;&amp; imm-&gt;Get(lkey, value, &amp;s)) { // Done } else { s = current-&gt;Get(options, lkey, value, &amp;stats); have_stat_update = true; } --- }leveldb的snapshot机制不会在snapshot生成时直接影响系统，而是通过影响压缩操作来影响。比如redis的快照机制，其生成快照时，相当于dump整个内存中的db。通过前面的Get分析，我们知道，因为SequenceNumber对查找的影响，使得后续的写是完全透明的。但compact操作会将写入的值与已有的值进行合并。这样，后续写入的值可能会覆盖前面已写过的值或者后续的删除操作删除了该key。实际上，在进行major compact时（minor compact只是将immunable memtable转储至1个level 0的sstable文件），会检查这些(key,value)的sequencenumber。当有相同key的多个操作序列时，正常情况下，只会按操作第一个序列（也就是最新的），后续的直接drop。但snapshot机制，会使得对于这些多个操作序列的除第一个之外的操作，只有sequencenumber比系统中最小的snapshot还小的才丢弃（实际细节比这个还复杂点）。这样，这些操作序列都会被保存下来用于后续的snapshot操作。这部分代码主要在DBImpl::DoCompactionWork函数中，更多细节有待我们讲述压缩机制时再来分析。 Status DBImpl::DoCompactionWork(CompactionState* compact) { if (snapshots_.empty()) { compact-&gt;smallestsnapshot = versions-&gt;LastSequence(); } else { compact-&gt;smallestsnapshot = snapshots.oldest()-&gt;number_; } Iterator* input = versions_-&gt;MakeInputIterator(compact-&gt;compaction); input-&gt;SeekToFirst(); Status status; ParsedInternalKey ikey; std::string current_user_key; bool has_current_user_key = false; SequenceNumber last_sequence_for_key = kMaxSequenceNumber; // 遍历输入 for (; input-&gt;Valid() &amp;&amp; !shuttingdown.Acquire_Load(); ) { // Prioritize immutable compaction work if (has_imm_.NoBarrier_Load() != NULL) { --- } Slice key = input-&gt;key(); if (compact-&gt;compaction-&gt;ShouldStopBefore(key) &amp;&amp; compact-&gt;builder != NULL) { // 输出新文件 status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } // Handle key/value, add to state, etc. bool drop = false; // 因为有delete、多次插入，同一个key可能出现多次 if (!ParseInternalKey(key, &amp;ikey)) { // Do not hide error keys current_user_key.clear(); has_current_user_key = false; last_sequence_for_key = kMaxSequenceNumber; } else { if (!has_current_user_key || // 注意这里的比较是直接比较user_key，也就是不带sequencenumber user_comparator()-&gt;Compare(ikey.user_key, Slice(current_user_key)) != 0) { // First occurrence of this user key current_user_key.assign(ikey.user_key.data(), ikey.user_key.size()); has_current_user_key = true; last_sequence_for_key = kMaxSequenceNumber; } if (last_sequence_for_key &lt;= compact-&gt;smallest_snapshot) { // Hidden by an newer entry for same user key // 多个key，后续的是更老的，直接drop，保留第一个 drop = true; // (A) } else if (ikey.type == kTypeDeletion &amp;&amp; ikey.sequence &lt;= compact-&gt;smallest_snapshot &amp;&amp; compact-&gt;compaction-&gt;IsBaseLevelForKey(ikey.user_key)) { // For this user key: // (1) there is no data in higher levels // (2) data in lower levels will have larger sequence numbers // (3) data in layers that are being compacted here and have // smaller sequence numbers will be dropped in the next // few iterations of this loop (by rule (A) above). // Therefore this deletion marker is obsolete and can be dropped. drop = true; } last_sequence_for_key = ikey.sequence; } --- 由于snapshot对压缩机制有影响，对snapshot后续插入的多个值都不会压缩。因此在使用时，用完后要尽可能快释放。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释7–key与value]]></title>
      <url>%2Fleveldb-7-key-value%2F</url>
      <content type="text"><![CDATA[作为一个kv的系统，key的存储至关重要。在leveldb中，主要涉及到如下几个key，user_key、InternalKey与LookupKey(memtable_key)。其关系构成如下图。user_key就是用户输入的key，而InternalKey在user_key的基础上封装了sequence_num+type。sequence_num是一个全局递增的序列号，每一次Put操作都会递增。这样，不同时间的写入操作会得到不一样的sequence_num。前面章节中提到的sstable单条record中的key，其内部其实就是一个InternalKey。sequence_num主要跟snapshot机制与version机制相关，对压缩会产生一定影响。这些我们在后续章节分析。根据type字段，可以获知本次写入操作是写还是删除（也就是说删除是一种特殊的写）。而LookupKey/memtable_key用于在memtable中，多了一个长度字段。代码主要在dbformat.cc/.h中。[caption id=”attachment_797” align=”alignnone” width=”475” caption=”leveldb lookup key”][/caption] static uint64_t PackSequenceAndType(uint64_t seq, ValueType t) { assert(seq &lt;= kMaxSequenceNumber); assert(t &lt;= kValueTypeForSeek); return (seq &lt;&lt; \8) | t;}skiplist中的单个节点不仅存储了Key，也存储了value。格式如下图。尽管单个节点的开头部分是一个LookupKey，但其内部比较时，还是使用的InternalKey。也就是说，比较时，先使用InternalKey内部的user_key进行比较，再比较sequence_num。这样不管是memtable还是sstable文件，其内部都是按InternalKey有序的。[caption id=”attachment_799” align=”alignnone” width=”621” caption=”leveldb skiplist node”][/caption] int InternalKeyComparator::Compare(const Slice&amp; akey, const Slice&amp; bkey) const { // Order by: // increasing user key (according to user-supplied comparator) // decreasing sequence number // decreasing type (though sequence# should be enough to disambiguate) int r = usercomparator-&gt;Compare(ExtractUserKey(akey), ExtractUserKey(bkey)); if (r == 0) { const uint64_t anum = DecodeFixed64(akey.data() + akey.size() - 8); const uint64_t bnum = DecodeFixed64(bkey.data() + bkey.size() - 8); if (anum &gt; bnum) { r = -1; } else if (anum &lt; bnum) { r = +1; } } return r;}在进行Get操作时，leveldb会使用用户传入的user_key与当前db最大的sequence_num进行合并，以得到LookupKey（实际还会受snapshot机制影响）。但在内部查找时还是使用的InternalKey。 Put操作会稍微复杂点。leveldb的3种写入操作最终都会封装成WriteBatch。这3种写入操作分别是Put(写入单条key/value)、Delete(删除单条key)、Write(批量进行Put与Delete操作)。WriteBath的内部格式如下图。WriteBatch所表示的多条记录最终会按SkipList所要求的格式1条条地顺序插入到memtable中。[caption id=”attachment_801” align=”alignnone” width=”621” caption=”leveldb writebatch”][/caption] Status WriteBatchInternal::InsertInto(const WriteBatch b, MemTable memtable) { MemTableInserter inserter; // memtable 的初始sequence为WriteBatchInternal中的seq inserter.sequence = WriteBatchInternal::Sequence(b); inserter.mem = memtable; return b-&gt;Iterate(&amp;inserter);} Status WriteBatch::Iterate(Handler* handler) const { Slice input(rep_); if (input.size() &lt; kHeader) { return Status::Corruption(“malformed WriteBatch (too small)”); } // kHeader=12 = seq(8字节) + count(4字节) input.remove_prefix(kHeader); Slice key, value; int found = 0; while (!input.empty()) { found++; char tag = input[0]; // type:1字节 input.remove_prefix(1); switch (tag) { case kTypeValue: if (GetLengthPrefixedSlice(&amp;input, &amp;key) &amp;&amp; GetLengthPrefixedSlice(&amp;input, &amp;value)) { handler-&gt;Put(key, value); } else { return Status::Corruption(“bad WriteBatch Put”); } break; case kTypeDeletion: if (GetLengthPrefixedSlice(&amp;input, &amp;key)) { handler-&gt;Delete(key); } else { return Status::Corruption(“bad WriteBatch Delete”); } break; default: return Status::Corruption(“unknown WriteBatch tag”); } } if (found != WriteBatchInternal::Count(this)) { return Status::Corruption(“WriteBatch has wrong count”); } else { return Status::OK(); }}关于读写压缩等对key/value的具体操作，我们在后续章节进行分析，这里只需要了解大概。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释6–log文件]]></title>
      <url>%2Fleveldb-6-log%2F</url>
      <content type="text"><![CDATA[在leveldb中，除了sstable文件格式，还有log文件格式。该文件格式用于存储写操作的日志与manifest文件（不同的文件名）。前者用于异常回滚。后者用于记录sstable文件的元数据。整体架构中提到过，leveldb在将记录写入内存中的memtable之前，会先写入log文件，memtable会延后持久化。在这个过程中进程可能down掉。有了log写操作文件后，即使系统发生故障，levelDB也可以根据log写操作日志文件恢复内存的memtable内容，不会造成丢失数据。而manifest文件用于记录所有的sstable文件的元数据，比如sstable文件的编号，key范围。 log文件格式也是分块存储的。跟sstable文件不同的是，一方面sstable是有序存储的，因此为了加速读取，有相关索引，而log文件始终是顺序读写的，不需要定位某个key，因而不需要索引信息。另一方面sstable的data block虽大致按4KB分块，但实际上存储的块大小通常会比4KB大，这主要是因为单条record不会跨block。而log文件中的block则严格保证为一样，默认值为32KB。因此，log文件中的record可能会跨块，为了区分record是否结束，不同的block有不同的类型（kFullType/kFirstType/kMiddleType/kLastType）。如果block剩余的空间足以存储新的record，则type取值为kFullType，否则则有可能出现1个FirstType的block（剩余部分block存储record的部分内容）+ 0或者多个kMiddleType的block + 1个kLastType的block。除了type字段之外，record还包括check sum与数据的长度，最后才是具体的数据。校验和针对type+length+data。详细的格式如下图所示。如果剩余block的空间&lt;7（checksum+length+type），则剩余的直接填充0，另起一块存储数据。leveldb严格按照32k为一个单位读写一个block。[caption id=”attachment_757” align=”alignnone” width=”430” caption=”leveldb log”][/caption]doc/log_format.txt中讲述了这种文件格式的benefit与downside。好处如下：（1）容错性好。不需要额外的信息来同步。发现出错了，直接跳至下一个block（32K为一个block）。（2）容易切分，适合mapreduce等大数据处理方式。切分时，需按逻辑上的一个Record。（3）对于大记录，也不需要额外的字段来表示其长度。自然的按32k切分了嘛。一些限制如下：（1）对于小记录，没有pack机制。（2）没有压缩。对于这些限制，作者认为可以通过添加新的type来实现。 代码主要在log_format.h，log_reader.cc/.h，log_writer.cc/.h。前者定义了一些，log::Writer类用于写，比较简单，log::Reader考虑到容错，稍微复杂些。但整体上还是很简单的，给点耐心看吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释5--sstable文件]]></title>
      <url>%2Fleveldb-5-sstable%2F</url>
      <content type="text"><![CDATA[本节主要介绍sstable文件的格式及单个sstable文件的get/put。sstable文件全称是sorted string table，是一个有序且带索引的文件。 从前面的分析可知，level 0级的sstable文件是由内存中的immunable memtable经过minor compact形成的(也就是直接dump)，而level&gt;0级的文件是由level-1级别的文件经过major compact形成的。关于compact过程，我们在后续章节分析。这里着重于分析单个sstable文件的相关接口，代码在table/目录。 下图是sstable文件的整体结构。整体上，sstable文件分为数据区与索引区，尾部的footer指出了meta index block与data index block的偏移与大小，data index block指出了各data block的偏移与大小，meta index block指出了各meta block的偏移与大小。先看footer结构。如下图。footer位于sstable文件尾部，占用空间固定为48个字节。其末尾8个字节是一个magic_number。metaindex_handle与index_handle物理上占用了40个字节，但实际上存储可能连32字节都不到。每一个handle的结构BlockHandle如右图，逻辑上分别表示offset+size，在内存中占用16个字节，但存储时由于采用可变长度编码，每个handle的物理存储通常不到8+8字节。因此这里两个handle总共占用不到32个字节，剩余填充0。[caption id=”attachment_723” align=”alignnone” width=”711” caption=”leveldb footer + block handle”][/caption]BlockHandle指出了block的偏移与大小。在sstable文件中，一般有多个data block，多个meta block(当前版本只有一个filter block，可扩充)，1个meta index block，1个data index block。其中filter block的内部结构稍微不同于其他Block，但都是用BlockHandle来指向的。Footer与BlockHandle的代码主要format.cc/.h。其中Footer封装了footer的结构，BlockHandle对应图中的BlockHandle，另外有BlockContent表示从Block中实际读入的整个Block的字符串，而ReadBlock全局函数通过BlockHandle(指出了Block的偏移与大小)从指定文件中读取Block内容，并通过BlockContent返回。读取时可能需要校验crc32并解压。ReadBlock部分代码如下： Status ReadBlock(RandomAccessFile* file, const ReadOptions& options, const BlockHandle& handle, BlockContents* result) { result->data = Slice(); result->cachable = false; result->heap_allocated = false; // Read the block contents as well as the type/crc footer. // See table_builder.cc for the code that built this structure. size_t n = static_cast(handle.size()); char* buf = new char[n + kBlockTrailerSize]; Slice contents; Status s = file->Read(handle.offset(), n + kBlockTrailerSize, &contents, buf); if (!s.ok()) { delete[] buf; return s; } if (contents.size() != n + kBlockTrailerSize) { delete[] buf; return Status::Corruption("truncated block read"); } // Check the crc of the type and the block contents const char* data = contents.data(); // Pointer to where Read put the data // crc32校验 if (options.verify_checksums) { const uint32_t crc = crc32c::Unmask(DecodeFixed32(data + n + 1)); // data + type + crc const uint32_t actual = crc32c::Value(data, n + 1); if (actual != crc) { delete[] buf; s = Status::Corruption("block checksum mismatch"); return s; } } switch (data[n]) { case kNoCompression: --- // Ok break; // snappy压缩 case kSnappyCompression: { size_t ulength = 0; if (!port::Snappy_GetUncompressedLength(data, n, &ulength)) { delete[] buf; return Status::Corruption("corrupted compressed block contents"); } char* ubuf = new char[ulength]; if (!port::Snappy_Uncompress(data, n, ubuf)) { delete[] buf; delete[] ubuf; return Status::Corruption("corrupted compressed block contents"); } delete[] buf; result->data = Slice(ubuf, ulength); result->heap_allocated = true; result->cachable = true; break; } default: --- } --- } 前面通过ReadBlock读取的BlockContent内容一般通过new Block(BlockContent)初始化为具体的Block对象。现在我们看看Block的内部结构，如下图。逻辑上主要分为数据与重启点。重启点也是一个指针，指出了一些特殊的位置。data block中的key是有序存储的，相邻的key之间可能有重复，因此存储时采用前缀压缩，后一个key只存储与前一个key不同的部分。那些重启点指出的位置就表示该key不按前缀压缩，而是完整存储该key。除了减少压缩空间之外，重启点的第二个作用就是加速读取。如果说data index block可以通过二分来定位具体的block，那么重启点则可以通过二分的方法来定位具体的重启点位置，进一步减少了需要读取的数据。对于leveldb来讲，可以通过options.block_size与options.block_restart_interval来设置block的大小与重启点的间隔。默认data block的大小为4K。而重启点则每隔16个key。具体的单条record的存储格式如下图所示。[caption id=”attachment_726” align=”alignnone” width=”459” caption=”leveldb block”][/caption][caption id=”attachment_729” align=”alignnone” width=”646” caption=”leveldb单条记录存储格式”][/caption]在进一步分析Block代码之前，我们先来搞清楚这样一个问题。前面提到，data block与meta index block、data index block都是采用block来存储的(filter block稍微不同)。而对于block来讲，其都是按(key,value)格式存储一条条的record的。对于这些不同类型的block，其(key,value)都是什么了？总结如下图。现在只有一个meta block用于filter，因此meta index block中也只有一条记录，其key是filter. + filter_policy的name。[caption id=”attachment_732” align=”alignnone” width=”568” caption=”不同block的(key,value)对”][/caption]block的具体实现在block.cc/.h与blockbuilder.cc/.h中。前者用于读，后者用于写。读取时，一般先通过前面提到的ReadBlock读取一块至BlockContent中，然后再通过Block的构造函数初始化，并通过Block::Iter子类用于外部遍历或者定位具体某个key。Block类代码简单。Block::Iter的代码内部结构如下图所示。data指向读入的BlockContent内容，current指向当前的value，restart则指出了重启点数据的起始位置，restart_index则对应当前value的重启点索引。查找时，先通过restart_index指向的value定位，找到相应的重启点后，再在重启点内部遍历。顺序遍历整个block时，需要注意修改restartindex。代码细节请自行阅读。[caption id=”attachment_735” align=”alignnone” width=”492” caption=”leveldb Block::Iter”][/caption]写block，则通过blockbuilder.cc/.h中的BlockBuilder类来完成。一般是通过多次调用Add方法，再Finish结束这个block。BlockBuilder的内部结构如下图。由于block的重启点位于block尾部，因此需要先保存这些重启点，Finish时再顺序写入。图中的restarts vector就是用来保存各个重启点偏移的。counter_用于计数，当超过重启间隔时，需开启一个新的重启点。最后Finish时顺序写入这些重启点。代码也很简单。[caption id=”attachment_738” align=”alignnone” width=”388” caption=”leveldb BlockBuilder”][/caption]现在我们再来看看稍微特殊的FilterBlock。FilterBlock内部不像Block有复杂的(key,value)格式，其只有一个字符串。该字符串的内部结构如下图。filterblock保存多个filter string，每个filter string针对一段block，间隔为2^baselg，保存在末尾，默认值为2KB，也就是说，filter block每隔2kb block offset的key生成一个filter string，offsetarray指出了这些filter string的偏移。过滤时，一般先通过data index block获取key的大致block offset，再通过filter string的offsetarray获取该block offset的filter string，再进行过滤。生成时，对同一个block offset范围的数据一起构建filter string。具体代码细节在filter_block.cc/.h中。读写分别用FilterBlockReader与FilterBlockBuilder来封装。[caption id=”attachment_740” align=”alignnone” width=”610” caption=”leveldb filter block”][/caption]最后我们来看看table类。table类表示了整个sstable文件。外部通过这个类访问sstable，table内部访问block/meta block。 外部使用时，主要有两类接口。一类是读，另一类是写。读接口在table.cc/.h中实现，写接口在table_builder.cc/.h中实现。读接口的调用顺序为，Open打开一个sst文件，然后NewIterator返回一个迭代器用于遍历整个table，或者调用InternalGet用于Seek某个key。这些接口一般通过table_cache类来调用。table_cache是table的缓存。table_cache会先查看是否有cache，没有则读table文件，写cache。table_cache的具体细节我们在Get章节中分析。写接口的调用顺序为，TableBuilder构造函数初始化–&gt;多次调用Add(key,value)–&gt;Finish/Abandon(异常退出)结束。InternalGet的主要代码如下。内部会先查询filter与BlockCache。 Status Table::InternalGet(const ReadOptions&amp; options, const Slice&amp; k, void arg, void (saver)(void, const Slice&amp;, const Slice&amp;)) { Status s; Iterator iiter = rep_-&gt;indexblock-&gt;NewIterator(rep-&gt;options.comparator); //先定位该值的offset iiter-&gt;Seek(k); if (iiter-&gt;Valid()) { Slice handlevalue = iiter-&gt;value(); FilterBlockReader* filter = rep-&gt;filter; BlockHandle handle; // 根据该offset查询filter if (filter != NULL &amp;&amp; handle.DecodeFrom(&amp;handle_value).ok() &amp;&amp; !filter-&gt;KeyMayMatch(handle.offset(), k)) { // Not found } else { Slice handle = iiter-&gt;value(); // 没被过滤，表示该key可能存在，需要读取对应的block Iterator block_iter = BlockReader(this, options, iiter-&gt;value()); block_iter-&gt;Seek(k); if (block_iter-&gt;Valid()) { (saver)(arg, block_iter-&gt;key(), block_iter-&gt;value()); } s = block_iter-&gt;status(); delete block_iter; } }} Iterator Table::BlockReader(void arg, const ReadOptions&amp; options, const Slice&amp; index_value) { Table table = reinterpret_cast&lt;Table&gt;(arg); Cache blockcache = table-&gt;rep-&gt;options.block_cache; Block block = NULL; Cache::Handle* cache_handle = NULL; BlockHandle handle; Slice input = index_value; Status s = handle.DecodeFrom(&amp;input); // We intentionally allow extra stuff in index_value so that we // can add more features in the future. // block:先查block cache // cache的key是table的cache_id + block_offset // 每个table文件对应一个唯一的cache_id if (s.ok()) { BlockContents contents; if (block_cache != NULL) { char cache_key_buffer[16]; EncodeFixed64(cache_key_buffer, table-&gt;rep_-&gt;cache_id); EncodeFixed64(cache_key_buffer+8, handle.offset()); Slice key(cache_key_buffer, sizeof(cache_key_buffer)); cache_handle = block_cache-&gt;Lookup(key); if (cache_handle != NULL) { block = reinterpret_cast&lt;Block*&gt;(block_cache-&gt;Value(cache_handle)); } else { // 没有则直接读取，并加入block cache s = ReadBlock(table-&gt;rep_-&gt;file, options, handle, &amp;contents); if (s.ok()) { block = new Block(contents); if (contents.cachable &amp;&amp; options.fill_cache) { cache_handle = block_cache-&gt;Insert( key, block, block-&gt;size(), &amp;DeleteCachedBlock); } } } } else { s = ReadBlock(table-&gt;rep_-&gt;file, options, handle, &amp;contents); if (s.ok()) { block = new Block(contents); } } }}写table的主要函数Add与Finish代码如下。代码清晰，请自行阅读。 void TableBuilder::Add(const Slice&amp; key, const Slice&amp; value) { Rep* r = rep_; assert(!r-&gt;closed); if (!ok()) return; // 需要有序插入 if (r-&gt;num_entries &gt; 0) { assert(r-&gt;options.comparator-&gt;Compare(key, Slice(r-&gt;last_key)) &gt; 0); } // 前面提到过data index block的key，其值需要等候后一个block的key写入时才能构造，pending_index_entry就表示了前一个block的index，此时构造其key，并写入index_block if (r-&gt;pending_index_entry) { assert(r-&gt;data_block.empty()); r-&gt;options.comparator-&gt;FindShortestSeparator(&amp;r-&gt;last_key, key); std::string handle_encoding; r-&gt;pending_handle.EncodeTo(&amp;handle_encoding); r-&gt;index_block.Add(r-&gt;last_key, Slice(handle_encoding)); r-&gt;pending_index_entry = false; } // 往filter block中添加key if (r-&gt;filter_block != NULL) { r-&gt;filter_block-&gt;AddKey(key); } r-&gt;last_key.assign(key.data(), key.size()); r-&gt;num_entries++; r-&gt;data_block.Add(key, value); const size_t estimated_block_size = r-&gt;data_block.CurrentSizeEstimate(); // block_size限制 if (estimated_block_size &gt;= r-&gt;options.block_size) { Flush(); }} Status TableBuilder::Finish() { Rep* r = rep_; Flush(); assert(!r-&gt;closed); r-&gt;closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; // Write filter block if (ok() &amp;&amp; r-&gt;filter_block != NULL) { WriteRawBlock(r-&gt;filter_block-&gt;Finish(), kNoCompression, &amp;filter_block_handle); } // Write metaindex block if (ok()) { BlockBuilder meta_index_block(&amp;r-&gt;options); if (r-&gt;filter_block != NULL) { // Add mapping from “filter.Name” to location of filter data std::string key = “filter.”; key.append(r-&gt;options.filter_policy-&gt;Name()); std::string handle_encoding; filter_block_handle.EncodeTo(&amp;handle_encoding); meta_index_block.Add(key, handle_encoding); } // TODO(postrelease): Add stats and other meta blocks WriteBlock(&amp;meta_index_block, &amp;metaindex_block_handle); } // Write index block if (ok()) { if (r-&gt;pending_index_entry) { r-&gt;options.comparator-&gt;FindShortSuccessor(&amp;r-&gt;last_key); std::string handle_encoding; r-&gt;pending_handle.EncodeTo(&amp;handle_encoding); r-&gt;index_block.Add(r-&gt;last_key, Slice(handle_encoding)); r-&gt;pending_index_entry = false; } WriteBlock(&amp;r-&gt;index_block, &amp;index_block_handle); } // Write footer if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(&amp;footer_encoding); r-&gt;status = r-&gt;file-&gt;Append(footer_encoding); if (r-&gt;status.ok()) { r-&gt;offset += footer_encoding.size(); } } return r-&gt;status;}另外还有two_level_iterator.cc/.h与merger.cc/.h类。前者用于构造table上的迭代器，后者用于多个迭代器的归并。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释4–平台相关]]></title>
      <url>%2Fleveldb-4-port%2F</url>
      <content type="text"><![CDATA[0 编译leveldb没有使用autoconf/automake来生成Makefile，而是自己编写了Makefile以及平台检测脚本。Makefile中会先调用检测脚本build_detect_platform，并将设置的环境变量保存在build_config.mk文件里。随后Makefile会include build_config.mk文件以进行编译，相关Makefile代码如下，其中的PLATFORM_CCFLAGS等参数将在build_detect_platform中设置并输出给build_config.mk文件： detect what platform we’re building on$(shell CC=$(CC) CXX=$(CXX) TARGET_OS=$(TARGET_OS) \ ./build_detect_platform build_config.mk ./) this file is generated by the previous line to set build flags and sourcesinclude build_config.mk CFLAGS += -I. -I./include $(PLATFORM_CCFLAGS) $(OPT)CXXFLAGS += -I. -I./include $(PLATFORM_CXXFLAGS) $(OPT) LDFLAGS += $(PLATFORM_LDFLAGS) LIBS += $(PLATFORM_LIBS)build_detect_platform脚本内部使用’uname -s’命令来检测类Unix/Linux操作系统的各版本并设置合适的变量(如CC/PLATFORM等变量，以及选择合适的底层平台文件，主要是port/port_posix.cc)，然后会依次检测编译环境中是否支持c++ 0x、snappy、tcmalloc，比如检测snappy是否安装的代码如下，对于snappy，若已安装，则会定义-DSNAPPY宏： # Test whether Snappy library is installed # http://code.google.com/p/snappy/ $CXX $CXXFLAGS -x c++ - -o /dev/null 2&gt;/dev/null &lt;&lt;EOF #include &lt;snappy.h&gt; int main() {} EOF if [ “$?” = 0 ]; then COMMON_FLAGS=”$COMMON_FLAGS -DSNAPPY” PLATFORM_LIBS=”$PLATFORM_LIBS -lsnappy” fi如果你想更进一步，连leveldb本身提供的Makefile都不想使用，你想将源代码集成进来，则只需要注意包含port/port_posix.cc文件或者其他合适的跨平台文件，以及定义合适的平台参数，比如对于POSIX环境，可简单的使用’-DLEVELDB_PLATFORM_POSIX’就可以编译。 这里简单介绍了leveldb的编译过程，下面来看看其对平台的具体支持。 leveldb大致通过3层来实现平台语义的支持。第一层是对外接口，在env.cc/.h与mutexlock.h中实现，mutexlock.h提供了锁的上层接口，而env则主要提供了文件与线程操作的上层接口。这部分代码对于不同平台没有多大差别。主要定义了一个抽象接口。第二层是文件与线程操作的实际实现，对于posix，则为env_posix.cc。第三层则是一些原子操作的封装，比如信号、锁等。对于posix，对应的文件为port_posix.cc/.h。 第一层：Env：环境对象，提供文件、目录、线程等的操作接口。用户可自定义新Env对象，以加强对系统的控制；否则会使用默认的Env对象。对于posix，就是PosixEnv对象。SequentialFile：顺序读文件RandomAccessFile：随机读文件WritableFile：顺序写文件FileLock：文件锁 第二层：PosixSequentialFile是对SequentialFile接口的实现，也就是简单调用fread读文件。posix的RandomAccessFile实现有两个，一个是PosixRandomAccessFile，另一个是PosixMmapReadableFile，前者调用pread读取指定偏移的文件数据，后者为使用mmap来读取文件。PosixMmapFile是对WritableFile接口的实现，始终使用mmap来写文件。 最终的PosixEnv对象通过调用这些子类来对外提供服务。对于随机读文件，在64位环境下，最多允许1000个文件使用mmap的方式，其他则使用pread方式。 第三层：利用C++的语言特性定义了Mutex、CondVar，以在使用过程中自动释放pthread_mutex_t、pthread_cond_t资源。另外，也提供了Snappy的操作接口：Snappy_Compress与Snappy_Uncompress。 大部分代码并不复杂，这里就不一一分析了。 helpers/memenv目录下实现了自定义的Env类 — InMemoryEnv，完全基于内存操作的Env对象，对自定义Env对象感兴趣的可了解下。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释3–基础类]]></title>
      <url>%2Fleveldb-3-util%2F</url>
      <content type="text"><![CDATA[这一节主要分析下util目录下的代码。这部分代码相对独立，功能明确，很容易看明白。可先尝试理解这部分代码，以为进一步理解leveldb代码打下基础。 1 内存管理(arena.cc/.h)leveldb的大部分内存管理依赖于C++语言的默认实现，也就是不对内存进行管理。只是在memtable的实现中用到了一个简单的内存管理器(arena)。因为memtable的内部实现skip list写入时，需要分配新节点，大量节点的直接分配可能会带来较多的碎片，影响运行效率。因此，leveldb在每个memtable中都会绑定一个arena，在memtable进行minor compact后，memtable销毁时进行统一释放。下图是一个arena某个运行时刻的截图。从图中可以看出，arena内部使用的基本块大小为4K，已分配的块的指保存在一个vector中。具体分配策略如下。若分配的内存可从剩余的块中分配，则直接从剩余块中分配并调整剩余块指针的位置。否则检查要分配的size是否大于1/4的block(也就是1K)，若大于则直接new所需大小的内存，将指针保存在vector中并返回内存指针。从这里可以看出，vector保存的内存块的指针大小可能&gt;4K，也就是对大内存块直接分配。前面两个条件若都不满足，则需要new一个基本块(=4K)，将new出来的新块作为当前块，从这块当中分配内存并调整当前内存指针位置。原来当前块的剩余内存就被浪费掉了。实际中可考虑用tcmalloc/jemalloc以提升内存管理效率。[caption id=”attachment_670” align=”alignnone” width=”599” caption=”leveldb arena”][/caption] // alloc_ptr_指向基本块空闲内存位置，alloc_bytes_remaining_为剩余内存大小 // blocks_保存了所有的块，包括了基本块与直接分配的大块 // Allocation state char* alloc_ptr_; size_t alloc_bytes_remaining_; // Array of new[] allocated memory blocks std::vector blocks_; // Bytes of memory in blocks allocated so far size_t blocks_memory_; 2 过滤器bloom filter的实现(bloom.cc)leveldb通过接口NewBloomFilterPolicy提供了一个filter的实现，使用时通过传入该filter可提升对不存在的数查找时的效率。open db时的options.filter_policy参数指定该filter。如果指定了filter，leveldb会在生成新sst文件时对该文件的所有key构造一个合适的bloom filter字符串并保存在sst文件中。现在我们来看看这个bloom filter的实现机制。bloom filter需要指定bits_per_key参数，表示每个key需要检测多少个bit位/保存多少个bit位。创建bloom filter的核心代码如下： virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Compute bloom filter size (in both bits and bytes) size_t bits = n * bits_per_key_; // For small n, we can see a very high false positive rate. Fix it // by enforcing a minimum bloom filter length. if (bits < 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; const size_t init_size = dst->size(); dst->resize(init_size + bytes, 0); // 将探测需要的尾数放至最后 dst->push_back(static_cast(k_)); // Remember # of probes in filter char* array = &(*dst)[init_size]; for (size_t i = 0; i < n; i++) { // Use double-hashing to generate a sequence of hash values. // See analysis in [Kirsch,Mitzenmacher 2006]. uint32_t h = BloomHash(keys[i]); const uint32_t delta = (h >> 17) | (h Lookup(key); 如果用户想自定义block_cache算法，则可参考cache.h中Cache类所定义的抽象接口，主要接口代码如下所示。可以看出，也就是key的增删查改以及句柄的释放。 // When the inserted entry is no longer needed, the key and // value will be passed to "deleter". virtual Handle* Insert(const Slice& key, void* value, size_t charge, void (*deleter)(const Slice& key, void* value)) = 0; // If the cache has no mapping for "key", returns NULL. // // Else return a handle that corresponds to the mapping. The caller // must call this->Release(handle) when the returned mapping is no // longer needed. virtual Handle* Lookup(const Slice& key) = 0; // Release a mapping returned by a previous Lookup(). // REQUIRES: handle must not have been released yet. // REQUIRES: handle must have been returned by a method on *this. virtual void Release(Handle* handle) = 0; // Return the value encapsulated in a handle returned by a // successful Lookup(). // REQUIRES: handle must not have been released yet. // REQUIRES: handle must have been returned by a method on *this. virtual void* Value(Handle* handle) = 0; // If the cache contains entry for key, erase it. Note that the // underlying entry will be kept around until all existing handles // to it have been released. virtual void Erase(const Slice& key) = 0; // Return a new numeric id. May be used by multiple clients who are // sharing the same cache to partition the key space. Typically the // client will allocate a new id at startup and prepend the id to // its cache keys. virtual uint64_t NewId() = 0; NewLRUCache函数可调用自带的LRU实现。现在我们来看看LRU的具体实现。下图是LRUCache的内部实现图与相关代码。 [caption id="attachment_683" align="alignnone" width="471" caption="单个LRUCache"][![单个LRUCache](http://www.petermao.com/wp-content/uploads/2013/10/leveldb单个LRUCache.png "leveldb单个LRUCache")](http://www.petermao.com/wp-content/uploads/2013/10/leveldb单个LRUCache.png)[/caption] struct LRUHandle { void* value; void (*deleter)(const Slice&, void* value); LRUHandle* next_hash; // hash链表指针 LRUHandle* next; // 循环链表指针 LRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t? size_t key_length; uint32_t refs; uint32_t hash; // Hash of key(); used for fast sharding and comparisons char key_data[1]; // Beginning of key -- }; class HandleTable { --- uint32_t length_; uint32_t elems_; LRUHandle** list_; --- }; class LRUCache { --- // Initialized before use. size_t capacity_; --- // Dummy head of LRU list. // lru.prev is newest entry, lru.next is oldest entry. LRUHandle lru_; HandleTable table_; }; static const int kNumShardBits = 4; static const int kNumShards = 1 14) | B; *(ptr++) = v>>21; } else { *(ptr++) = v | B; *(ptr++) = (v>>7) | B; *(ptr++) = (v>>14) | B; *(ptr++) = (v>>21) | B; *(ptr++) = v>>28; } return reinterpret_cast(ptr); 5 默认比较器的实现—字节级比较器由于leveldb内部是有序的，因此其比较器是至关重要的。头文件中的Comparator类提供了比较器的抽象接口。Name定义了该比较器的名字，会在open时进行检查。至于FindShortestSeparator、FindShortSuccessor表示什么意思，可参看默认的字节级比较器BytewiseComparator的实现，相当简单。 // Three-way comparison. Returns value: // < 0 iff "a" < "b", // == 0 iff "a" == "b", // > 0 iff "a" > "b" virtual int Compare(const Slice& a, const Slice& b) const = 0; // The name of the comparator. Used to check for comparator // mismatches (i.e., a DB created with one comparator is // accessed using a different comparator. // // The client of this package should switch to a new name whenever // the comparator implementation changes in a way that will cause // the relative ordering of any two keys to change. // // Names starting with "leveldb." are reserved and should not be used // by any clients of this package. virtual const char* Name() const = 0; // Advanced functions: these are used to reduce the space requirements // for internal data structures like index blocks. // If *start < limit, changes *start to a short string in [start,limit). // Simple comparator implementations may return with *start unchanged, // i.e., an implementation of this method that does nothing is correct. virtual void FindShortestSeparator( std::string* start, const Slice& limit) const = 0; // Changes *key to a short string >= *key. // Simple comparator implementations may return with *key unchanged, // i.e., an implementation of this method that does nothing is correct. virtual void FindShortSuccessor(std::string* key) const = 0; 未完待续 6 crc32 7 hash 8 histogram 9 logging.cc 10 random.h]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释1--整体介绍]]></title>
      <url>%2Fleveldb-1-overview%2F</url>
      <content type="text"><![CDATA[概括的说，leveldb是一个kv型的存储库。主要特点如下： 单机嵌入式接口持久化存储KV系统，提供读写删除操作(kv都是字节序列)支持快照功能，使得读操作不受写操作影响磁盘上的数据是有序的，且分级存储，同时在文件保存相应的索引以加速读操作删除是一种特殊类型的写操作提供压缩操作以减少存储空间写入，延迟写入，通过log保证数据不丢失 整体架构如下图所示： [caption id=”attachment_597” align=”alignnone” width=”703” caption=”leveldb整体结构”][/caption] 为了支持kv型的读写删除操作，leveldb在内存与磁盘中分别用了一些数据结构/数据文件。从上图可以看出，内存中主要有memtable与immutable memtable，而磁盘上主要有CURRENT、MANIFEST、.log、LOCK、LOG以及分级的.sst等文件。 内存中的memtable与immutable table本质上都是一个skip list，保存当前写入内存而未持久化至磁盘的数据。对于特定的db来讲，一开始只有一个memtable，当memtable写入操作超过阈值(或者对应的.log文件超过阈值时)后，会转化为immutable memtable。同时会产生新的memtable以供后续数据插入。而immutable memtable是只能读不能写的。 磁盘上的LOCK文件锁住当前db。LOG文件是leveldb运行过程中产生的一些日志(注意跟.log文件区分)。.log文件则跟memtable配合，保存未持久化的写操作，以防运行过程中程序异常导致数据丢失。sst文件保存了最终的数据。随着压缩的进行，会产生不同level的sst文件。为了描述这些sst文件，比如level0有哪些文件，对应的key-range范围，则有相应的MANIFEST文件。而CURRENT文件则指向描述db最新状态信息的MANIFEST。因为leveldb支持version与snapshot机制，随着leveldb的运行，会不断的增删sst数据文件，原有的描述文件可能仍在使用，而CURRENT指向的MANIFEST则始终表示最新的描述文件。 再来看看leveldb运行过程中的主要操作。先来看写入操作(put)。从架构图可以看出，leveldb会先将写入操作写至.log文件，写入成功后再写入memtable。由于写log是追加写，而写memtable是skip list的内存操作，因此leveldb的写是相当快的。 对于读操作(get)来讲，主要分为3步。先查memtable，没有找到则在immutable memtable中继续查找；若还没有找到，则表示对应的key-value数据可能已持久化至磁盘中，因此需要借助MANIFEST文件来找到对应的sst文件，然后再在对应的sst文件中查找。由于MANIFEST文件常驻内存，而MANIFEST保存了不同sst文件的key-range，因此查找对应的文件是很快的。找到对应的文件后，由于sst文件内部是有序的，并且有相应的索引索引不同key-range的value，因此需要先读入这些索引，再在索引中二分查找，再根据索引找到对应的块，然后遍历相应的块查找。这个过程可能需要两次大范围的读文件操作。一次是读索引，一次是读对应的block。为了加快这个操作，leveldb有相应的table cache与block cache，以分别加速sst文件与block的读操作。另外有些数据可能在db中就不存在，为了加速这些数据的查找，每个sst文件都可以添加合适的bloom filter。bloom filter是在载入相应的sst文件后对数据进行过滤的。 最后来看看压缩(compact)操作。leveldb的压缩操作分为minor compact与major compact。所有的压缩操作都是启动一个新线程来完成的。从架构图可以看出，minor compact就是遍历immutable memtable(本质上是一个skip list)，dump至一个新的level0 sst文件。从这里可以看出，新dump出来的sst文件可能会跟已有的level0级别的sst文件存在key-range上的重叠。而level[1-6]上的sst文件则不会存在这个这个问题，level[1-6]上的文件是由上一级文件进行major compact得到的。当level 0层文件数超过阈值(&gt;=4)，会触发level0与level1层文件合并。而对于大于等于level1以上的level，总文件字节数超过阈值(10M 100M 1G 10G 100G 1T(level=6))则会触发该操作。至于选择哪个文件与下一级别的文件进行compact，则是轮流进行的。另外如果特定level文件的seek次数(也就是访问次数)超过阈值，也会触发对应的major compact操作，此时则会直接选择该文件与下一level进行major compact操作。 后续我们将先分析内存与磁盘上的数据结构/文件格式代码，再分析GET/PUT等操作对应代码。在此之前，我们看看leveldb源代码的目录结构。doc/: 包括了一些文档，&gt; benchmark.html 性能测试文档&gt; impl.html 内部实现文档&gt; index.html 使用文档&gt; log_format.html .log文件格式&gt; table_format.html .sst文件格式 helpers/:提供了一个简单的内存文件操作接口，相当于实现了一个Env接口。Env接口对应了leveldb运行的底层平台。include/leveldb/:对外接口，使用时include该目录下的头文件即可。&gt; cache.h cache接口，用户可实现合适的cache接口，以加速读操作&gt; c.h leveldb是用c++编写的，这里提供了c的对外封装&gt; comparator.hdb.henv.hfilter_policy.hiterator.hoptions.hslice.hstatus.htable_builder.htable.hwrite_batch.h util/:&gt; arena&gt; bloom&gt; cache&gt; coding&gt; comparator&gt; crc32c&gt; env&gt; env_posix&gt; filter_policy&gt; hash&gt; histogram&gt; logging&gt; mutexlock&gt; options&gt; posix_logger&gt; random&gt; status&gt; testharness&gt; testutil port/:&gt; atomic_pointer&gt; port_example&gt; port&gt; port_posix&gt; README&gt; thread_annotations table/: sst table相关操作的封装&gt; block_builder&gt; block&gt; filter_block&gt; format&gt; iterator&gt; iterator_wrapper&gt; merger&gt; table_builder&gt; table&gt; two_level_iterator db/:leveldb主要操作的实现部分&gt; builder&gt; c&gt; db_bench&gt; dbformat&gt; db_impl&gt; db_iter&gt; filename&gt; log_format&gt; log_reader&gt; log_writer&gt; memtable&gt; repair&gt; skiplist&gt; snapshot&gt; table_cache&gt; version_edit&gt; version_set&gt; write_batch&gt; write_batch_internal]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leveldb注释0--start]]></title>
      <url>%2Fleveldb-0-start%2F</url>
      <content type="text"><![CDATA[因为工作需要，曾对leveldb进行过测试与实际使用，而对leveldb的源代码进行学习，则纯粹是出于一个码农对美好世界进行探究的好奇。接下来将尽可能从源代码上给出leveldb代码的详尽注释，这里先列出自己在阅读前后的主要参考。 0 官方文档http://leveldb.googlecode.com/svn/trunk/源代码，主要使用了1.7.0版本https://leveldb.googlecode.com/files/leveldb-1.7.0.tar.gz http://leveldb.googlecode.com/svn/trunk/doc/index.html官方使用手册，比较详细 http://leveldb.googlecode.com/svn/trunk/doc/benchmark.html官方的性能测试 http://leveldb.googlecode.com/svn/trunk/doc/impl.html比较粗略的介绍了leveldb的主要实现细节 http://leveldb.googlecode.com/svn/trunk/doc/table_format.txthttp://leveldb.googlecode.com/svn/trunk/doc/log_format.txtsstable文件格式与log文件格式 1 原理【朗格科技】LevelDb日知录系列 http://www.samecity.com/blog/Index.asp?SortID=12数据分析与处理之二（Leveldb 实现原理）http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html比较详尽的介绍了leveldb各模块的工作原理与实现细节，缺点是没有源代码细节。对leveldb最初细节的了解，也起于该文。建议读者可先阅读此文，以对leveldb的原理有最初的印象。另外，作者似乎没有把该系列写完。 leveldb实现解析.pdf从代码细节上进行了分析，不过对于初接触者恐怕容易陷入过多的细节，缺乏整体性的介绍。建议对原理有了大致了解，且初读代码后，再进行对照学习。 2 性能关于LevelDB http://blog.sina.com.cn/s/blog_593af2a70100ztjn.html LevelDB 写操作出现停顿的问题分析 http://www.ideawu.net/blog/archives/709.html Performance data for LevelDB, Berkley DB and BangDB for Random Operationshttp://highscalability.com/blog/2012/11/29/performance-data-for-leveldb-berkley-db-and-bangdb-for-rando.html leveldb与kyoto cabinet性能测试http://blog.creapptives.com/post/8330476086/leveldb-vs-kyoto-cabinet-my-findings 3 相关实现go实现 https://code.google.com/p/leveldb-go Tair ldb(LevelDB)原理与应用案例.pdf leveldb+redis https://github.com/qiye/redis-storagessdb https://github.com/ideawu/ssdbsophia https://github.com/pmwkaa/sophia]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用v8识别js重定向]]></title>
      <url>%2Fv8-js-redirect%2F</url>
      <content type="text"><![CDATA[一 重定向爬虫抓取的过程中，遇到重定向时需要识别出重定向后的url。总结下，主要有如下3类重定向：1 http协议的3XX301 永久移动302 临时305 必须通过指定定代理才能访问相应资源307 临时 比如 访问http://www.meilishuo.com/，响应如下12345678HTTP/1.1 301 Moved PermanentlyServer: mls/1.1Date: Mon, 01 Jul 2013 07:19:15 GMTTransfer-Encoding: chunkedConnection: keep-aliveLocation: /welcomeCache-Control: no-cache,must-revalidate,no-storePragma: no-cache 2 html的meta字段meta字段的refresh属性1234&lt;html&gt;&lt;meta http-equiv="refresh" content="1; url=http://www.petermao.com" /&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 3 js重定向有好几种，后面总结下，这里给个简单例子：1234&lt;html&gt;&lt;body&gt;&lt;script type='text/javascript'&gt;window.open("http://www.petermao.com")&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 解决方案：第一种在获得http首部后，根据状态码，就可以判断是否重定向，再根据location属性来获取重定后的url。 第二种已经获得了网页内容。一般是是通过正则匹配出meta，refresh以及content属性，再获取重定向后的url。当然也可以进行dom解析，正确性会好点，但可能会存在性能问题，个人感觉没有这个必要。正则匹配即可。 第三种是难点。我们先来看看js重定向有哪些表现形式。 二 JS重定向分类主要有如下几类，这里得到了朋友@秋声落叶的帮助，欢迎其他朋友补充。1 window：window.open(url) 2 location属性：1234567891011location=url;location.href=url;window.location=url;document.location=url;window.location.href=url;window.location.replace(url)window.location.assign(url)location.replace(url);location.assign(url);self.location=url;top.location=url; 3 navigate方法：12window.navigate(url)top.navigate(url); 4 document+meta字段123document.write('&lt;META HTTP-EQUIV="refresh" content="0;url=http://www.petermao.com"&gt;');document.writeIn('&lt;META HTTP-EQUIV="refresh" content="0;url=http://www.petermao.com"&gt;');document.close(); 三 JS重定向识别的1个思路1 问题要识别前面的重定向，通过正则可能不现实，比如前面的JS可以封装在函数里，通过变量来传递重定向后的url。比如前面的location的变种：12345678910111213&lt;html&gt;&lt;head&gt;&lt;script type='text/javascript'&gt;function AutoRedirect(seconds,redirectUrl)&#123;setTimeout("Redirect('"+redirectUrl+"')",seconds);&#125;function Redirect(redirectUrl)&#123;location.href=redirectUrl;&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;script type='text/javascript'&gt;AutoRedirect(2, 'http://www.petermao.com');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 此时要获得这些JS，只能通过完整的解析了，基于DOM树可以完整的获取这些JS。将script当作html的一个标签即可。在完整获得这些JS后，怎么知道其实际表示什么内容了？这个就需要JS运行环境了。可以考虑开源的V8与SpiderMonkey了。（本文只考虑V8）。 2 使用V8的问题不过这里有个问题，对于在浏览器中运行的html网页，浏览器会提供1个叫做window的全局对象，像location、top等对象，以及settimeout等函数就是window的子属性/方法，而这个window的实现被封装在BOM模型（浏览器对象模型）中，该模型不像DOM、JS那样有统一的标准。V8只提供JS运行环境，像window等对象是由浏览器内核提供的，比如webkit中有BOM的实现。之前曾短暂研究过webkit，不过这货太过复杂，还未入门，抽取其中的库来解析与运行网页，不知道是否可行，有研究的朋友可以一起讨论下。前面那些抽取出来的JS，如果直接交给V8来运行，会出现undefined的错误，也就是window、location、settimeout等在V8中并没有定义。 3 最后在只有V8的情况下，如何运行网页中的JS了？那就需要mock了。 V8中表示JS运行环境的是Context对象，我们可以一开始在Context中初始化window等全局对象。然后轮流运行解析出来的JS脚本，最后通过location属性来获取重定向后的url。 最后给出部分初始化mocker代码如下，完整的需要mock前面给出的JS重定向的各种变形。实际mock可以参考Node.js，该货基于V8，提供了一些有用函数的实现，也有利于了解V8的特性。 至于V8的使用，还是翻翻官方文档吧，这里就不说了。 类似的，对于有些JSON数据，或者AJAX的抓取，应该也可以考虑类似的思路。欢迎大家讨论。 12345678910111213141516171819window = &#123;&#125;; location = &#123;&#125;; window.location = &#123;&#125;; location.href = &#123;&#125;; window.location.href = &#123;&#125;; top=&#123;&#125;;parent=&#123;&#125;;location.assign = function (url) &#123; window.location.href = url;&#125;location.replace = function (url) &#123; window.location.href = url;&#125;window.location.assign = location.assign;window.location.replace = location.replace;---]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于sitemap的链接收集]]></title>
      <url>%2Fsitemap%2F</url>
      <content type="text"><![CDATA[0 sitemap基础 sitemap反映了一个网站网页的整体结构示意图。通过该网页，站长会将需要被抓取的页面全部列出来。sitemap允许级联。按照[1]，sitemap允许三种格式：txt、xml、索引格式。但实际上，大量网站使用普通的html网页作为sitemap链接，因此html网页也需要考虑。 1 sitemap的用途sitemap将网站需要被抓取的页面全部列出来，因此该页面一般在站长更新网站内容后更新，可以较好的反映网站内容的实时变化，方便了爬虫的实时抓取。 但另一方面，sitemap是站长主动向大的搜索引擎提交的，因此这些sitemap页面的具体地址并不固定，理论上是可以是任何形式的链接。那么我们如何找到这些链接了？ 2 寻找网站的sitemap链接对sitemap地址进行简单的分析后，个人认为主要可以从以下几个方面来收集sitemap链接：a Robots中的sitemap指令Robots.txt中通过sitemap指令来指示sitemap链接的地址，因此我们可以通过对网站的Robots.txt进行解析得到。eg：http://zhanghanyang.lofter.com/robots.txt b 首页锚文本发现如果首页存在指向sitemap页面的链接的话，一般其锚文本为如下几种：站点地图网站地图网站导航所以这一步可以通过对host首页进行解析得到。eg：http://redicecn.com/中的网站地图。 c 固定地址尝试sitemap页面通常放在一些固定的地址，比如http://www.williamlong.info/的sitemap页面就是http://www.williamlong.info/sitemap.asp. 观察了下，这些链接是常用的sitemap链接：/sitemap.xml/sitemap.txt/sitemap.html/sitemap.htm/sitemap.php/sitemap.asp/sitemap.jsp/sitemap_baidu.xml/sitemap 另外，对于个人博客来将，通常会有专门的插件来生成sitemap地址，不同的插件生成的默认地址一般不同。比如对DedeCms建站系统，默认路径是/data/sitemap.html。我们可以进一步调研下常用的sitemap自动生成插件，查看下他们的默认路径。 d 人工注入这个必须可以嘛。 3 基于sitemap的链接收集要注意，我们的目的是搜集网站的新链接，而收集sitemap链接只是为了更好的发现这些链接。 因此对于这样1个系统，应该主要由两个子系统组成，一个是sitemap本身的链接发现，另一个就是基于sitemap链接来收集网站的新链接。sitemap本身的链接发现在上个部分已经进行了汇总，为了简便与可扩展，这应该作为1个独立的子系统。 3.1 sitemap本身的链接发现a 输入：一批domain 生成该domain的robots链接、首页链接并抓取解析Robots页中的sitemap指令执行的链接，首页中锚文本为网站地图、站点题图等链接输出上述链接集合作为S1 S1 + 人工注入的sitemap链接作为集合S2 + 猜测可能的sitemap地址&lt;猜测的方法见上一节&gt;作为集合S3就是Sitemap链接的种子，将这批数据注入到第二个子系统。 补充说明：在第一轮链接发现后，不需要频繁的进行新sitemap链接发现，只需要人工注入几个未发现的domain的sitemap地址即可。 3.2 基于sitemap链接来收集新链接a 数据的存储：我们怎么来存储这些sitemap链接了？要记得的一个要点是存储这些sitemap链接是为了之后调度，再更好的发现新增的网页内容链接。之前我曾考虑过按host来设计这个子系统，因为一般来说，1个网站我们只需要考虑1个有效的sitemap地址。但经过实践，这样会很复杂，也不便于扩展。按照sitemap最标准的格式，是允许链式指向的，也就是sitemap链接里也可以包含sitemap链接，这会导致存储这些包含关系很复杂；另一方面，考虑到可以将该系统扩展到其他类似系统，按host或者domain来设计不够灵活。因此最好还是一个单独的sitemap链接作为1个单独的key，其他相关属性作为value。 数据的重复性一开始按domain来设计还有一个考虑的原因，就是一个网站通常会提供多个sitemap链接，这些sitemap链接包含的内容应该是等价的。按照我们第一个子系统的方案，1个domain最多有10个左右的可能sitemap链接，会导致10倍左右的额外存储开销。 但最后考虑到系统复杂性、容错与可扩展性，还是允许这些的冗余存在。现实的情况是，我们很难保证1个网站今天使用这个sitemap链接，难保1个月之后不使用其他的sitemap链接，因此使用唯一的sitemap链接是不现实的。另一方面，因为现实中大部分网站只有1~2个sitemap链接，再加上合理的调度方案，这不会导致系统其他组件的过多开销。由于sitemap链接是针对domain级别的，整个存储总量也并不大。 b 调度只要能不断的发现新的链接即可。调度的频率可以根据实际资源&lt;比如爬虫&gt;分配。 c 抓取由于对同一个domain最多只有10个左右的链接，因此对网站不存在较大压力，只要网络带宽足够，可以很快的抓完。 d 解析根据前面的描述，sitemap链接有几种可能的表现形式（txt、html、xml），我们需要对这些一一解析。两个子系统可以共用1个解析模块。 e 垃圾页可能会存在一些spam，反复修改sitemap中的链接。这个需要其他模块反馈。比如对于这种sitemap链接http://gzfriendly.com/sitemap.xml，里面包含了大量不属于该domain的链接，解析时可直接过滤掉。 f 反馈抓取成功率、抓取的性能；贡献的好链接数等。 4 系统的可扩展性类似的，我们可以将该系统扩展到类似于网站主动提供内容索引页的系统，实时且方便。实时爬虫通常需要这些变化快的链接，相同点都是提取出新产生的url链接，计算合理的更新频率并进行调度。比如RSS页、hub页的收集。 5 致谢感谢@redice的帮助，在sitemap的链接发现过程中，提供了诸多帮助。 附：参考[1] 百度sitemap文档http://zhanzhang.baidu.com/wiki/93 [2] google sitemap文档https://support.google.com/webmasters/topic/8476?hl=en [3] Google Sitemaps使用指南http://www.williamlong.info/archives/327.html [4] robots中的sitemap指令http://en.wikipedia.org/wiki/Robots.txt#Sitemap]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网页spam相关论文了解]]></title>
      <url>%2Fspam%2F</url>
      <content type="text"><![CDATA[最近对网页spam有兴趣，找了些论文看看，部分还没看完，做个小结吧，后续再慢慢看，慢慢补充了。主要内容都是参考里的论文里的。 1 spam基础那些误导搜索引擎排名的行为或者从搜索引擎中获得不应有的利益的行为都可以称为spam，具体是否为spam取决于搜索引擎的判断标准。 简单来说，那些搜索引擎明文允许进行的或者即使搜索引擎不存在但仍存在的“优化”行为可以认为不是spam。比如一些针对特定client、终端的优化行为不能认为是spam。再比如垃圾网站里贴出的链接，这些链接不应该作为spam，因为这些链接的存在超出了链接目的地的owner的控制，惩罚的应该是垃圾网站本身而不是它指向的链接。Anything that would still be done if search engines did not exist, or anything that a search engine has given written permission to do.[2] 英文spam据估计在15%左右，也可以按语言、domain等进行区分。中文的估计更高。 产生spam的主要原因是一方面搜索引擎是web的入口，另一方面搜索引擎按page的质量进行排序，用户只关注top10的点击，这些点击带来了相当多的流量，spam的目的就是排在前面，以获得更好的流量。 spam的主要危害如下：影响用户体验；浪费资源；对好网站不公平； 2 相关理论模型所有rank相关的模型都涉及了，有些还没来得及细细研究。 tf-idf = sum(tf(t)*idf(t)) t是查询query与文档term的交集tf: 单词占文章的百分比idf: 单词在所有文档集合中的出现频率百分比的倒数因为web的页面内容web owner可以随意修改，idf是全局的，一般认为spam没法控制，因此spam主要针对tf VSM：将文档document与查询query表示成term权重的向量，计算文档与查询的相似度以进行排序&lt;余弦距离是常用的方法&gt; pagerank：使用入链信息来打分the importance of a certain page inﬂuences and is being inﬂuenced by theimportance of some other pages. hits：使用入链与出链信息来打分&lt;分别对应authority与hub得分&gt;According to the circular deﬁnition of HITS, important hub pages are those that point to many important authority pages,while important authority pages are those pointed to by many hubs.spam通过影响出链信息，再影响入链信息。 browserrank query-click rank factor：前面的hits、pagerank等也是排序因子，按照[3]，排序因子主要可以分为 on the page factor + off the page factor。具体可以参考[3]，对了解spam的产生原因有帮助。 评价标准：查准率查全率 3 分类host/domain name spam:购买过期域名、host堆积&lt;比如一个domain下N多个子host，这些子host的前缀通常是随机的数字或者常用的term&gt;，某些热门关键词的domain&lt;比如由热门词汇组成很长的domain，中间以-等分隔&gt; content spam：位置：web页面中的任何一个位置都可以。常用的比如title meta body anchor&lt;针对指向的page&gt; url。策略主要可以分为重复(repeat)与隐藏(hide)。重复: 单个单词重复(Repetition) 大量词汇堆砌(Dumping) 穿插(Weaving)：拷贝一篇好的页面，再在里面穿插些spam词汇或者链接 短语堆积(Phrase stitching):从不同的文章里抽取些词汇再汇总 链接堆积隐藏：背景色与文本色一样的文本/链接 可视化属性设置为false 使用脚本生成文本与链接 通过img的点击指向新链接 size特别短小的文本&lt;用户不可见&gt; link spam：位置：出链(outlink)与入链(inlink)出链：克隆好的导航网站/目录(open dir clone)，比如国外的dmoz.org、http://dir.yahoo.com等，国内的hao123等入链：链接农场(link farm)：一堆链接的指向复杂度超出了阈值 蜜罐(honey pot)：从别的网站里拷贝的一些好页面，里面包含了一些spam链接；open dir clone也可放入此类。 在一些权威网站里贴垃圾链接(insert link at dir) 在一些open的平台里帖链接，比如blog、wiki、social site、留言板 友情链接交换(link exchange) 购买过期的域名(expired domain)：在购买的废弃域名里张贴大量链接，主要废弃域名的排名会存在一段时间 关于隐藏(hiding)技术与重定向(redirect)技术：有的论文将隐藏技术与重定向技术作为单独的一类spam技术进行划分。隐藏技术包括基于IP的(搜集特定crawler的IP)与基于HTTP协议头的User Agent(crawler一般会用这个字段标识自身)，另外前面所讲的基于内容的隐藏技术也划分到此类。重定向技术包括http协议重定向、meta重定向与JS重定向。 除去已划分到内容spam部分的基于内容的隐藏技术，剩下的个人认为没有多少意思。对于基于特定IP/http请求头的隐藏技术，检测方法很容易，无非是多次爬取，基于内容的比较，对于大的商业crawler，这样做貌似不够友好，也浪费资源；至于重定向，比如JS重定向，随着越来越多的好网页在特殊情况下也会使用这种技术，个人认为采用类似于V8的JS引擎，已可以检测出重定向后的URL，就不用考虑此类问题了。 用户的角度：基于浏览行为的，基于query点击的， 4 检测content：基于规则的与基于统计的 link：link farm检测pagerank分值较低的trustrank分值spamrank分值较高的初始种子的选择：人工选择好的、差的，in link 与out link的交集&lt;不同domain的&gt; &gt; 阈值； 用户行为的事后分析：click-model针对click-logbrowser-model针对browse-log 附：参考[1] Monika Henzinger, Rajeev Motwani, and Craig Silverstein. Challenges in web search engines. SIGIR Forum, 36(2), 2002.[2] The Classification of Search Engine Spam http://www.silverdisc.co.uk/articles/spam-classification[3] Web Spam Taxonomy[4] Survey on web spam detection-principles and algorithms[5] Alan Perkins. The classification of search engine spam. http://www.ebrandmanagement.com/whitepapers/spam-classification/.[6] Zolt´an Gy¨ongyi and Hector Garcia-Molina. Link spam alliances. Technical report, Stanford University, 2005. rank factorgoogle ranking factor: http://www.vaughns-1-pagers.com/internet/google-ranking-factors.htm pagerank[1] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical report, Stanford University, 1998.[2] Monica Bianchini, Marco Gori, and Franco Scarselli. Inside PageRank. ACM Transactions on Internet Technology, 5(1), 2005.[3] T. Haveliwala. Efficient computation of PageRank. Tech. rep., Stanford University, 1999.[4] T. Haveliwala. Topic-sensitive PageRank. In Proceedings of the Eleventh International Conference on World Wide Web, 2002.[5] S. Kamvar, T. Haveliwala, C. Manning, and G. Golub. Extrapolation methods for accelerating PageRank computations. In Proceedings of the Twelfth International Conference on World Wide Web, 2003.[6] A. Langville and C. Meyer. Deeper inside PageRank. Tech. rep., North Carolina State University, 2003.[7] Jon Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5), 1999. anti-spam[1] Dennis Fetterly, Mark Manasse, and Marc Najork. Spam, damn spam, and statistics. In Proceedings of the Seventh International Workshop on the Web and Databases (WebDB), 2004.[2] Z. Gy¨ongyi and H. Garcia-Molina. Seed selection in TrustRank. Tech. rep., Stanford University, 2004.[3] Pr0 - Google’s PageRank 0, http://pr.efactory.de/e-pr0.shtml. 2002.[4] Combating Web Spam with TrustRank[5] Detecting Spam Web Pages through Content Analysis[6] Identifying Link Farm Spam Pages[7] Fighting against Web Spam: A Novel Propagation Method based on Click-through Data]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析25–VM（下）]]></title>
      <url>%2F121%2F</url>
      <content type="text"><![CDATA[这一节介绍下redis中的多线程机制。 先看看多线程换出的机制。 serverCron函数中调用 vmSwapOneObjectThreaded开始多线程方式换出value，vmSwapOneObjectThreaded会调用 vmSwapOneObject（参看上一节的解释），而vmSwapOneObject最终会调用vmSwapObjectThreaded。 static int vmSwapObjectThreaded(robj *key, robj *val, redisDb *db) { iojob *j; assert(key->storage == REDIS_VM_MEMORY); assert(key->refcount == 1); j = zmalloc(sizeof(*j)); j->type = REDIS_IOJOB_PREPARE_SWAP; j->db = db; j->key = key; j->val = val; incrRefCount(val); j->canceled = 0; j->thread = (pthread_t) -1; key->storage = REDIS_VM_SWAPPING; lockThreadedIO(); queueIOJob(j); unlockThreadedIO(); return REDIS_OK; } vmSwapObjectThreaded 会创建一个类型为REDIS_IOJOB_PREPARE_SWAP的job，然后使用queueIOJob来排队。而queueIOJob所做的主要工作就是就是将新job加入到server.io_newjobs，并在创建的线程数还没超过配置值时，创建新的线程。 /* This function must be called while with threaded IO locked */ static void queueIOJob(iojob *j) { redisLog(REDIS_DEBUG,"Queued IO Job %p type %d about key '%s'\n", (void*)j, j->type, (char*)j->key->ptr); listAddNodeTail(server.io_newjobs,j); if (server.io_active_threads < server.vm_max_threads) spawnIOThread(); } 从spawnIOThread中可以知道，新线程的入口点是IOThreadEntryPoint。 static void spawnIOThread(void) { pthread_t thread; sigset_t mask, omask; int err; sigemptyset(&mask); sigaddset(&mask,SIGCHLD); sigaddset(&mask,SIGHUP); sigaddset(&mask,SIGPIPE); pthread_sigmask(SIG_SETMASK, &mask, &omask); while ((err = pthread_create(&thread,&server.io_threads_attr,IOThreadEntryPoint,NULL)) != 0) { redisLog(REDIS_WARNING,"Unable to spawn an I/O thread: %s", strerror(err)); usleep(1000000); } pthread_sigmask(SIG_SETMASK, &omask, NULL); server.io_active_threads++; } IOThreadEntryPoint会将io_newjobs中的job移入server.io_processing，然后在做完job类型的工作后（加载value/计算value所需交换页数/换出value），将job从server.io_processing移入io_processed中。然后往 server.io_ready_pipe_write所在的管道（io_ready_pipe_read、io_ready_pipe_write组成管道的两端）写入一个字节，让睡眠中的vmThreadedIOCompletedJob继续运行，该函数会做些后续工作。 static void *IOThreadEntryPoint(void *arg) { iojob *j; listNode *ln; REDIS_NOTUSED(arg); pthread_detach(pthread_self()); while(1) { /* Get a new job to process */ lockThreadedIO(); if (listLength(server.io_newjobs) == 0) { /* No new jobs in queue, exit. */ redisLog(REDIS_DEBUG,"Thread %ld exiting, nothing to do", (long) pthread_self()); server.io_active_threads--; unlockThreadedIO(); return NULL; } ln = listFirst(server.io_newjobs); j = ln->value; listDelNode(server.io_newjobs,ln); /* Add the job in the processing queue */ j->thread = pthread_self(); listAddNodeTail(server.io_processing,j); ln = listLast(server.io_processing); /* We use ln later to remove it */ unlockThreadedIO(); redisLog(REDIS_DEBUG,"Thread %ld got a new job (type %d): %p about key '%s'", (long) pthread_self(), j->type, (void*)j, (char*)j->key->ptr); /* Process the Job */ if (j->type == REDIS_IOJOB_LOAD) { j->val = vmReadObjectFromSwap(j->page,j->key->vtype); } else if (j->type == REDIS_IOJOB_PREPARE_SWAP) { FILE *fp = fopen("/dev/null","w+"); j->pages = rdbSavedObjectPages(j->val,fp); fclose(fp); } else if (j->type == REDIS_IOJOB_DO_SWAP) { if (vmWriteObjectOnSwap(j->val,j->page) == REDIS_ERR) j->canceled = 1; } /* Done: insert the job into the processed queue */ redisLog(REDIS_DEBUG,"Thread %ld completed the job: %p (key %s)", (long) pthread_self(), (void*)j, (char*)j->key->ptr); lockThreadedIO(); listDelNode(server.io_processing,ln); listAddNodeTail(server.io_processed,j); unlockThreadedIO(); /* Signal the main thread there is new stuff to process */ assert(write(server.io_ready_pipe_write,"x",1) == 1); } return NULL; /* never reached */ } static void vmThreadedIOCompletedJob(aeEventLoop *el, int fd, void *privdata, int mask) { char buf[1]; int retval, processed = 0, toprocess = -1, trytoswap = 1; REDIS_NOTUSED(el); REDIS_NOTUSED(mask); REDIS_NOTUSED(privdata); if (privdata != NULL) trytoswap = 0; /* check the comments above... */ /* For every byte we read in the read side of the pipe, there is one * I/O job completed to process. */ while((retval = read(fd,buf,1)) == 1) { iojob *j; listNode *ln; robj *key; struct dictEntry *de; redisLog(REDIS_DEBUG,"Processing I/O completed job"); /* Get the processed element (the oldest one) */ lockThreadedIO(); assert(listLength(server.io_processed) != 0); if (toprocess == -1) { toprocess = (listLength(server.io_processed)*REDIS_MAX_COMPLETED_JOBS_PROCESSED)/100; if (toprocess value; listDelNode(server.io_processed,ln); unlockThreadedIO(); /* If this job is marked as canceled, just ignore it */ if (j->canceled) { freeIOJob(j); continue; } /* Post process it in the main thread, as there are things we * can do just here to avoid race conditions and/or invasive locks */ redisLog(REDIS_DEBUG,"Job %p type: %d, key at %p (%s) refcount: %d\n", (void*) j, j->type, (void*)j->key, (char*)j->key->ptr, j->key->refcount); de = dictFind(j->db->dict,j->key); assert(de != NULL); key = dictGetEntryKey(de); if (j->type == REDIS_IOJOB_LOAD) { redisDb *db; /* Key loaded, bring it at home */ key->storage = REDIS_VM_MEMORY; key->vm.atime = server.unixtime; vmMarkPagesFree(key->vm.page,key->vm.usedpages); redisLog(REDIS_DEBUG, "VM: object %s loaded from disk (threaded)", (unsigned char*) key->ptr); server.vm_stats_swapped_objects--; server.vm_stats_swapins++; dictGetEntryVal(de) = j->val; incrRefCount(j->val); db = j->db; freeIOJob(j); /* Handle clients waiting for this key to be loaded. */ handleClientsBlockedOnSwappedKey(db,key); } else if (j->type == REDIS_IOJOB_PREPARE_SWAP) { /* Now we know the amount of pages required to swap this object. * Let's find some space for it, and queue this task again * rebranded as REDIS_IOJOB_DO_SWAP. */ if (!vmCanSwapOut() || vmFindContiguousPages(&j->page,j->pages) == REDIS_ERR) { /* Ooops... no space or we can't swap as there is * a fork()ed Redis trying to save stuff on disk. */ freeIOJob(j); key->storage = REDIS_VM_MEMORY; /* undo operation */ } else { /* Note that we need to mark this pages as used now, * if the job will be canceled, we'll mark them as freed * again. */ vmMarkPagesUsed(j->page,j->pages); j->type = REDIS_IOJOB_DO_SWAP; lockThreadedIO(); queueIOJob(j); unlockThreadedIO(); } } else if (j->type == REDIS_IOJOB_DO_SWAP) { robj *val; /* Key swapped. We can finally free some memory. */ if (key->storage != REDIS_VM_SWAPPING) { printf("key->storage: %d\n",key->storage); printf("key->name: %s\n",(char*)key->ptr); printf("key->refcount: %d\n",key->refcount); printf("val: %p\n",(void*)j->val); printf("val->type: %d\n",j->val->type); printf("val->ptr: %s\n",(char*)j->val->ptr); } redisAssert(key->storage == REDIS_VM_SWAPPING); val = dictGetEntryVal(de); key->vm.page = j->page; key->vm.usedpages = j->pages; key->storage = REDIS_VM_SWAPPED; key->vtype = j->val->type; decrRefCount(val); /* Deallocate the object from memory. */ dictGetEntryVal(de) = NULL; redisLog(REDIS_DEBUG, "VM: object %s swapped out at %lld (%lld pages) (threaded)", (unsigned char*) key->ptr, (unsigned long long) j->page, (unsigned long long) j->pages); server.vm_stats_swapped_objects++; server.vm_stats_swapouts++; freeIOJob(j); /* Put a few more swap requests in queue if we are still * out of memory */ if (trytoswap && vmCanSwapOut() && zmalloc_used_memory() > server.vm_max_memory) { int more = 1; while(more) { lockThreadedIO(); more = listLength(server.io_newjobs)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析24–VM（中）]]></title>
      <url>%2F118%2F</url>
      <content type="text"><![CDATA[VM根据value换进换出的策略又有两种使用方式：阻塞方式和多线程方式（server.vm_max_threads == 0为阻塞方式）。 这一节主要介绍阻塞方式。 redis 启动重建db（aof方式或者快照方式）时，可能会因为内存限制将某些value换出到磁盘，此时只使用阻塞方式换出 value（vmSwapOneObjectBlocking函数）。除此之外，redis只在serverCron函数（该函数事件处理章节分析过）中换出value。我们来看看serverCron中的处理代码，阻塞方式使用函数vmSwapOneObjectBlocking换出value，多线程方式使用函数vmSwapOneObjectThreaded换出value。 static int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) { --- /* Swap a few keys on disk if we are over the memory limit and VM * is enbled. Try to free objects from the free list first. */ if (vmCanSwapOut()) { while (server.vm_enabled && zmalloc_used_memory() > server.vm_max_memory) { --- if (tryFreeOneObjectFromFreelist() == REDIS_OK) continue; retval = (server.vm_max_threads == 0) ? vmSwapOneObjectBlocking() : vmSwapOneObjectThreaded(); --- } } --- return 100; } 无论是阻塞方式vmSwapOneObjectBlocking换出value，还是多线程方式vmSwapOneObjectThreaded换出value，最终都调用vmSwapOneObject（调用参数不一样）来换出value。 vmSwapOneObject会对每个db，随机选择5项，计算它的swappability，然后如果是多线程方式，则调用vmSwapObjectThreaded来换出value，否则使用vmSwapObjectBlocking换出value。 static int vmSwapOneObject(int usethreads) { int j, i; struct dictEntry *best = NULL; double best_swappability = 0; redisDb *best_db = NULL; robj *key, *val; for (j = 0; j < server.dbnum; j++) { redisDb *db = server.db+j; /* Why maxtries is set to 100? * Because this way (usually) we'll find 1 object even if just 1% - 2% * are swappable objects */ int maxtries = 100; if (dictSize(db->dict) == 0) continue; for (i = 0; i < 5; i++) { dictEntry *de; double swappability; if (maxtries) maxtries--; de = dictGetRandomKey(db->dict); key = dictGetEntryKey(de); val = dictGetEntryVal(de); /* Only swap objects that are currently in memory. * * Also don't swap shared objects if threaded VM is on, as we * try to ensure that the main thread does not touch the * object while the I/O thread is using it, but we can't * control other keys without adding additional mutex. */ if (key->storage != REDIS_VM_MEMORY || (server.vm_max_threads != 0 && val->refcount != 1)) { if (maxtries) i--; /* don't count this try */ continue; } val->vm.atime = key->vm.atime; /* atime is updated on key object */ swappability = computeObjectSwappability(val); if (!best || swappability > best_swappability) { best = de; best_swappability = swappability; best_db = db; } } } if (best == NULL) return REDIS_ERR; key = dictGetEntryKey(best); val = dictGetEntryVal(best); redisLog(REDIS_DEBUG,"Key with best swappability: %s, %f", key->ptr, best_swappability); /* Unshare the key if needed */ if (key->refcount > 1) { robj *newkey = dupStringObject(key); decrRefCount(key); key = dictGetEntryKey(best) = newkey; } /* Swap it */ if (usethreads) { vmSwapObjectThreaded(key,val,best_db); return REDIS_OK; } else { if (vmSwapObjectBlocking(key,val) == REDIS_OK) { dictGetEntryVal(best) = NULL; return REDIS_OK; } else { return REDIS_ERR; } } } vmSwapObjectBlocking会在计算所需的交换页后，阻塞性的将value写到vm文件中（函数vmWriteObjectOnSwap），最后标记相应vm页为已使用。 static int vmSwapObjectBlocking(robj *key, robj *val) { off_t pages = rdbSavedObjectPages(val,NULL); off_t page; assert(key->storage == REDIS_VM_MEMORY); assert(key->refcount == 1); if (vmFindContiguousPages(&page,pages) == REDIS_ERR) return REDIS_ERR; if (vmWriteObjectOnSwap(val,page) == REDIS_ERR) return REDIS_ERR; key->vm.page = page; key->vm.usedpages = pages; key->storage = REDIS_VM_SWAPPED; key->vtype = val->type; decrRefCount(val); /* Deallocate the object from memory. */ vmMarkPagesUsed(page,pages); redisLog(REDIS_DEBUG,"VM: object %s swapped out at %lld (%lld pages)", (unsigned char*) key->ptr, (unsigned long long) page, (unsigned long long) pages); server.vm_stats_swapped_objects++; server.vm_stats_swapouts++; return REDIS_OK; } 对于value的加载，如果是多线程方式，会使用blockClientOnSwappedKeys提前加载，但阻塞方式则只有到相应命令执行时才会加载。最终无论是阻塞方式还是多线程方式，都会调用lookupKey来查找key是否在内存中，若不在，则使用vmLoadObject加载value，该函数是阻塞式的读入value。 static robj *lookupKey(redisDb *db, robj *key) { dictEntry *de = dictFind(db->dict,key); if (de) { robj *key = dictGetEntryKey(de); robj *val = dictGetEntryVal(de); if (server.vm_enabled) { if (key->storage == REDIS_VM_MEMORY || key->storage == REDIS_VM_SWAPPING) { /* If we were swapping the object out, stop it, this key * was requested. */ if (key->storage == REDIS_VM_SWAPPING) vmCancelThreadedIOJob(key); /* Update the access time of the key for the aging algorithm. */ key->vm.atime = server.unixtime; } else { int notify = (key->storage == REDIS_VM_LOADING); /* Our value was swapped on disk. Bring it at home. */ redisAssert(val == NULL); val = vmLoadObject(key); dictGetEntryVal(de) = val; /* Clients blocked by the VM subsystem may be waiting for * this key... */ if (notify) handleClientsBlockedOnSwappedKey(db,key); } } return val; } else { return NULL; } }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析23–VM（上）]]></title>
      <url>%2F116%2F</url>
      <content type="text"><![CDATA[VM是Redis2.0新增的一个功能。在没有VM之前，redis会把db中的所有数据放在内存中。随着redis的不断运行，所使用的内存会越来越大。但同时，client对某些数据的访问频度明显会比其他数据高。redis引入VM功能来试图解决这个问题。简言之，VM使得redis会把很少访问的value保存到磁盘中。但同时，所有value的key都放在内存中，这是为了让被换出的value的查找在启用VM前后性能差不多。 VM在redis中算是redis中最复杂的模块之一，我们分三节来介绍。这一节介绍redis的主要数据结构，下一节介绍非阻塞方式，最后一节介绍多线程方式。 我们先来看看redis中的通用对象结构redisObject ： // VM启用时, 对象所处位置 #define REDIS_VM_MEMORY 0 /* The object is on memory */ #define REDIS_VM_SWAPPED 1 /* The object is on disk */ #define REDIS_VM_SWAPPING 2 /* Redis is swapping this object on disk */ #define REDIS_VM_LOADING 3 /* Redis is loading this object from disk */ /* The VM object structure */ struct redisObjectVM { off_t page; /* the page at witch the object is stored on disk */ off_t usedpages; /* number of pages used on disk */ time_t atime; /* Last access time */ } vm; /* The actual Redis Object */ // 通用类型 // 对于key，需额外标志保存value的位置、类型等 typedef struct redisObject { void *ptr; unsigned char type; unsigned char encoding; unsigned char storage; /* If this object is a key, where is the value? * REDIS_VM_MEMORY, REDIS_VM_SWAPPED, ... */ unsigned char vtype; /* If this object is a key, and value is swapped out, * this is the type of the swapped out object. */ int refcount; /* VM fields, this are only allocated if VM is active, otherwise the * object allocation function will just allocate * sizeof(redisObjct) minus sizeof(redisObjectVM), so using * Redis without VM active will not have any overhead. */ struct redisObjectVM vm; } robj; robj 中的type保存了对象的类型，如string、list、set等。storage保存了该key对象对应的value所处的位置：内存、磁盘、正在被换出到磁盘，正在加载。vtype表示该key对象所对应的value的类型。page和usedpages保存了该key对象所对应的 value，atime是value的最后一次访问时间。因此，当robj所表示的key对象的storage类型为REDIS_VM_SWAPPED 时，就表示该key的value已不在内存中，需从VM中page的位置加载该value，vaue的类型为vtype，大小为usedpages。 创建对象的时候，根据是否启用VM机制，来分配合适大小的robj对象大小。 static robj *createObject(int type, void *ptr) { --- else { if (server.vm_enabled) { pthread_mutex_unlock(&server.obj_freelist_mutex); o = zmalloc(sizeof(*o)); } else { o = zmalloc(sizeof(*o)-sizeof(struct redisObjectVM)); } } --- if (server.vm_enabled) { /* Note that this code may run in the context of an I/O thread * and accessing to server.unixtime in theory is an error * (no locks). But in practice this is safe, and even if we read * garbage Redis will not fail, as it's just a statistical info */ o->vm.atime = server.unixtime; o->storage = REDIS_VM_MEMORY; } return o; } VM的所有相关结构保存在redisServer 的如下几个字段中。 /* Global server state structure */ struct redisServer { --- /* Virtual memory state */ FILE *vm_fp; int vm_fd; off_t vm_next_page; /* Next probably empty page */ off_t vm_near_pages; /* Number of pages allocated sequentially */ unsigned char *vm_bitmap; /* Bitmap of free/used pages */ time_t unixtime; /* Unix time sampled every second. */ /* Virtual memory I/O threads stuff */ /* An I/O thread process an element taken from the io_jobs queue and * put the result of the operation in the io_done list. While the * job is being processed, it's put on io_processing queue. */ list *io_newjobs; /* List of VM I/O jobs yet to be processed */ list *io_processing; /* List of VM I/O jobs being processed */ list *io_processed; /* List of VM I/O jobs already processed */ list *io_ready_clients; /* Clients ready to be unblocked. All keys loaded */ pthread_mutex_t io_mutex; /* lock to access io_jobs/io_done/io_thread_job */ pthread_mutex_t obj_freelist_mutex; /* safe redis objects creation/free */ pthread_mutex_t io_swapfile_mutex; /* So we can lseek + write */ pthread_attr_t io_threads_attr; /* attributes for threads creation */ int io_active_threads; /* Number of running I/O threads */ int vm_max_threads; /* Max number of I/O threads running at the same time */ /* Our main thread is blocked on the event loop, locking for sockets ready * to be read or written, so when a threaded I/O operation is ready to be * processed by the main thread, the I/O thread will use a unix pipe to * awake the main thread. The followings are the two pipe FDs. */ int io_ready_pipe_read; int io_ready_pipe_write; /* Virtual memory stats */ unsigned long long vm_stats_used_pages; unsigned long long vm_stats_swapped_objects; unsigned long long vm_stats_swapouts; unsigned long long vm_stats_swapins; --- }; vm_fp 和vm_fd指向磁盘上的vm文件，通过这两个指针来读写vm文件。vm_bitmap管理着vm文件中每一页的分配与释放情况（每一项为0表示该页空闲，为1表示已使用）。每一页的大小通过vm-page-size来配置，页数通过vm-pages来配置。值得一提的是，redis中的每一页最多只能放置一个对象，一个对象可以放在连续的多个页上。unixtime只是缓存时间值，这在计算value的最近使用频率时会用到。接下来的结构跟多线程方式换出/换进vlue有关。使用多线程方式时，换进/换出value被看成一个个的job，job的类型有如下几种： /* VM threaded I/O request message */ #define REDIS_IOJOB_LOAD 0 /* Load from disk to memory */ #define REDIS_IOJOB_PREPARE_SWAP 1 /* Compute needed pages */ #define REDIS_IOJOB_DO_SWAP 2 /* Swap from memory to disk */ typedef struct iojob { int type; /* Request type, REDIS_IOJOB_* */ redisDb *db;/* Redis database */ robj *key; /* This I/O request is about swapping this key */ robj *val; /* the value to swap for REDIS_IOREQ_*_SWAP, otherwise this * field is populated by the I/O thread for REDIS_IOREQ_LOAD. */ off_t page; /* Swap page where to read/write the object */ off_t pages; /* Swap pages needed to save object. PREPARE_SWAP return val */ int canceled; /* True if this command was canceled by blocking side of VM */ pthread_t thread; /* ID of the thread processing this entry */ } iojob; 类型为REDIS_IOJOB_LOAD的job用来加载某个value，类型为REDIS_IOJOB_DO_SWAP的job就用来换出某个 value，在换出value之前，需要创建类型为REDIS_IOJOB_PREPARE_SWAP的job来计算所需的交换页数。 无论是上述3种中的哪一种，新建的job都会使用queueIOJob放在io_newjobs队列中，而线程入口函数IOThreadEntryPoint 会将io_newjobs中的job移入server.io_processing，然后在做完job类型的工作后（加载value/计算value所需交换页数/换出value），将job从server.io_processing移入io_processed中。然后往 server.io_ready_pipe_write所在的管道（io_ready_pipe_read、io_ready_pipe_write组成管道的两端）写入一个字节，让睡眠中的vmThreadedIOCompletedJob继续运行，该函数会做些后续工作。 io_ready_clients保存了可以继续运行的client链表（之前因为等待value已阻塞），后面几个结构跟多线程的保护和全局的vm统计有关。 VM的初始化在vmInit中，主要做的工作就是上面介绍的几个结构的初始化。除此之外，最重要的工作就是设置管道的read事件的处理函数vmThreadedIOCompletedJob，该函数会在管道可读时运行，跟多线程的运行密切相关。 static void vmInit(void) { off_t totsize; int pipefds[2]; size_t stacksize; struct flock fl; if (server.vm_max_threads != 0) zmalloc_enable_thread_safeness(); /* we need thread safe zmalloc() */ redisLog(REDIS_NOTICE,"Using '%s' as swap file",server.vm_swap_file); /* Try to open the old swap file, otherwise create it */ if ((server.vm_fp = fopen(server.vm_swap_file,"r+b")) == NULL) { server.vm_fp = fopen(server.vm_swap_file,"w+b"); } if (server.vm_fp == NULL) { redisLog(REDIS_WARNING, "Can't open the swap file: %s. Exiting.", strerror(errno)); exit(1); } server.vm_fd = fileno(server.vm_fp); /* Lock the swap file for writing, this is useful in order to avoid * another instance to use the same swap file for a config error. */ fl.l_type = F_WRLCK; fl.l_whence = SEEK_SET; fl.l_start = fl.l_len = 0; if (fcntl(server.vm_fd,F_SETLK,&fl) == -1) { redisLog(REDIS_WARNING, "Can't lock the swap file at '%s': %s. Make sure it is not used by another Redis instance.", server.vm_swap_file, strerror(errno)); exit(1); } /* Initialize */ server.vm_next_page = 0; server.vm_near_pages = 0; server.vm_stats_used_pages = 0; server.vm_stats_swapped_objects = 0; server.vm_stats_swapouts = 0; server.vm_stats_swapins = 0; totsize = server.vm_pages*server.vm_page_size; redisLog(REDIS_NOTICE,"Allocating %lld bytes of swap file",totsize); if (ftruncate(server.vm_fd,totsize) == -1) { redisLog(REDIS_WARNING,"Can't ftruncate swap file: %s. Exiting.", strerror(errno)); exit(1); } else { redisLog(REDIS_NOTICE,"Swap file allocated with success"); } server.vm_bitmap = zmalloc((server.vm_pages+7)/8); redisLog(REDIS_VERBOSE,"Allocated %lld bytes page table for %lld pages", (long long) (server.vm_pages+7)/8, server.vm_pages); memset(server.vm_bitmap,0,(server.vm_pages+7)/8); /* Initialize threaded I/O (used by Virtual Memory) */ server.io_newjobs = listCreate(); server.io_processing = listCreate(); server.io_processed = listCreate(); server.io_ready_clients = listCreate(); pthread_mutex_init(&server.io_mutex,NULL); pthread_mutex_init(&server.obj_freelist_mutex,NULL); pthread_mutex_init(&server.io_swapfile_mutex,NULL); server.io_active_threads = 0; if (pipe(pipefds) == -1) { redisLog(REDIS_WARNING,"Unable to intialized VM: pipe(2): %s. Exiting." ,strerror(errno)); exit(1); } server.io_ready_pipe_read = pipefds[0]; server.io_ready_pipe_write = pipefds[1]; redisAssert(anetNonBlock(NULL,server.io_ready_pipe_read) != ANET_ERR); /* LZF requires a lot of stack */ pthread_attr_init(&server.io_threads_attr); pthread_attr_getstacksize(&server.io_threads_attr, &stacksize); /* Solaris may report a stacksize of 0, let's set it to 1 otherwise 115 * multiplying it by 2 in the while loop later will not really help ;) */ if (!stacksize) stacksize = 1; while (stacksize < REDIS_THREAD_STACK_SIZE) stacksize *= 2; pthread_attr_setstacksize(&server.io_threads_attr, stacksize); /* Listen for events in the threaded I/O pipe */ if (aeCreateFileEvent(server.el, server.io_ready_pipe_read, AE_READABLE, vmThreadedIOCompletedJob, NULL) == AE_ERR) oom("creating file event"); }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析22–协议]]></title>
      <url>%2F114%2F</url>
      <content type="text"><![CDATA[redis默认使用tcp协议的6379端口，其协议是文本行格式的而不是二进制格式的，每一行都以”\r\n”结尾，非常容易理解。 参考ProtocolSpecification.html就知道，发布到redis的命令有如下几种返回格式（对于不存在的值，会返回-1，此时client library应返回合适的nil对象（比如C语言的NULL），而不是空字符串）： 1）第一个字节是字符“-”，后面跟着一行出错信息（error reply） 比如lpop命令，当操作的对象不是一个链表时，会返回如下出错信息： “-ERR Operation against a key holding the wrong kind of value\r\n” 2）第一个字节是字符“+”，后面跟着一行表示执行结果的提示信息（line reply） 比如set命令执行成功后，会返回”+OK\r\n” 3）第一个字节是字符“$”，后面先跟一行，仅有一个数字，该数字表示下一行字符的个数（若不存在，则数字为-1）（bulk reply） 比如get命令，成功时返回值的类似于“$7\r\nmyvalue”，不存在时返回的信息为“$-1\r\n” 4）第一个字节是字符“*”，后面先跟一行，仅有一个数字，该数字表示bulk reply的个数（若不存在，则数字为-1）（multi-bulk reply） 比如lrange命令，若要求返回0–2之间的值，则成功时返回值类似于”3\r\n$6\r\nvalue1\r\n$7\r\nmyvalue\r\n$5\r\nhello\r\n”，不存在时返回的信息类似于”-1\r\n”。 5）第一个字节是字符“:”，后面跟着一个整数值（integer reply） 比如incr命令，成功时会返回对象+1后的值。 而client发布命令的格式有如下几种，第一个字符串都必须是命令字，不同的参数之间用1个空格来分隔：1）Inline Command:：仅一行 比如 EXISTS命令，client发送的字节流类似于”EXISTS mykey\r\n”。2）Bulk Command：类似于返回协议的bulk reply，一般有两行，第一行依次为“命令字 参数 一个数字”，该数字表示下一行字符的个数 比如SET命令，client发送的字节流类似于”SET mykey 5\r\nhello\r\n”。 3）multi-bulk Command：跟返回协议的multi-bulk reply类似。 比如上面的SET命令，用multi-bulk协议表示则为“*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$5\r\nhello\r\n”。 尽管对于某些命令该协议发送的字节流比bulk command形式要多，但它可以支持任何一种命令，支持跟多个二进制安全参数的命令（bulk command仅支持一个），也可以使得client library不修改代码就能支持redis新发布的命令（只要把不支持的命令按multi-bulk形式发布即可）。redis的官方文档中还提到，未来可能仅支持client采用multi-bulk Command格式发布命令。 另外提一下，client library可以连续发布多条命令，而不是等到redis返回前一条命令的执行结果才发布新的命令，这种机制被称作pipelining，支持redis的client library大多支持这种机制，读者可自行参考。 最后来看看redis实现时用来返回信息的相关函数。 redis 会使用addReplySds、addReplyDouble、addReplyLongLong、addReplyUlong、 addReplyBulkLen、addReplyBulk、addReplyBulkCString等来打包不同的返回信息，最终调用addReply 来发送信息。 addReply会将发送信息添加到相应redisClient的reply链表尾部，并使用 sendReplyToClient来发送。sendReplyToClient会遍历reply链表，并依次发送，其间如果可以打包 reply（server.glueoutputbuf为真），则可以使用glueReplyBuffersIfNeeded把reply链表中的值合并到一个缓冲区，然后一次性发送。 static void addReply(redisClient c, robj obj) { if (listLength(c-&gt;reply) == 0 &amp;&amp; (c-&gt;replstate == REDIS_REPL_NONE || c-&gt;replstate == REDIS_REPL_ONLINE) &amp;&amp; aeCreateFileEvent(server.el, c-&gt;fd, AE_WRITABLE, sendReplyToClient, c) == AE_ERR) return; if (server.vm_enabled &amp;&amp; obj-&gt;storage != REDIS_VM_MEMORY) { obj = dupStringObject(obj); obj-&gt;refcount = 0; /* getDecodedObject() will increment the refcount */ } listAddNodeTail(c-&gt;reply,getDecodedObject(obj)); } static void sendReplyToClient(aeEventLoop el, int fd, void privdata, int mask) { redisClient c = privdata; int nwritten = 0, totwritten = 0, objlen; robj o; REDIS_NOTUSED(el); REDIS_NOTUSED(mask); /* Use writev() if we have enough buffers to send */ if (!server.glueoutputbuf &amp;&amp; listLength(c-&gt;reply) &gt; REDIS_WRITEV_THRESHOLD &amp;&amp; !(c-&gt;flags &amp; REDIS_MASTER)) { sendReplyToClientWritev(el, fd, privdata, mask); return; } while(listLength(c-&gt;reply)) { if (server.glueoutputbuf &amp;&amp; listLength(c-&gt;reply) &gt; 1) glueReplyBuffersIfNeeded(c); o = listNodeValue(listFirst(c-&gt;reply)); objlen = sdslen(o-&gt;ptr); if (objlen == 0) { listDelNode(c-&gt;reply,listFirst(c-&gt;reply)); continue; } if (c-&gt;flags &amp; REDIS_MASTER) { /* Don&apos;t reply to a master */ nwritten = objlen - c-&gt;sentlen; } else { nwritten = write(fd, ((char*)o-&gt;ptr)+c-&gt;sentlen, objlen - c-&gt;sentlen); if (nwritten &lt;= 0) break; } c-&gt;sentlen += nwritten; totwritten += nwritten; /* If we fully sent the object on head go to the next one */ if (c-&gt;sentlen == objlen) { listDelNode(c-&gt;reply,listFirst(c-&gt;reply)); c-&gt;sentlen = 0; } /* Note that we avoid to send more thank REDIS_MAX_WRITE_PER_EVENT * bytes, in a single threaded server it&apos;s a good idea to serve * other clients as well, even if a very large request comes from * super fast link that is always able to accept data (in real world * scenario think about &apos;KEYS *&apos; against the loopback interfae) */ if (totwritten &gt; REDIS_MAX_WRITE_PER_EVENT) break; } if (nwritten == -1) { if (errno == EAGAIN) { nwritten = 0; } else { redisLog(REDIS_VERBOSE, &quot;Error writing to client: %s&quot;, strerror(errno)); freeClient(c); return; } } if (totwritten &gt; 0) c-&gt;lastinteraction = time(NULL); if (listLength(c-&gt;reply) == 0) { c-&gt;sentlen = 0; aeDeleteFileEvent(server.el,c-&gt;fd,AE_WRITABLE); } }关于client library的实现，可按照前面介绍的格式自己实现，也可以阅读现有的client library来加深理解。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析21– 事务]]></title>
      <url>%2F112%2F</url>
      <content type="text"><![CDATA[redis的事务较简单，并不具备事务的acid的全部特征。主要原因之一是redis事务中的命令并不是立即执行的，会一直排队到发布exec命令才执行所有的命令；另一个主要原因是它不支持回滚，事务中的命令可以部分成功，部分失败，命令失败时跟不在事务上下文执行时返回的信息类似。不知道在未来会不会提供更好的支持。 我们且来看看现在redis事务的实现。 redis中跟事务相关的主要结构如下所示。每个redisClient的multiState保存了事务上下文要执行的命令。 / Client MULTI/EXEC state /typedef struct multiCmd { robj *argv; int argc; struct redisCommand cmd;} multiCmd; typedef struct multiState { multiCmd commands; / Array of MULTI commands / int count; / Total number of MULTI commands */} multiState; typedef struct redisClient {multiState mstate; /* MULTI/EXEC state */ --- } redisClient;client通过发布multi命令进入事务上下文。处于事务上下文的client会设置REDIS_MULTI标志，multi命令会立即返回。 static void multiCommand(redisClient *c) { c-&gt;flags |= REDIS_MULTI; addReply(c,shared.ok);}处于事务上下文中的client会将在exec命令前发布的命令排队到mstate，并不立即执行相应命令且立即返回 shared.queued（如果之前参数检查不正确，则会返回出错信息，那就不会排队到mstate中），这在processCommand函数中反映出来（对processCommand的详细解释可参看前面命令处理章节）。queueMultiCommand只是简单的扩大mstate数组，并将当前命令加入其中。 static int processCommand(redisClient *c) { / Exec the command / if (c-&gt;flags &amp; REDIS_MULTI &amp;&amp; cmd-&gt;proc != execCommand &amp;&amp; cmd-&gt;proc != discardCommand) { queueMultiCommand(c,cmd); addReply(c,shared.queued); } else { if (server.vm_enabled &amp;&amp; server.vm_max_threads &gt; 0 &amp;&amp; blockClientOnSwappedKeys(c,cmd)) return 1; call(c,cmd); } --- }当client发布exec命令时，则redis会调用execCommand来执行事务上下文中的命令集合。注意，在此之前，redis会使用execBlockClientOnSwappedKeys提前加载其命令集所需的key（该函数最终是调用前面介绍过的 waitForMultipleSwappedKeys来加载key）。因为这在命令表cmdTable是这样设置的： {“exec”,execCommand,1,REDIS_CMD_INLINE|REDIS_CMD_DENYOOM,execBlockClientOnSwappedKeys,0,0,0},execCommand会检查是不是处于事务上下文，然后使用execCommandReplicateMulti向 slave/monitor/aof（前提是使用这些功能）发送/写入multi命令字，因为multi命令本身没有排队，而execCommand会在执行完后写入exec命令的，必须让exec和multi命令配对，这之后就是调用call依次执行每个命令了。从这里没有检查call的返回就可以看出，如果命令执行失败了，只能由call命令本身返回出错信息，这里并不检查命令执行的成功与否，最后就是清空mstate中的命令字并取消 REDIS_MULTI状态了。 static void execCommand(redisClient c) { int j; robj *orig_argv; int orig_argc; if (!(c-&gt;flags &amp; REDIS_MULTI)) { addReplySds(c,sdsnew(&quot;-ERR EXEC without MULTI\r\n&quot;)); return; } /* Replicate a MULTI request now that we are sure the block is executed. * This way we&apos;ll deliver the MULTI/..../EXEC block as a whole and * both the AOF and the replication link will have the same consistency * and atomicity guarantees. */ execCommandReplicateMulti(c); /* Exec all the queued commands */ orig_argv = c-&gt;argv; orig_argc = c-&gt;argc; addReplySds(c,sdscatprintf(sdsempty(),&quot;*%d\r\n&quot;,c-&gt;mstate.count)); for (j = 0; j &lt; c-&gt;mstate.count; j++) { c-&gt;argc = c-&gt;mstate.commands[j].argc; c-&gt;argv = c-&gt;mstate.commands[j].argv; call(c,c-&gt;mstate.commands[j].cmd); } c-&gt;argv = orig_argv; c-&gt;argc = orig_argc; freeClientMultiState(c); initClientMultiState(c); c-&gt;flags &amp;= (~REDIS_MULTI); /* Make sure the EXEC command is always replicated / AOF, since we * always send the MULTI command (we can&apos;t know beforehand if the * next operations will contain at least a modification to the DB). */ server.dirty++; }最后稍微提一下，如果事务上下文执行过程中，redis突然down掉，也就是最后的exec命令没有写入，此时会让 slave/monitor/aof处于不正确的状态。redis会在重启后会检查到这一情况，这是在loadAppendOnlyFile中完成的。当然这一检测执行的前提是down掉前和重启后都使用aof进行持久化。redis在检测到这一情况后，会退出程序。用户可调用用redis-check- aof工具进行修复。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析20–发布/订阅]]></title>
      <url>%2F110%2F</url>
      <content type="text"><![CDATA[redis的发布/订阅（publish/subscribe）功能类似于传统的消息路由功能，发布者发布消息，订阅者接收消息，沟通发布者和订阅者之间的桥梁是订阅的channel或者pattern。发布者向指定的publish或者pattern发布消息，订阅者阻塞在订阅的channel或者pattern。可以看到，发布者不会指定哪个订阅者才能接收消息，订阅者也无法只接收特定发布者的消息。这种订阅者和发布者之间的关系是松耦合的，订阅者不知道是谁发布的消息，发布者也不知道谁会接收消息。 redis的发布/订阅功能主要通过SUBSCRIBE、UNSUBSCRIBE、PSUBSCRIBE、PUNSUBSCRIBE 、PUBLISH五个命令来表现。其中SUBSCRIBE、UNSUBSCRIBE用于订阅或者取消订阅channel，而PSUBSCRIBE、PUNSUBSCRIBE用于订阅或者取消订阅pattern，发布消息则通过publish命令。 对于发布/订阅功能的实现，我们先来看看几个与此相关的结构。 struct redisServer { / Pubsub / dict pubsub_channels;/ Map channels to list of subscribed clients */ list pubsub_patterns;/ A list of pubsub_patterns */} typedef struct redisClient { dict pubsub_channels; / channels a client is interested in (SUBSCRIBE) / list pubsub_patterns; / patterns a client is interested in (SUBSCRIBE) /} redisClient;在redis的全局server变量（redisServer类型）中，channel和订阅者之间的关系用字典pubsub_channels来保存，特定channel和所有订阅者组成的链表构成pubsub_channels字典中的一项，即字典中的每一项可表示为（channel，订阅者链表）；pattern和订阅者之间的关系用链表pubsub_patterns来保存，链表中的每一项可表示成（pattern，redisClient）组成的字典。 在特定订阅者redisClient的结构中，pubsub_channels保存着它所订阅的channel的字典，而订阅的模式则保存在链表pubsub_patterns中。 从上面的解释，我们再来看看订阅/发布命令的最坏时间复杂度（注意字典增删查改一项的复杂度为O(1)，而链表的查删复杂度为O(N)，从链表尾部增加一项的复杂度为O(1)）。 SUBSCRIBE： 订阅者用SUBSCRIBE订阅特定channel，这需要在订阅者的redisClient结构中的pubsub_channels增加一项（复杂度为 O(1)），然后在redisServer 的pubsub_channels找到该channel（复杂度为O(1)），并在该channel的订阅者链表的尾部增加一项（复杂度为O(1)，注意，如果pubsub_channels中没找到该channel，则插入的复杂度也同为O(1)），因此订阅者用SUBSCRIBE订阅特定 channel的最坏时间复杂度为O(1)。 UNSUBSCRIBE： 订阅者取消订阅时，需要先从订阅者的redisClient结构中的pubsub_channels删除一项（复杂度为O(1)），然后在 redisServer 的pubsub_channels找到该channel（复杂度为O(1)），然后在channel的订阅者链表中删除该订阅者（复杂度为O(1)），因此总的复杂度为O(N)，N为特定channel的订阅者数。 PSUBSCRIBE： 订阅者用PSUBSCRIBE订阅pattern时，需要先在redisClient结构中的pubsub_patterns先查找是否已存在该 pattern（复杂度为O(N)），并在不存在的情况下往redisClient结构中的pubsub_patterns和redisServer结构中的pubsub_patterns链表尾部各增加一项（复杂度都为O(1)），因此，总的复杂度为O(N)，其中N为订阅者已订阅的模式。 PUNSUBSCRIBE： 订阅者用PUNSUBSCRIBE取消对pattern的订阅时，需要先在redisClient结构中的pubsub_patterns链表中删除该 pattern（复杂度为O(N)），并在redisServer结构中的pubsub_patterns链表中删除订阅者和pattern组成的映射（复杂度为O(M），因此，总的复杂度为O(N+M)，其中N为订阅者已订阅的模式，而M为系统中所有订阅者和所有pattern组成的映射数。 PUBLISH： 发布消息时，只会向特定channel发布，但该channel可能会匹配某个pattern。因此，需要先在redisServer结构中的 pubsub_channels找到该channel的订阅者链表（O(1)），然后发送给所有订阅者（复杂度为O(N)），然后查看 redisServer结构中的pubsub_patterns链表中的所有项，看channel是否和该项中的pattern匹配（复杂度为O(M)）（注意，这并不包括模式匹配的复杂度），因此，总的复杂度为O(N+M)，。其中N为该channel的订阅者数，而M为系统中所有订阅者和所有 pattern组成的映射数。另外，从这也可以看出，一个订阅者是可能多次收到同一个消息的。 解释了发布/订阅的算法后，其代码就好理解了，这里仅给出PUBLISH命令的处理函数publishCommand的代码，更多相关命令的代码请参看redis的源代码。 static void publishCommand(redisClient *c) { int receivers = pubsubPublishMessage(c-&gt;argv[1],c-&gt;argv[2]); addReplyLongLong(c,receivers);} / Publish a message /static int pubsubPublishMessage(robj channel, robj message) { int receivers = 0; struct dictEntry de; listNode ln; listIter li; /* Send to clients listening for that channel */ de = dictFind(server.pubsub_channels,channel); if (de) { list *list = dictGetEntryVal(de); listNode *ln; listIter li; listRewind(list,&amp;li); while ((ln = listNext(&amp;li)) != NULL) { redisClient *c = ln-&gt;value; addReply(c,shared.mbulk3); addReply(c,shared.messagebulk); addReplyBulk(c,channel); addReplyBulk(c,message); receivers++; } } /* Send to clients listening to matching channels */ if (listLength(server.pubsub_patterns)) { listRewind(server.pubsub_patterns,&amp;li); channel = getDecodedObject(channel); while ((ln = listNext(&amp;li)) != NULL) { pubsubPattern *pat = ln-&gt;value; if (stringmatchlen((char*)pat-&gt;pattern-&gt;ptr, sdslen(pat-&gt;pattern-&gt;ptr), (char*)channel-&gt;ptr, sdslen(channel-&gt;ptr),0)) { addReply(pat-&gt;client,shared.mbulk4); addReply(pat-&gt;client,shared.pmessagebulk); addReplyBulk(pat-&gt;client,pat-&gt;pattern); addReplyBulk(pat-&gt;client,channel); addReplyBulk(pat-&gt;client,message); receivers++; } } decrRefCount(channel); } return receivers; }最后提醒一下，处于发布/订阅模式的client，是无法发布上述五种命令之外的命令（quit除外），这是在processCommand函数中检查的，可以参看前面命令处理章节对该函数的解释。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析19–主从复制]]></title>
      <url>%2F108%2F</url>
      <content type="text"><![CDATA[先说下Redis主从复制的特点。 官方文档ReplicationHowto中提到以下特点：1. 一个master支持多个slave2. slave可以接受其他slave的连接，作为其他slave的master，从而形成一个master-slave的多级结构3. 复制在master端是非阻塞的，也就是master在向client复制时可处理其他client的命令，而slave在第一次同步时是阻塞的4. 复制被利用来提供可扩展性，比如可以将slave端用作数据冗余，也可以将耗时的命令（比如sort）发往某些slave从而避免master的阻塞，另外也可以用slave做持久化，这只需要将master的配置文件中的save指令注释掉。 client可以在一开始时作为slave连接master，也可以在运行后发布sync命令，从而跟master建立主从关系。 接下来我们分别从slave和master的视角概述下redis的主从复制的运行机制。 如果redis作为slave运行，则全局变量server.replstate的状态有REDIS_REPL_NONE（不处于复制状态）、 REDIS_REPL_CONNECT（需要跟master建立连接）、REDIS_REPL_CONNECTED（已跟master建立连接）三种。在读入slaveof配置或者发布slaveof命令后，server.replstate取值为REDIS_REPL_CONNECT，然后在syncWithMaster跟master执行第一次同步后，取值变为REDIS_REPL_CONNECTED。 如果redis作为master运行，则对应某个客户端连接的变量slave.replstate的状态有REDIS_REPL_WAIT_BGSAVE_START（等待bgsave运行）、REDIS_REPL_WAIT_BGSAVE_END（bgsave已dump db，该bulk传输了）、REDIS_REPL_SEND_BULK（正在bulk传输）、REDIS_REPL_ONLINE（已完成开始的bulk传输，以后只需发送更新了）。对于slave客户端（发布sync命令），一开始slave.replstate都处于REDIS_REPL_WAIT_BGSAVE_START状态（后面详解syncCommand函数），然后在后台dump db后（backgroundSaveDoneHandler函数），处于REDIS_REPL_WAIT_BGSAVE_END 状态，然后updateSlavesWaitingBgsave会将状态置为REDIS_REPL_SEND_BULK，并设置write事件的函数 sendBulkToSlave，在sendBulkToSlave运行后，状态就变为REDIS_REPL_ONLINE了，此后master会一直调用replicationFeedSlaves给处于REDIS_REPL_ONLINE状态的slave发送新命令。 我们先看处于master端的redis会执行的代码。 slave端都是通过发布sync命令来跟master同步的，sync命令的处理函数syncCommand如下所示。 该函数中的注释足够明了。如果slave的client设置了REDIS_SLAVE标志，说明master已用syncCommand处理了该 slave。如果master还有对这个client的reply没有发送，则返回出错信息。此后若server.bgsavechildpid != -1且有slave处于REDIS_REPL_WAIT_BGSAVE_END状态，则说明dump db的后台进程刚结束，此时新的slave可直接用保存的rdb进行bulk传输（注意复制reply参数，因为master是非阻塞的，此时可能执行了一些命令，call函数会调用replicationFeedSlaves函数将命令参数保存到slave的reply参数中）。如果没有slave处于REDIS_REPL_WAIT_BGSAVE_END状态，但server.bgsavechildpid != -1，则说明bgsave后台进程没有运行完，需要等待其结束（bgsave后台进程结束后会处理等待的slave）。如果server.bgsavechildpid 等于 -1，则需要启动一个后台进程来dump db了。最后将当前client加到master的slaves链表中。 static void syncCommand(redisClient *c) { /* ignore SYNC if aleady slave or in monitor mode */ if (c->flags & REDIS_SLAVE) return; /* SYNC can't be issued when the server has pending data to send to * the client about already issued commands. We need a fresh reply * buffer registering the differences between the BGSAVE and the current * dataset, so that we can copy to other slaves if needed. */ if (listLength(c->reply) != 0) { addReplySds(c,sdsnew("-ERR SYNC is invalid with pending input\r\n")); return; } redisLog(REDIS_NOTICE,"Slave ask for synchronization"); /* Here we need to check if there is a background saving operation * in progress, or if it is required to start one */ if (server.bgsavechildpid != -1) { /* Ok a background save is in progress. Let's check if it is a good * one for replication, i.e. if there is another slave that is * registering differences since the server forked to save */ redisClient *slave; listNode *ln; listIter li; listRewind(server.slaves,&li); while((ln = listNext(&li))) { slave = ln->value; if (slave->replstate == REDIS_REPL_WAIT_BGSAVE_END) break; } if (ln) { /* Perfect, the server is already registering differences for * another slave. Set the right state, and copy the buffer. */ listRelease(c->reply); c->reply = listDup(slave->reply); c->replstate = REDIS_REPL_WAIT_BGSAVE_END; redisLog(REDIS_NOTICE,"Waiting for end of BGSAVE for SYNC"); } else { /* No way, we need to wait for the next BGSAVE in order to * register differences */ c->replstate = REDIS_REPL_WAIT_BGSAVE_START; redisLog(REDIS_NOTICE,"Waiting for next BGSAVE for SYNC"); } } else { /* Ok we don't have a BGSAVE in progress, let's start one */ redisLog(REDIS_NOTICE,"Starting BGSAVE for SYNC"); if (rdbSaveBackground(server.dbfilename) != REDIS_OK) { redisLog(REDIS_NOTICE,"Replication failed, can't BGSAVE"); addReplySds(c,sdsnew("-ERR Unalbe to perform background save\r\n")); return; } c->replstate = REDIS_REPL_WAIT_BGSAVE_END; } c->repldbfd = -1; c->flags |= REDIS_SLAVE; c->slaveseldb = 0; listAddNodeTail(server.slaves,c); return; } 此后slave无论处于REDIS_REPL_WAIT_BGSAVE_START还是REDIS_REPL_WAIT_BGSAVE_END，都只能等 dump db的后台进程运行结束后才会被处理。该进程结束后会执行backgroundSaveDoneHandler函数，而该函数调用 updateSlavesWaitingBgsave来处理slaves。 updateSlavesWaitingBgsave和syncCommand一样，涉及到slave的几个状态变换。对于等待dump db的slave，master都会将其放入server.slaves 链表中。此时，若slave->replstate == REDIS_REPL_WAIT_BGSAVE_START，说明当前dump db不是该slave需要的，redis需要重新启动后台进程来dump db。若slave->replstate == REDIS_REPL_WAIT_BGSAVE_END，则说明当前dump db正是该slave所需要的，此时设置slave的write事件的处理函数sendBulkToSlave。 static void updateSlavesWaitingBgsave(int bgsaveerr) { listNode *ln; int startbgsave = 0; listIter li; listRewind(server.slaves,&li); while((ln = listNext(&li))) { redisClient *slave = ln->value; if (slave->replstate == REDIS_REPL_WAIT_BGSAVE_START) { startbgsave = 1; slave->replstate = REDIS_REPL_WAIT_BGSAVE_END; } else if (slave->replstate == REDIS_REPL_WAIT_BGSAVE_END) { struct redis_stat buf; if (bgsaveerr != REDIS_OK) { freeClient(slave); redisLog(REDIS_WARNING,"SYNC failed. BGSAVE child returned an error"); continue; } if ((slave->repldbfd = open(server.dbfilename,O_RDONLY)) == -1 || redis_fstat(slave->repldbfd,&buf) == -1) { freeClient(slave); redisLog(REDIS_WARNING,"SYNC failed. Can't open/stat DB after BGSAVE: %s", strerror(errno)); continue; } slave->repldboff = 0; slave->repldbsize = buf.st_size; slave->replstate = REDIS_REPL_SEND_BULK; aeDeleteFileEvent(server.el,slave->fd,AE_WRITABLE); if (aeCreateFileEvent(server.el, slave->fd, AE_WRITABLE, sendBulkToSlave, slave) == AE_ERR) { freeClient(slave); continue; } } } if (startbgsave) { if (rdbSaveBackground(server.dbfilename) != REDIS_OK) { listIter li; listRewind(server.slaves,&li); redisLog(REDIS_WARNING,"SYNC failed. BGSAVE failed"); while((ln = listNext(&li))) { redisClient *slave = ln->value; if (slave->replstate == REDIS_REPL_WAIT_BGSAVE_START) freeClient(slave); } } } } sendBulkToSlave 的逻辑不复杂。它根据slave->repldbfd指向的db，先从dump后的rdb文件中读入db数据，然后发送。发送完后会删除write 事件，设置slave->replstate状态为REDIS_REPL_ONLINE，此后master就会在收到命令后调用call函数，然后使用replicationFeedSlaves同步更新该slave了。replicationFeedSlaves也是遍历slave链表，对处于REDIS_REPL_ONLINE状态的slave，发送当前命令及其参数。 static void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) { redisClient *slave = privdata; REDIS_NOTUSED(el); REDIS_NOTUSED(mask); char buf[REDIS_IOBUF_LEN]; ssize_t nwritten, buflen; if (slave->repldboff == 0) { /* Write the bulk write count before to transfer the DB. In theory here * we don't know how much room there is in the output buffer of the * socket, but in pratice SO_SNDLOWAT (the minimum count for output * operations) will never be smaller than the few bytes we need. */ sds bulkcount; bulkcount = sdscatprintf(sdsempty(),"$%lld\r\n",(unsigned long long) slave->repldbsize); if (write(fd,bulkcount,sdslen(bulkcount)) != (signed)sdslen(bulkcount)) { sdsfree(bulkcount); freeClient(slave); return; } sdsfree(bulkcount); } lseek(slave->repldbfd,slave->repldboff,SEEK_SET); buflen = read(slave->repldbfd,buf,REDIS_IOBUF_LEN); if (buflen]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析18–持久化之aof]]></title>
      <url>%2F106%2F</url>
      <content type="text"><![CDATA[Redis的aof功能的目的是在性能和持久化粒度上对持久化机制提供更好的支持。 快照方式持久化的粒度有时间（秒）和改变的key数两种，如果持久化的粒度较小，对性能会有较大的影响，因为每次都是dump整个db；如果持久化的粒度较大，则在指定时间内指定数目的数据的持久化无法保证。而aof持久化的粒度是每次会修改db数据的命令，因此粒度是最小的了，跟日志方式有点类似，由于仅记录一条命令，性能也最好。另外，跟日志类似，aof文件会越来越大，则可以通过执行BGREWRITEAOF命令在后台重建该文件。 我们先来看看redis如何记录命令的。 call函数是命令执行的函数（前面命令处理章节已详细介绍过该函数）。如果命令执行前后数据有修改，则server.dirty的取值会有变化。在启用了aof机制的情况下，call函数会调用feedAppendOnlyFile保存命令及其相关参数。 static void call(redisClient *c, struct redisCommand *cmd){ long long dirty; dirty = server.dirty; cmd->proc(c); dirty = server.dirty-dirty; if(server.appendonly && dirty) feedAppendOnlyFile(cmd,c->db->id,c->argv,c->argc); --- } feedAppendOnlyFile会首先检查当前命令所处的db是否跟前一条命令执行所处db一致。若不一致，则需要发布一条选择db的select命令，然后做些命令的转换工作（代码略去）。 紧接着，将命令参数所对应的buf保存到server.aofbuf中，该参数保存了一段时间内redis执行的命令及其参数，redis会在适当的时机将其刷到磁盘上的aof文件中；然后如果有后台重建aof文件，则也将该缓冲区保存到server.bgrewritebuf中，该缓冲区保存了重建aof文件的后台进程运行时redis所执行的命令及其参数，后台进程退出时需要将这些命令保存到重建文件中。 static void feedAppendOnlyFile(struct redisCommand *cmd, int dictid, robj **argv, int argc){ --- server.aofbuf = sdscatlen(server.aofbuf,buf,sdslen(buf)); --- if(server.bgrewritechildpid != -1) server.bgrewritebuf = sdscatlen(server.bgrewritebuf,buf,sdslen(buf)); sdsfree(buf); } 我们来看看server.aofbuf会在什么时机被刷新到磁盘aof文件中。 刷新采用的是flushAppendOnlyFile函数。该函数在beforeSleep中会被调用（事件处理章节已介绍过该函数），而该函数是在处理client事件之前执行执行的（事件循环函数aeMain是先执行beforesleep，然后执行aeProcessEvents），因此，server.aofbuf中的值会在向client发送响应之前刷新到磁盘上。 flushAppendOnlyFile调用write一次性写全部server.aofbuf缓冲区中的数据，并根据配置的同步策略，调用aof_fsync（对系统同步函数fsync的保证）进行同步，这样新的命令及其参数就被附加到aof文件当中了。 static void flushAppendOnlyFile(void){ time_t now; ssize_t nwritten; --- nwritten = write(server.appendfd,server.aofbuf,sdslen(server.aofbuf)); --- sdsfree(server.aofbuf); server.aofbuf = sdsempty(); /* Fsync if needed */ now = time(NULL); if(server.appendfsync == APPENDFSYNC_ALWAYS|| (server.appendfsync == APPENDFSYNC_EVERYSEC && now-server.lastfsync > 1)) { /* aof_fsync is defined as fdatasync() for Linux in order to avoid * flushing metadata. */ aof_fsync(server.appendfd);/* Let's try to get this data on the disk */ server.lastfsync = now; } } 接下来我们看看后台如何重建aof文件。 aof重建靠调用rewriteAppendOnlyFileBackground函数完成。查看该函数的调用关系就可以知道，该函数会在收到bgrewriteaof命令后执行，也会在收到config命令并且从不使用aof机制到开启aof机制时被调用，也会在运行redis的系统作为slave时，跟master建立连接后并在serverCron函数中执行syncWithMaster时调用。 rewriteAppendOnlyFileBackground重建aof的主要逻辑如下（代码略去）： 1）使用fork创建一个子进程 2）子进程调用rewriteAppendOnlyFile在一个临时文件里写能够反映当前db状态的数据和命令， 此时父进程会把这段时间内执行的能够改变当前db数据的命令放到server.bgrewritebuf中（参看前面对feedAppendOnlyFile的解释） 3）当子进程退出时，父进程收到信号，将上面的内存缓冲区中的数据flush到临时文件中，然后将临时文件rename成新的aof文件（backgroundRewriteDoneHandler）。 父进程会在serverCron函数中等待执行aof重写或者快照保存的子进程，代码如下： /* Check if a background saving or AOF rewrite in progress terminated */ if(server.bgsavechildpid != -1||server.bgrewritechildpid != -1){ int statloc; pid_t pid; if((pid = wait3(&statloc,WNOHANG,NULL))!= 0){ if(pid == server.bgsavechildpid){ backgroundSaveDoneHandler(statloc); } else { backgroundRewriteDoneHandler(statloc); } updateDictResizePolicy(); } } rewriteAppendOnlyFile将反映当前db状态的命令和参数写到一个临时文件中。该函数遍历db中的每条数据，redis中的db其实是一个大的hash表，每一条数据都用（key,val）来表示。从key可以知道val的类型（redis支持REDIS_STRING、REDIS_LIST、REDIS_SET、REDIS_ZSET、REDIS_HASH五种数据类型），然后解码val中的数据。写入时，按照客户端执行命令的形式写入。比如对于REDIS_STRING类型，则先写入"*3\r\n$3\r \nSET\r\n"，然后写入set的key，然后写入val；对于REDIS_LIST类型，将val强制转换为list类型后，先写入"*3\r \n$5\r\nRPUSH\r\n"，然后写入要操作的list的名字，然后写入list的第一个数据，循环前面3个步骤直到list遍历完；对于REDIS_SET类型，则对于每条数据先写入"*3\r\n$4\r\nSADD\r\n"；对于REDIS_ZSET类型，则对于每条数据先写入"*4\r\n$4\r\nZADD\r\n"；对于REDIS_HASH类型，则对于每条数据先写入"*4\r\n$4\r\nHSET\r\n"（代码简单但较琐碎，略去）。 最后我们介绍下redis启动时使用aof重建db的步骤。 启动时重建的关键是构建一个fake client，然后使用这个client向server发送从aof文件中读入的命令。 int loadAppendOnlyFile(char *filename){ --- fakeClient = createFakeClient(); while(1){ --- if(fgets(buf,sizeof(buf),fp)== NULL){ --- } // 解析buf为对应的命令及参数 // 查找命令 cmd = lookupCommand(argv[0]->ptr); --- // 执行命令 cmd->proc(fakeClient); --- } --- }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析17–持久化之快照]]></title>
      <url>%2F104%2F</url>
      <content type="text"><![CDATA[redis的持久化支持快照方式。快照方式会将整个db dump到磁盘上。 client 可以发布save/bgsave命令让server将db dump到磁盘上。其中bgsave会执行后台dump（新建子进程执行dump），而save是阻塞式的dump db，会影响其他client的命令执行。除了发布命令执行快照保存外，redis的serverCron也会按照配置的参数执行后台dump，另外 slave建立连接时，master也会执行一个后台dump，然后才发送数据给slave（这在主从复制一节中介绍）。 static int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) { --- /* Check if a background saving or AOF rewrite in progress terminated */ if (server.bgsavechildpid != -1 || server.bgrewritechildpid != -1) { --- } else { /* If there is not a background saving in progress check if * we have to save now */ time_t now = time(NULL); for (j = 0; j < server.saveparamslen; j++) { struct saveparam *sp = server.saveparams+j; if (server.dirty >= sp->changes && now-server.lastsave > sp->seconds) { redisLog(REDIS_NOTICE,"%d changes in %d seconds. Saving...", sp->changes, sp->seconds); rdbSaveBackground(server.dbfilename); break; } } } --- } 无论是新建子进程还是阻塞式的执行快照方式（新建子进程方式会先调用rdbSaveBackground），最终都会调用rdbSave来保存db。 在rdbSave中可以看到，redis是按type、key、val方式来保存db中的数据的。 rdbLoad是快照方式保存数据后server启动时加载数据的函数，是rdbSave的逆过程。 static int rdbSave(char *filename) { --- for (j = 0; j < server.dbnum; j++) { redisDb *db = server.db+j; --- /* Iterate this DB writing every entry */ while((de = dictNext(di)) != NULL) { robj *key = dictGetEntryKey(de); robj *o = dictGetEntryVal(de); time_t expiretime = getExpire(db,key); --- /* Save type, key, value */ if (rdbSaveType(fp,o->type) == -1) goto werr; if (rdbSaveStringObject(fp,key) == -1) goto werr; if (rdbSaveObject(fp,o) == -1) goto werr; --- } dictReleaseIterator(di); } --- /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. */ if (rename(tmpfile,filename) == -1) { redisLog(REDIS_WARNING,"Error moving temp DB file on the final destination: %s", strerror(errno)); unlink(tmpfile); return REDIS_ERR; } --- }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析16–阻塞式命令]]></title>
      <url>%2F102%2F</url>
      <content type="text"><![CDATA[redis现在只支持对list的阻塞式操作，相关的两个命令是brpop和blpop。 这两个命令在list中有元素时，跟普通的pop没有区别，弹出list的一个元素，然后返回。但在list没有元素时，会为redisClient设置REDIS_BLOCKED标志，然后client阻塞（设置REDIS_BLOCKED标志的redisClient会一直阻塞，参考命令处理章节），一直到新元素加入时（push操作的处理函数pushGenericCommand），才会返回。 这两个命令设置的处理函数brpopCommand和blpopCommand都会调用blockingPopGenericCommand。该函数在检查list中有元素后，会调用非阻塞的popGenericCommand来弹出一个元素，否则调用blockForKeys来处理阻塞的情况。 /* Blocking RPOP/LPOP */ static void blockingPopGenericCommand(redisClient *c, int where) { robj *o; long long lltimeout; time_t timeout; int j; /* Make sure timeout is an integer value */ if (getLongLongFromObjectOrReply(c,c->argv[c->argc-1],&lltimeout, "timeout is not an integer") != REDIS_OK) return; /* Make sure the timeout is not negative */ if (lltimeout < 0) { addReplySds(c,sdsnew("-ERR timeout is negative\r\n")); return; } for (j = 1; j < c->argc-1; j++) { o = lookupKeyWrite(c->db,c->argv[j]); if (o != NULL) { if (o->type != REDIS_LIST) { addReply(c,shared.wrongtypeerr); return; } else { list *list = o->ptr; if (listLength(list) != 0) { /* If the list contains elements fall back to the usual * non-blocking POP operation */ robj *argv[2], **orig_argv; int orig_argc; /* We need to alter the command arguments before to call * popGenericCommand() as the command takes a single key. */ orig_argv = c->argv; orig_argc = c->argc; argv[1] = c->argv[j]; c->argv = argv; c->argc = 2; /* Also the return value is different, we need to output * the multi bulk reply header and the key name. The * "real" command will add the last element (the value) * for us. If this souds like an hack to you it's just * because it is... */ addReplySds(c,sdsnew("*2\r\n")); addReplyBulk(c,argv[1]); popGenericCommand(c,where); /* Fix the client structure with the original stuff */ c->argv = orig_argv; c->argc = orig_argc; return; } } } } /* If we are inside a MULTI/EXEC and the list is empty the only thing * we can do is treating it as a timeout (even with timeout 0). */ if (c->flags & REDIS_MULTI) { addReply(c,shared.nullmultibulk); return; } /* If the list is empty or the key does not exists we must block */ timeout = lltimeout; if (timeout > 0) timeout += time(NULL); blockForKeys(c,c->argv+1,c->argc-2,timeout); } blockForKeys会在db->blockingkeys记下client和等待的key的对应关系，然后给client设置REDIS_BLOCKED标志，这样client就一直阻塞了。 static void blockForKeys(redisClient *c, robj **keys, int numkeys, time_t timeout) { dictEntry *de; list *l; int j; --- if (c->fd < 0) return; c->blockingkeys = zmalloc(sizeof(robj*)*numkeys); c->blockingkeysnum = numkeys; c->blockingto = timeout; for (j = 0; j < numkeys; j++) { /* Add the key in the client structure, to map clients -> keys */ c->blockingkeys[j] = keys[j]; incrRefCount(keys[j]); /* And in the other "side", to map keys -> clients */ de = dictFind(c->db->blockingkeys,keys[j]); if (de == NULL) { int retval; /* For every key we take a list of clients blocked for it */ l = listCreate(); retval = dictAdd(c->db->blockingkeys,keys[j],l); incrRefCount(keys[j]); assert(retval == DICT_OK); } else { l = dictGetEntryVal(de); } listAddNodeTail(l,c); } /* Mark the client as a blocked client */ c->flags |= REDIS_BLOCKED; server.blpop_blocked_clients++; } 等待的client会一直阻塞，直到有push操作，此时会调用unblockClientWaitingData来解除client的阻塞。 /* Unblock a client that's waiting in a blocking operation such as BLPOP */ // 减少对所阻塞对象的引用 static void unblockClientWaitingData(redisClient *c) { dictEntry *de; list *l; int j; assert(c->blockingkeys != NULL); /* The client may wait for multiple keys, so unblock it for every key. */ for (j = 0; j < c->blockingkeysnum; j++) { /* Remove this client from the list of clients waiting for this key. */ de = dictFind(c->db->blockingkeys,c->blockingkeys[j]); assert(de != NULL); l = dictGetEntryVal(de); listDelNode(l,listSearchKey(l,c)); /* If the list is empty we need to remove it to avoid wasting memory */ if (listLength(l) == 0) dictDelete(c->db->blockingkeys,c->blockingkeys[j]); decrRefCount(c->blockingkeys[j]); } /* Cleanup the client structure */ zfree(c->blockingkeys); c->blockingkeys = NULL; c->flags &= (~REDIS_BLOCKED); server.blpop_blocked_clients--; /* We want to process data if there is some command waiting * in the input buffer. Note that this is safe even if * unblockClientWaitingData() gets called from freeClient() because * freeClient() will be smart enough to call this function * *after* c->querybuf was set to NULL. */ if (c->querybuf && sdslen(c->querybuf) > 0) processInputBuffer(c); }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析15–val加载机制]]></title>
      <url>%2F100%2F</url>
      <content type="text"><![CDATA[这一节主要介绍下val的加载。 对于某些命令，比如get somekey，当运行到processCommand时可能key对应的val不在内存中。在运行命令绑定的处理函数之前，redis会提前加载其val。 在 processCommand中，在vm开启并启用多线程时，会调用 blockClientOnSwappedKeys来加载可能已swap的val，如果blockClientOnSwappedKeys返回0，说明有 swap的val没被加载，则返回不调用call了（此时client会设置 REDIS_IO_WAIT标志，并已放到等待列表中）。代码如下： static int processCommand(redisClient *c) {if (server.vm_enabled &amp;&amp; server.vm_max_threads &gt; 0 &amp;&amp; blockClientOnSwappedKeys(c,cmd)) return 1; call(c,cmd); --- }在blockClientOnSwappedKeys函数中，如果命令设置了预加载函数，比如zunionstore和zinterstore就设置了预加载函数 zunionInterBlockClientOnSwappedKeys，则使用设置的预加载函数加载swap的val，否则使用 waitForMultipleSwappedKeys加载swap的val，不过查看 zunionInterBlockClientOnSwappedKeys和waitForMultipleSwappedKeys的实现就可以发现， 这些函数最终都调用waitForSwappedKey。在预加载函数返回后，若client的io_keys链表非空（io_keys是该client 不在内存中的val的链表），则设置client的REDIS_IO_WAIT标志，取消client的read 事件（在这之前，client已放到对应key的阻塞队列中了）。 static int blockClientOnSwappedKeys(redisClient c, struct redisCommand cmd) { if (cmd-&gt;vm_preload_proc != NULL) { cmd-&gt;vm_preload_proc(c,cmd,c-&gt;argc,c-&gt;argv); } else { waitForMultipleSwappedKeys(c,cmd,c-&gt;argc,c-&gt;argv); } /* If the client was blocked for at least one key, mark it as blocked. */ if (listLength(c-&gt;io_keys)) { c-&gt;flags |= REDIS_IO_WAIT; aeDeleteFileEvent(server.el,c-&gt;fd,AE_READABLE); server.vm_blocked_clients++; return 1; } else { return 0; } }我们看看waitForMultipleSwappedKeys的实现。waitForMultipleSwappedKeys会根据命令字表中设置的预加载参数，加载需要加载的val。 static void waitForMultipleSwappedKeys(redisClient c, struct redisCommand cmd, int argc, robj **argv) { int j, last; if (cmd-&gt;vm_firstkey == 0) return; last = cmd-&gt;vm_lastkey; if (last &lt; 0) last = argc+last; for (j = cmd-&gt;vm_firstkey; j &lt;= last; j += cmd-&gt;vm_keystep) { redisAssert(j &lt; argc); waitForSwappedKey(c,argv[j]); }}比如我们查看get命令字的设置。后面的1,1,1就是表示加载的val在argv中的位置，每个get命令最多需要预加载1个val。 {“get”,getCommand,2,REDIS_CMD_INLINE,NULL,1,1,1},waitForSwappedKey 涉及到vm的多线程，建议先粗略理解下，并在阅读vm章节后再返回此处阅读。该函数所做的主要工作就是将(c, key) 加到c-&gt;db-&gt;io_keys中，而db其实指向全局server的db，然后创建一个job，插入到工作线程中，让工作线程完成val 的加载。 static int waitForSwappedKey(redisClient c, robj key) { struct dictEntry de; robj o; list *l; /* If the key does not exist or is already in RAM we don&apos;t need to * block the client at all. */ de = dictFind(c-&gt;db-&gt;dict,key); if (de == NULL) return 0; o = dictGetEntryKey(de); if (o-&gt;storage == REDIS_VM_MEMORY) { return 0; } else if (o-&gt;storage == REDIS_VM_SWAPPING) { /* We were swapping the key, undo it! */ vmCancelThreadedIOJob(o); return 0; } /* OK: the key is either swapped, or being loaded just now. */ /* Add the key to the list of keys this client is waiting for. * This maps clients to keys they are waiting for. */ listAddNodeTail(c-&gt;io_keys,key); incrRefCount(key); /* Add the client to the swapped keys =&gt; clients waiting map. */ de = dictFind(c-&gt;db-&gt;io_keys,key); if (de == NULL) { int retval; /* For every key we take a list of clients blocked for it */ l = listCreate(); retval = dictAdd(c-&gt;db-&gt;io_keys,key,l); incrRefCount(key); assert(retval == DICT_OK); } else { l = dictGetEntryVal(de); } listAddNodeTail(l,c); /* Are we already loading the key from disk? If not create a job */ if (o-&gt;storage == REDIS_VM_SWAPPED) { iojob *j; o-&gt;storage = REDIS_VM_LOADING; j = zmalloc(sizeof(*j)); j-&gt;type = REDIS_IOJOB_LOAD; j-&gt;db = c-&gt;db; j-&gt;key = o; j-&gt;key-&gt;vtype = o-&gt;vtype; j-&gt;page = o-&gt;vm.page; j-&gt;val = NULL; j-&gt;canceled = 0; j-&gt;thread = (pthread_t) -1; lockThreadedIO(); queueIOJob(j); unlockThreadedIO(); } return 1; }插入工作线程的job在运行完后，会调用 vmThreadedIOCompletedJob，在该函数中会调用handleClientsBlockedOnSwappedKey处理阻塞的 client，而handleClientsBlockedOnSwappedKey所做的主要工作就是将所有val已加载的client放到 server.io_ready_clients中，此时client已ready好了，但还没有加入read事件循环（因为之前因为等待val已删除其 read事件）。 static void handleClientsBlockedOnSwappedKey(redisDb db, robj key) { struct dictEntry de; list l; listNode *ln; int len; de = dictFind(db-&gt;io_keys,key); if (!de) return; l = dictGetEntryVal(de); len = listLength(l); /* Note: we can&apos;t use something like while(listLength(l)) as the list * can be freed by the calling function when we remove the last element. */ while (len--) { ln = listFirst(l); redisClient *c = ln-&gt;value; if (dontWaitForSwappedKey(c,key)) { /* Put the client in the list of clients ready to go as we * loaded all the keys about it. */ listAddNodeTail(server.io_ready_clients,c); } } }最后还剩下一个问题，那就是处于server.io_ready_clients的clint会在什么时候增加read事件，从而继续让其接收客户端的输入了。这个工作在beforeSleep函数中完成（前面的事件循环中有详细介绍）。beforeSleep会为server.io_ready_clients中的client增加read事件，调用processInputBuffer处理其输入。 另外注意，如果client需要的val在检查时都在内存中，但当执行命令处理函数时，该val被swap出去了，则只能使用vmLoadObject直接加载了（阻塞方式）。对此种情况，redis的解释是In practical terms this should onlyhappen with SORT BY command or if there is a bug in this function（参考blockClientOnSwappedKeys前的注释）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析14–命令处理的一般过程]]></title>
      <url>%2F98%2F</url>
      <content type="text"><![CDATA[这个部分我们介绍下命令处理的一般过程。在createClient时，为client的read事件设置了readQueryFromClient函数。我们来看看怎么处理client的命令的。readQueryFromClient使用read一次读入REDIS_IOBUF_LEN字节，并保存在client中的querybuf参数中，然后调用processInputBuffer继续处理。 static void readQueryFromClient(aeEventLoop el, int fd, void privdata, int mask){ redisClient c =(redisClient)privdata; nread = read(fd, buf, REDIS_IOBUF_LEN); if(nread){ c-&gt;querybuf = sdscatlen(c-&gt;querybuf, buf, nread); c-&gt;lastinteraction = time(NULL); } else { return; } processInputBuffer(c);}processInputBuffer只处理不在REDIS_BLOCKED 和REDIS_IO_WAIT状态的client，也就是已ready好的client。另外如果c-&gt;bulklen ==-1（对于一般命令，c-&gt;bulklen都为-1，对于用multibulk协议传输的命令，下一个函数有更详细的介绍），则按行解析querybuf，并将解析到的参数保存在argv中，然后调用processCommand进行下一步处理，并且如果processCommand返回非0，会继续处理client输入。 static void processInputBuffer(redisClient *c) { again:if (c-&gt;flags &amp; REDIS_BLOCKED || c-&gt;flags &amp; REDIS_IO_WAIT) return; if (c-&gt;bulklen == -1) { -- if (p) { --- if (c-&gt;argc) { --- if (processCommand(c) &amp;&amp; sdslen(c-&gt;querybuf)) goto again; } -- } } else { -- int qbl = sdslen(c-&gt;querybuf); if (processCommand(c) &amp;&amp; sdslen(c-&gt;querybuf)) goto again; return; } }processCommand这段代码对multi-bulk 协议的解析写得真不敢恭维，转来转去的，真没劲。参看代码中的解释。解析完multibulk后，如果输入的命令是quit，则表示客户端退出了，释放其连接，返回0，表示不用继续处理了。接着使用 lookupCommand查看命令在cmdTable中对应的命令项，然后又是multbulk，接着检查安全认证情况，接着检查内存使用（前面内存章节中有介绍），接着查看 pubsub_channels、pubsub_patterns长度是否为0，若不为0，则表示处于订阅模式下（后文介绍），只允许命令 subscribeCommand、unsubscribeCommand、psubscribeCommand、 punsubscribeCommand。接着如果client处于事务模式下，则在命令不是execCommand、discardCommand的情况下将命令排队（事务处理后文也有介绍）。接着看看是否需要预先加载key，最后终于来到call函数中调用命令了。 static int processCommand(redisClient *c) { struct redisCommand *cmd; --- // 第一个字符是 * 表示后面是multi-bulk协议格式 // 解析得到后面的data 项数 if (c-&gt;multibulk == 0 &amp;&amp; c-&gt;argc == 1 &amp;&amp; ((char*)(c-&gt;argv[0]-&gt;ptr))[0] == &apos;*&apos;) { c-&gt;multibulk = atoi(((char*)c-&gt;argv[0]-&gt;ptr)+1); if (c-&gt;multibulk &lt;= 0) { resetClient(c); return 1; } else { decrRefCount(c-&gt;argv[c-&gt;argc-1]); c-&gt;argc--; return 1; } } else if (c-&gt;multibulk) { // 解析时对于普通的命令: c-&gt;bulklen始终 = -1, //前面已获得 c-&gt;multibulk值, c-&gt;bulklen一开始为-1，随后在 if (c-&gt;bulklen == -1) 中置为需要读取的字符个数，然后返回到processInputBuffer的else中处理得到输入的参数，然后再到这儿时就会进入 if (c-&gt;bulklen == -1) 的else中，将参数保存到mbargv中，这样一直到 c-&gt;multibulk为0，才解析完multibulk协议，进行下一步处理。 if (c-&gt;bulklen == -1) { if (((char*)c-&gt;argv[0]-&gt;ptr)[0] != &apos;$&apos;) { addReplySds(c,sdsnew(&quot;-ERR multi bulk protocol error\r\n&quot;)); resetClient(c); return 1; } else { int bulklen = atoi(((char*)c-&gt;argv[0]-&gt;ptr)+1); decrRefCount(c-&gt;argv[0]); if (bulklen &lt; 0 || bulklen &gt; 1024*1024*1024) { c-&gt;argc--; addReplySds(c,sdsnew(&quot;-ERR invalid bulk write count\r\n&quot;)); resetClient(c); return 1; } c-&gt;argc--; c-&gt;bulklen = bulklen+2; /* add two bytes for CR+LF */ return 1; } } else { c-&gt;mbargv = zrealloc(c-&gt;mbargv,(sizeof(robj*))*(c-&gt;mbargc+1)); c-&gt;mbargv[c-&gt;mbargc] = c-&gt;argv[0]; c-&gt;mbargc++; c-&gt;argc--; c-&gt;multibulk--; if (c-&gt;multibulk == 0) { robj **auxargv; int auxargc; /* Here we need to swap the multi-bulk argc/argv with the * normal argc/argv of the client structure. */ auxargv = c-&gt;argv; c-&gt;argv = c-&gt;mbargv; c-&gt;mbargv = auxargv; auxargc = c-&gt;argc; c-&gt;argc = c-&gt;mbargc; c-&gt;mbargc = auxargc; /* We need to set bulklen to something different than -1 * in order for the code below to process the command without * to try to read the last argument of a bulk command as * a special argument. */ c-&gt;bulklen = 0; /* continue below and process the command */ } else { c-&gt;bulklen = -1; return 1; } } } /* -- end of multi bulk commands processing -- */ --- if (!strcasecmp(c-&gt;argv[0]-&gt;ptr,&quot;quit&quot;)) { freeClient(c); return 0; } --- cmd = lookupCommand(c-&gt;argv[0]-&gt;ptr); if (!cmd) { addReplySds(c, sdscatprintf(sdsempty(), &quot;-ERR unknown command &apos;%s&apos;\r\n&quot;, (char*)c-&gt;argv[0]-&gt;ptr)); resetClient(c); return 1; } else if ((cmd-&gt;arity &gt; 0 &amp;&amp; cmd-&gt;arity != c-&gt;argc) || (c-&gt;argc &lt; -cmd-&gt;arity)) { addReplySds(c, sdscatprintf(sdsempty(), &quot;-ERR wrong number of arguments for &apos;%s&apos; command\r\n&quot;, cmd-&gt;name)); resetClient(c); return 1; } else if (cmd-&gt;flags &amp; REDIS_CMD_BULK &amp;&amp; c-&gt;bulklen == -1) { /* This is a bulk command, we have to read the last argument yet. */ int bulklen = atoi(c-&gt;argv[c-&gt;argc-1]-&gt;ptr); decrRefCount(c-&gt;argv[c-&gt;argc-1]); if (bulklen &lt; 0 || bulklen &gt; 1024*1024*1024) { c-&gt;argc--; addReplySds(c,sdsnew(&quot;-ERR invalid bulk write count\r\n&quot;)); resetClient(c); return 1; } c-&gt;argc--; c-&gt;bulklen = bulklen+2; /* add two bytes for CR+LF */ --- if ((signed)sdslen(c-&gt;querybuf) &gt;= c-&gt;bulklen) { c-&gt;argv[c-&gt;argc] = createStringObject(c-&gt;querybuf,c-&gt;bulklen-2); c-&gt;argc++; c-&gt;querybuf = sdsrange(c-&gt;querybuf,c-&gt;bulklen,-1); } else { /* Otherwise return... there is to read the last argument * from the socket. */ return 1; } } /* Let&apos;s try to encode the bulk object to save space. */ if (cmd-&gt;flags &amp; REDIS_CMD_BULK) c-&gt;argv[c-&gt;argc-1] = tryObjectEncoding(c-&gt;argv[c-&gt;argc-1]); /* Check if the user is authenticated */ if (server.requirepass &amp;&amp; !c-&gt;authenticated &amp;&amp; cmd-&gt;proc != authCommand) { addReplySds(c,sdsnew(&quot;-ERR operation not permitted\r\n&quot;)); resetClient(c); return 1; } if (server.maxmemory) freeMemoryIfNeeded(); if (server.maxmemory &amp;&amp; (cmd-&gt;flags &amp; REDIS_CMD_DENYOOM) &amp;&amp; zmalloc_used_memory() &gt; server.maxmemory) { addReplySds(c,sdsnew(&quot;-ERR command not allowed when used memory &gt; &apos;maxmemory&apos;\r\n&quot;)); resetClient(c); return 1; } /* Only allow SUBSCRIBE and UNSUBSCRIBE in the context of Pub/Sub */ if ((dictSize(c-&gt;pubsub_channels) &gt; 0 || listLength(c-&gt;pubsub_patterns) &gt; 0) &amp;&amp; cmd-&gt;proc != subscribeCommand &amp;&amp; cmd-&gt;proc != unsubscribeCommand &amp;&amp; cmd-&gt;proc != psubscribeCommand &amp;&amp; cmd-&gt;proc != punsubscribeCommand) { addReplySds(c,sdsnew(&quot;-ERR only (P)SUBSCRIBE / (P)UNSUBSCRIBE / QUIT allowed in this context\r\n&quot;)); resetClient(c); return 1; } /* Exec the command */ if (c-&gt;flags &amp; REDIS_MULTI &amp;&amp; cmd-&gt;proc != execCommand &amp;&amp; cmd-&gt;proc != discardCommand) { queueMultiCommand(c,cmd); addReply(c,shared.queued); } else { if (server.vm_enabled &amp;&amp; server.vm_max_threads &gt; 0 &amp;&amp; blockClientOnSwappedKeys(c,cmd)) return 1; call(c,cmd); } /* Prepare the client for the next command */ resetClient(c); return 1; }call函数首先调用命令字绑定的处理函数，返回时检查是否修改数据，若有修改，则在aof启用的情况下，写aof log，并在数据改变或者强制复制的情况下向slaves复制，最后向monitors发送当前命令及参数。 / Call() is the core of Redis execution of a command /static void call(redisClient c, struct redisCommand cmd) { long long dirty; dirty = server.dirty; cmd-&gt;proc(c); dirty = server.dirty-dirty; if (server.appendonly &amp;&amp; dirty) feedAppendOnlyFile(cmd,c-&gt;db-&gt;id,c-&gt;argv,c-&gt;argc); if ((dirty || cmd-&gt;flags &amp; REDIS_CMD_FORCE_REPLICATION) &amp;&amp; listLength(server.slaves)) replicationFeedSlaves(server.slaves,c-&gt;db-&gt;id,c-&gt;argv,c-&gt;argc); if (listLength(server.monitors)) replicationFeedMonitors(server.monitors,c-&gt;db-&gt;id,c-&gt;argv,c-&gt;argc); server.stat_numcommands++; }最后介绍下命令表，定义如下： struct redisCommand { char name; redisCommandProc proc; int arity; int flags; /* Use a function to determine which keys need to be loaded * in the background prior to executing this command. Takes precedence * over vm_firstkey and others, ignored when NULL */ redisVmPreloadProc *vm_preload_proc; /* What keys should be loaded in background when calling this command? */ int vm_firstkey; /* The first argument that&apos;s a key (0 = no keys) */ int vm_lastkey; /* THe last argument that&apos;s a key */ int vm_keystep; /* The step between first and last key */ };对于每一个命令字，都有一个name和一个处理函数，对于某些key，在启用vm的情况下，需要使用vm_preload_proc预先加载某些key。下一节我们介绍下key的预先加载。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析13–client连接（下）]]></title>
      <url>%2F96%2F</url>
      <content type="text"><![CDATA[这一节我们介绍下client连接的几个标志。 client的flags的取值有如下几种： #define REDIS_SLAVE 1 / This client is a slave server / #define REDIS_MASTER 2 / This client is a master server / #define REDIS_MONITOR 4 / This client is a slave monitor, see MONITOR / #define REDIS_MULTI 8 / This client is in a MULTI context / #define REDIS_BLOCKED 16 / The client is waiting in a blocking operation / #define REDIS_IO_WAIT 32 / The client is waiting for Virtual Memory I/O /redis支持主从复制、监控特性，对这些server的连接状态也是保存在当前server的redisClient结构中， 并在redisClient的flags标志中设置REDIS_SLAVE 、REDIS_MASTER、 REDIS_MONITOR参数。我们在后续的章节中详细分析redis的主从复制、监控等特性。 一般的client仅会设置REDIS_MULTI、REDIS_BLOCKED、REDIS_IO_WAIT中的1个或多个。REDIS_MULTI跟redis的事务支持相关，我们后续介绍。REDIS_BLOCKED标志跟redis支持的list阻塞式pop（BLPOP、BRPOP）有关，也就是当list为空的时候，会阻塞client，一直到有元素加入list，此时再pop。REDIS_IO_WAIT跟命令字有关，对于某些命令，如果启用vm的话，需要提前加载其key。我们在命令处理章节中分析。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析12–client连接（中）]]></title>
      <url>%2F94%2F</url>
      <content type="text"><![CDATA[这一节我们简略介绍下client连接的3个核心函数。涉及的很多参数跟redis的诸多特性有关，可在阅读后续章节后返回查看其相关值。 redis将新client的连接状态保存在redisClient结构体中，该结构体的代码如下： typedef struct redisClient { int fd; // client对应的文件描述符 redisDb *db; // 当前操作的db int dictid; // sds querybuf; // 请求字符串 robj **argv, **mbargv; // 解析请求后的参数数组（mbargv用于multi命令） int argc, mbargc; int bulklen; /* bulk read len. -1 if not in bulk read mode */ int multibulk; /* multi bulk command format active */ list *reply; int sentlen; time_t lastinteraction; /* time of the last interaction, used for timeout */ int flags; /* REDIS_SLAVE | REDIS_MONITOR | REDIS_MULTI ... */ int slaveseldb; /* slave selected db, if this client is a slave */ int authenticated; /* when requirepass is non-NULL */ int replstate; /* replication state if this is a slave */ int repldbfd; /* replication DB file descriptor */ long repldboff; /* replication DB file offset */ off_t repldbsize; /* replication DB file size */ multiState mstate; /* MULTI/EXEC state */ robj **blockingkeys; /* The key we are waiting to terminate a blocking * operation such as BLPOP. Otherwise NULL. */ int blockingkeysnum; /* Number of blocking keys */ time_t blockingto; /* Blocking operation timeout. If UNIX current time * is >= blockingto then the operation timed out. */ list *io_keys; /* Keys this client is waiting to be loaded from the * swap file in order to continue. */ dict *pubsub_channels; /* channels a client is interested in (SUBSCRIBE) */ list *pubsub_patterns; /* patterns a client is interested in (SUBSCRIBE) */ } redisClient; 新建client状态是用createClient函数完成，释放是用freeClient，超时是用closeTimedoutClients。 createClient 在上一节中的acceptHandler中被调用，一开始会将client设置成非阻塞、非延迟的，然后设置client有read事件时的处理函数 readQueryFromClient，接着默认选择db 0，清空命令缓冲区和参数数组等，最后将client加到全局的server.clients中。详细代码如下： static redisClient *createClient(int fd) { redisClient *c = zmalloc(sizeof(*c)); anetNonBlock(NULL,fd); anetTcpNoDelay(NULL,fd); if (!c) return NULL; if (aeCreateFileEvent(server.el,fd,AE_READABLE, readQueryFromClient, c) == AE_ERR) { close(fd); zfree(c); return NULL; } selectDb(c,0); c->fd = fd; c->querybuf = sdsempty(); c->argc = 0; c->argv = NULL; c->bulklen = -1; c->multibulk = 0; c->mbargc = 0; c->mbargv = NULL; c->sentlen = 0; c->flags = 0; c->lastinteraction = time(NULL); c->authenticated = 0; c->replstate = REDIS_REPL_NONE; c->reply = listCreate(); listSetFreeMethod(c->reply,decrRefCount); listSetDupMethod(c->reply,dupClientReplyValue); c->blockingkeys = NULL; c->blockingkeysnum = 0; c->io_keys = listCreate(); listSetFreeMethod(c->io_keys,decrRefCount); c->pubsub_channels = dictCreate(&setDictType,NULL); c->pubsub_patterns = listCreate(); listSetFreeMethod(c->pubsub_patterns,decrRefCount); listSetMatchMethod(c->pubsub_patterns,listMatchObjects); listAddNodeTail(server.clients,c); initClientMultiState(c); return c; } freeClient用于释放client连接状态，在redis接收的client数超过了配置的 server.maxclients、读写client信息出错时、client连接超时时，都会被调用。 static void freeClient(redisClient *c) { listNode *ln; /* Note that if the client we are freeing is blocked into a blocking * call, we have to set querybuf to NULL *before* to call * unblockClientWaitingData() to avoid processInputBuffer() will get * called. Also it is important to remove the file events after * this, because this call adds the READABLE event. */ // 释放querybuf sdsfree(c->querybuf); c->querybuf = NULL; if (c->flags & REDIS_BLOCKED) unblockClientWaitingData(c); /* Unsubscribe from all the pubsub channels */ // 释放channel pubsubUnsubscribeAllChannels(c,0); pubsubUnsubscribeAllPatterns(c,0); dictRelease(c->pubsub_channels); listRelease(c->pubsub_patterns); /* Obvious cleanup */ aeDeleteFileEvent(server.el,c->fd,AE_READABLE); aeDeleteFileEvent(server.el,c->fd,AE_WRITABLE); listRelease(c->reply); freeClientArgv(c); close(c->fd); /* Remove from the list of clients */ ln = listSearchKey(server.clients,c); redisAssert(ln != NULL); listDelNode(server.clients,ln); /* Remove from the list of clients waiting for swapped keys, or ready * to be restarted, but not yet woken up again. */ // 从io等待队列中删除client if (c->flags & REDIS_IO_WAIT) { redisAssert(server.vm_enabled); if (listLength(c->io_keys) == 0) { ln = listSearchKey(server.io_ready_clients,c); /* When this client is waiting to be woken up (REDIS_IO_WAIT), * it should be present in the list io_ready_clients */ redisAssert(ln != NULL); listDelNode(server.io_ready_clients,ln); } else { while (listLength(c->io_keys)) { ln = listFirst(c->io_keys); dontWaitForSwappedKey(c,ln->value); } } server.vm_blocked_clients--; } listRelease(c->io_keys); /* Master/slave cleanup */ if (c->flags & REDIS_SLAVE) { if (c->replstate == REDIS_REPL_SEND_BULK && c->repldbfd != -1) close(c->repldbfd); list *l = (c->flags & REDIS_MONITOR) ? server.monitors : server.slaves; ln = listSearchKey(l,c); redisAssert(ln != NULL); listDelNode(l,ln); } if (c->flags & REDIS_MASTER) { server.master = NULL; server.replstate = REDIS_REPL_CONNECT; } /* Release memory */ zfree(c->argv); zfree(c->mbargv); freeClientMultiState(c); zfree(c); } closeTimedoutClients在serverCron中被调用。 static void closeTimedoutClients(void) { redisClient *c; listNode *ln; time_t now = time(NULL); listIter li; listRewind(server.clients,&li); while ((ln = listNext(&li)) != NULL) { c = listNodeValue(ln); if (server.maxidletime && !(c->flags & REDIS_SLAVE) && /* no timeout for slaves */ !(c->flags & REDIS_MASTER) && /* no timeout for masters */ dictSize(c->pubsub_channels) == 0 && /* no timeout for pubsub */ listLength(c->pubsub_patterns) == 0 && (now - c->lastinteraction > server.maxidletime)) { redisLog(REDIS_VERBOSE,"Closing idle client"); freeClient(c); } else if (c->flags & REDIS_BLOCKED) { if (c->blockingto != 0 && c->blockingto < now) { addReply(c,shared.nullmultibulk); unblockClientWaitingData(c); } } } }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析11–client连接（上）]]></title>
      <url>%2F92%2F</url>
      <content type="text"><![CDATA[接下来的三节我们介绍client连接，按接受client连接、client连接的3个核心函数、client连接的几个标志3个部分顺序介绍。 这一节我们介绍下redis如何接受client连接。 在main函数中调用的initServer中，可看到如下代码： static void initServer() { --- server.fd = anetTcpServer(server.neterr, server.port, server.bindaddr); --- if (aeCreateFileEvent(server.el, server.fd, AE_READABLE, acceptHandler, NULL) == AE_ERR) oom("creating file event"); --- } anetTcpServer 也就是socket、bind、listen的封装，返回listen后的fd，并保存在全局的server.fd中，然后调用 aeCreateFileEvent，使server.fd监听read事件，并在read事件响应后（也就是有新的client来到时），调用 acceptHandler来处理。可以看到acceptHandler最终调用系统api accept来处理。在acceptHandler返回后（尽管acceptHandler仅等待一个client连接，但由于server.fd的read事件一直被监听，所以会在aeProcessEvents反复被处理，从而导致acceptHandler反复被调用），acceptHandler调用createClient返回一个redisClient结构，保存新的client连接的状态。如果客户端连接数过多，则向客户端返回出错信息，并释放连接。从这里可以看出，redis对client连接的处理完全是使用事件来处理的，没有多进程，没有多线程。 static void acceptHandler(aeEventLoop *el, int fd, void *privdata, int mask) { --- cfd = anetAccept(server.neterr, fd, cip, &cport); --- if ((c = createClient(cfd)) == NULL) { redisLog(REDIS_WARNING,"Error allocating resoures for the client"); close(cfd); /* May be already closed, just ingore errors */ return; } /* If maxclient directive is set and this is one client more... close the * connection. Note that we create the client instead to check before * for this condition, since now the socket is already set in nonblocking * mode and we can send an error for free using the Kernel I/O */ if (server.maxclients && listLength(server.clients) > server.maxclients) { char *err = "-ERR max number of clients reached\r\n"; /* That's a best effort error message, don't check write errors */ if (write(c->fd,err,strlen(err)) == -1) { /* Nothing to do, Just to avoid the warning... */ } freeClient(c); return; } server.stat_numconnections++; } int anetAccept(char *err, int serversock, char *ip, int *port) { int fd; struct sockaddr_in sa; unsigned int saLen; while(1) { saLen = sizeof(sa); fd = accept(serversock, (struct sockaddr*)&sa, &saLen); if (fd == -1) { if (errno == EINTR) continue; else { anetSetError(err, "accept: %s\n", strerror(errno)); return ANET_ERR; } } break; } if (ip) strcpy(ip,inet_ntoa(sa.sin_addr)); if (port) *port = ntohs(sa.sin_port); return fd; }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析10–事件处理（下）]]></title>
      <url>%2F89%2F</url>
      <content type="text"><![CDATA[serverCron做的工作很多，后续的很多章节都与此有关。该函数较复杂，分段分析。一开始将当前时间保存，方便后续vm等机制对当前时间的访问： /* We take a cached value of the unix time in the global state because * with virtual memory and aging there is to store the current time * in objects at every object access, and accuracy is not needed. * To access a global var is faster than calling time(NULL) */ // 缓存时间 server.unixtime = time(NULL); 接着，如果收到SIGTERM等信号，则会在信号处理函数中设置server.shutdown_asap为1，此时就会调用prepareForShutdown做结束运行前的结尾工作： /* We received a SIGTERM, shutting down here in a safe way, as it is * not ok doing so inside the signal handler. */ if (server.shutdown_asap) { if (prepareForShutdown() == REDIS_OK) exit(0); redisLog(REDIS_WARNING,"SIGTERM received but errors trying to shut down the server, check the logs for more information"); } 接着，显示些db中的信息： /* Show some info about non-empty databases */ for (j = 0; j < server.dbnum; j++) { long long size, used, vkeys; size = dictSlots(server.db[j].dict); used = dictSize(server.db[j].dict); vkeys = dictSize(server.db[j].expires); if (!(loops % 50) && (used || vkeys)) { redisLog(REDIS_VERBOSE,"DB %d: %lld keys (%lld volatile) in %lld slots HT.",j,used,vkeys,size); /* dictPrintStats(server.dict); */ } } 当没有进行save或者rewrite aof的后台子进程运行时，会调用tryResizeHashTables、incrementallyRehash，以分别调整db的大小和重新rehash db。当有后台子进程运行时，进行rehash会使得系统使用较多的内存。 /* We don't want to resize the hash tables while a bacground saving * is in progress: the saving child is created using fork() that is * implemented with a copy-on-write semantic in most modern systems, so * if we resize the HT while there is the saving child at work actually * a lot of memory movements in the parent will cause a lot of pages * copied. */ if (server.bgsavechildpid == -1 && server.bgrewritechildpid == -1) { if (!(loops % 10)) tryResizeHashTables(); if (server.activerehashing) incrementallyRehash(); } 接着，显示些client的信息，并关闭timeout的连接： /* Show information about connected clients */ if (!(loops % 50)) { redisLog(REDIS_VERBOSE,"%d clients connected (%d slaves), %zu bytes in use", listLength(server.clients)-listLength(server.slaves), listLength(server.slaves), zmalloc_used_memory()); } /* Close connections of timedout clients */ if ((server.maxidletime && !(loops % 100)) || server.blpop_blocked_clients) closeTimedoutClients(); 如果有后台子进程进行save或者rewrite aof的工作，此时等待其退出，并调用backgroundSaveDoneHandler或者backgroundRewriteDoneHandler做些后续工作，否则检查是否需要save db： /* Check if a background saving or AOF rewrite in progress terminated */ if (server.bgsavechildpid != -1 || server.bgrewritechildpid != -1) { int statloc; pid_t pid; if ((pid = wait3(&statloc,WNOHANG,NULL)) != 0) { if (pid == server.bgsavechildpid) { backgroundSaveDoneHandler(statloc); } else { backgroundRewriteDoneHandler(statloc); } updateDictResizePolicy(); } } else { /* If there is not a background saving in progress check if * we have to save now */ time_t now = time(NULL); for (j = 0; j < server.saveparamslen; j++) { struct saveparam *sp = server.saveparams+j; if (server.dirty >= sp->changes && now-server.lastsave > sp->seconds) { redisLog(REDIS_NOTICE,"%d changes in %d seconds. Saving...", sp->changes, sp->seconds); rdbSaveBackground(server.dbfilename); break; } } } 释放expire的key：释放时从expired链表中随机选择，如果循环中超时的key的数量超过了设置值（REDIS_EXPIRELOOKUPS_PER_CRON）的25%，则继续释放： /* Try to expire a few timed out keys. The algorithm used is adaptive and * will use few CPU cycles if there are few expiring keys, otherwise * it will get more aggressive to avoid that too much memory is used by * keys that can be removed from the keyspace. */ for (j = 0; j < server.dbnum; j++) { int expired; redisDb *db = server.db+j; /* Continue to expire if at the end of the cycle more than 25% * of the keys were expired. */ do { long num = dictSize(db->expires); time_t now = time(NULL); expired = 0; if (num > REDIS_EXPIRELOOKUPS_PER_CRON) num = REDIS_EXPIRELOOKUPS_PER_CRON; while (num--) { dictEntry *de; robj *key; time_t t; if ((de = dictGetRandomKey(db->expires)) == NULL) break; t = (time_t) dictGetEntryVal(de); key = dictGetEntryKey(de); /* Don't expire keys that are in the contest of I/O jobs. * Otherwise decrRefCount will kill the I/O thread and * clients waiting for this keys will wait forever. * * In general this change will not have any impact on the * performance of the expiring algorithm but it's much safer. */ if (server.vm_enabled && (key->storage == REDIS_VM_SWAPPING || key->storage == REDIS_VM_LOADING)) continue; if (now > t) { deleteKey(db,dictGetEntryKey(de)); expired++; server.stat_expiredkeys++; } } } while (expired > REDIS_EXPIRELOOKUPS_PER_CRON/4); } 如果内存超出阈值，则释放内存，前面的内存章节中已对此进行过分析： /* Swap a few keys on disk if we are over the memory limit and VM * is enbled. Try to free objects from the free list first. */ if (vmCanSwapOut()) { while (server.vm_enabled && zmalloc_used_memory() > server.vm_max_memory) { int retval; if (tryFreeOneObjectFromFreelist() == REDIS_OK) continue; retval = (server.vm_max_threads == 0) ? vmSwapOneObjectBlocking() : vmSwapOneObjectThreaded(); if (retval == REDIS_ERR && !(loops % 300) && zmalloc_used_memory() > (server.vm_max_memory+server.vm_max_memory/10)) { redisLog(REDIS_WARNING,"WARNING: vm-max-memory limit exceeded by more than 10%% but unable to swap more objects out!"); } /* Note that when using threade I/O we free just one object, * because anyway when the I/O thread in charge to swap this * object out will finish, the handler of completed jobs * will try to swap more objects if we are still out of memory. */ if (retval == REDIS_ERR || server.vm_max_threads > 0) break; } } 当redis作为slave时，检查我们是否连接Master： /* Check if we should connect to a MASTER */ if (server.replstate == REDIS_REPL_CONNECT && !(loops % 10)) { redisLog(REDIS_NOTICE,"Connecting to MASTER..."); if (syncWithMaster() == REDIS_OK) { redisLog(REDIS_NOTICE,"MASTER SLAVE sync succeeded"); if (server.appendonly) rewriteAppendOnlyFileBackground(); } } 返回100，表示在接下来的100ms内，serverCron会重新被调用： return 100; }]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析9–事件处理（中）]]></title>
      <url>%2F86%2F</url>
      <content type="text"><![CDATA[接下来，我们分析下redis中事件的处理逻辑。 在函数initServer中调用aeCreateEventLoop完成初始化后，在main函数中调用ae_main，该函数是一个死循环： static void initServer() {server.el = aeCreateEventLoop(); --- } int main(int argc, char **argv) { initServer(); aeSetBeforeSleepProc(server.el,beforeSleep); aeMain(server.el);} void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); aeProcessEvents(eventLoop, AE_ALL_EVENTS); }}尽管aeMain函数有退出条件，但除了基准测试中会调用aeStop修改该值，该条件不会被改变。 aeMain在处理event之前，先调用beforeSleep，该函数先处理已ready的client，然后刷新aof缓冲区（aof机制后续章节会详细分析）： static void beforeSleep(struct aeEventLoop *eventLoop) { REDIS_NOTUSED(eventLoop); /* Awake clients that got all the swapped keys they requested */ if (server.vm_enabled &amp;&amp; listLength(server.io_ready_clients)) { listIter li; listNode *ln; listRewind(server.io_ready_clients,&amp;li); while((ln = listNext(&amp;li))) { redisClient *c = ln-&gt;value; struct redisCommand *cmd; /* Resume the client. */ listDelNode(server.io_ready_clients,ln); c-&gt;flags &amp;= (~REDIS_IO_WAIT); server.vm_blocked_clients--; aeCreateFileEvent(server.el, c-&gt;fd, AE_READABLE, readQueryFromClient, c); cmd = lookupCommand(c-&gt;argv[0]-&gt;ptr); assert(cmd != NULL); call(c,cmd); resetClient(c); /* There may be more data to process in the input buffer. */ if (c-&gt;querybuf &amp;&amp; sdslen(c-&gt;querybuf) &gt; 0) processInputBuffer(c); } } /* Write the AOF buffer on disk */ flushAppendOnlyFile(); }aeMain调用aeProcessEvents处理文件事件和timer事件。aeProcessEvents 先获得最先超时的timer，并记下该timer距此时的时间段，将该时间段作为aeApiPoll的超时时间（以能尽快调用timer处理，因为是先处理file事件，后处理timer事件），aeApiPoll返回后将调用注册的read、write函数进行读写： int aeProcessEvents(aeEventLoop *eventLoop, int flags){ int processed = 0, numevents; /* Nothing to do? return ASAP */ if (!(flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_FILE_EVENTS)) return 0; /* Note that we want call select() even if there are no * file events to process as long as we want to process time * events, in order to sleep until the next time event is ready * to fire. */ if (eventLoop-&gt;maxfd != -1 || ((flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_DONT_WAIT))) { int j; aeTimeEvent *shortest = NULL; struct timeval tv, *tvp; if (flags &amp; AE_TIME_EVENTS &amp;&amp; !(flags &amp; AE_DONT_WAIT)) shortest = aeSearchNearestTimer(eventLoop); if (shortest) { long now_sec, now_ms; /* Calculate the time missing for the nearest * timer to fire. */ aeGetTime(&amp;now_sec, &amp;now_ms); tvp = &amp;tv; tvp-&gt;tv_sec = shortest-&gt;when_sec - now_sec; if (shortest-&gt;when_ms &lt; now_ms) { tvp-&gt;tv_usec = ((shortest-&gt;when_ms+1000) - now_ms)*1000; tvp-&gt;tv_sec --; } else { tvp-&gt;tv_usec = (shortest-&gt;when_ms - now_ms)*1000; } if (tvp-&gt;tv_sec &lt; 0) tvp-&gt;tv_sec = 0; if (tvp-&gt;tv_usec &lt; 0) tvp-&gt;tv_usec = 0; } else { /* If we have to check for events but need to return * ASAP because of AE_DONT_WAIT we need to se the timeout * to zero */ if (flags &amp; AE_DONT_WAIT) { tv.tv_sec = tv.tv_usec = 0; tvp = &amp;tv; } else { /* Otherwise we can block */ tvp = NULL; /* wait forever */ } } // tvp为最近的一个timer numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j &lt; numevents; j++) { aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; /* note the fe-&gt;mask &amp; mask &amp; ... code: maybe an already processed * event removed an element that fired and we still didn&apos;t * processed, so we check if the event is still valid. */ if (fe-&gt;mask &amp; mask &amp; AE_READABLE) { rfired = 1; fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask); } if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) { if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask); } processed++; } } /* Check time events */ if (flags &amp; AE_TIME_EVENTS) processed += processTimeEvents(eventLoop); return processed; /* return the number of processed file/time events */ }接着，aeProcessEvents调用 processTimeEvents处理timer事件（此时至少有一个超时），processTimeEvents循环处理已超时的timer。注意，processTimeEvent并不一定会删除超时的timer，代码如下： static int processTimeEvents(aeEventLoop eventLoop) { int processed = 0; aeTimeEvent te; long long maxId; te = eventLoop-&gt;timeEventHead; maxId = eventLoop-&gt;timeEventNextId-1; // 中间注册的id必然比maxid大 while(te) { long now_sec, now_ms; long long id; if (te-&gt;id &gt; maxId) { te = te-&gt;next; continue; } aeGetTime(&amp;now_sec, &amp;now_ms); if (now_sec &gt; te-&gt;when_sec || (now_sec == te-&gt;when_sec &amp;&amp; now_ms &gt;= te-&gt;when_ms)) { int retval; id = te-&gt;id; retval = te-&gt;timeProc(eventLoop, id, te-&gt;clientData); processed++; --- if (retval != AE_NOMORE) { aeAddMillisecondsToNow(retval,&amp;te-&gt;when_sec,&amp;te-&gt;when_ms); } else { aeDeleteTimeEvent(eventLoop, id); } te = eventLoop-&gt;timeEventHead; } else { te = te-&gt;next; } } return processed; }当timer超时时，会调用timer创建时注册的timeProc，根据timerProc的返回值，是删除还是继续修改超时时间。注意，redis的主要循环处理函数serverCron就是靠这种定时机制得以反复运行的，该定时处理函数就一直返回100，这样就使得redis每隔100ms执行一次serverCron函数。 因此，redis的主要循环逻辑为一开始使用beforeSleep处理ready的client，然后处理相关的文件event，最后调用serverCron做一些工作。 下面一节分析下serverCron所做的工作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析8–事件处理（上）]]></title>
      <url>%2F84%2F</url>
      <content type="text"><![CDATA[redis是单进程单线程事件多路循环处理所有的客户端连接，它的运行都是靠事件触发的。 redis 的事件处理支持select、kqueue、epoll机制。其核心的poll函数aeApiPoll其实是一个封装函数，最终是调用 ae_select.c、ae_epoll.c还是ae_kqueue.c中的aeApiPoll（分别实现select、kqueue、epoll机制），取决于如下的宏定义： #ifdef HAVE_EPOLL #include "ae_epoll.c" #else #ifdef HAVE_KQUEUE #include "ae_kqueue.c" #else #include "ae_select.c" #endif #endif ae_select.c、ae_epoll.c、ae_kqueue.c分别对select、kqueue、epoll进制进行了封装，对select、kqueue、epoll的性能比较可在网上找到详细资料。 所有的事件保存在server.el中，el是如下的一个结构： typedef struct aeEventLoop { int maxfd; long long timeEventNextId; aeFileEvent events[AE_SETSIZE]; /* Registered events */ aeFiredEvent fired[AE_SETSIZE]; /* Fired events */ aeTimeEvent *timeEventHead; int stop; void *apidata; /* This is used for polling API specific data */ aeBeforeSleepProc *beforesleep; } aeEventLoop; 其中maxfd是当前事件集合中最大的文件描述符id，timeEventNextId是下一个timer的id，events和fired分别保存了已注册的和已释放的文件event，timeEventHead指向一个timer event的链表，apidata保存了aeApiPoll的私有数据，其实也就是要监控的文件集合，具体实现要看采用哪种机制（select、kqueue、epoll三种机制）。stop用于停止事件循环，仅用于基准测试。beforesleep是在每次事件循环前都要被调用的函数，在main函数中被设置为beforeSleep函数。 对于文件event，其中mask为要检测的事件（读或者写），rfileProc、wfileProc分别为有读写事件时要调用的函数指针，clientData为函数要处理的数据； typedef struct aeFileEvent { int mask; /* one of AE_(READABLE|WRITABLE) */ aeFileProc *rfileProc; aeFileProc *wfileProc; void *clientData; } aeFileEvent; 系统中的timer事件使用一个链表，每个timer有一个唯一的id，该timer在when_sec、 when_ms后被调用，调用函数为timeProc，timeProc处理的主要参数为clientData。在删除该timer时，需要调用 finalizerProc对clientData进行处理。 typedef struct aeTimeEvent { long long id; /* time event identifier. */ long when_sec; /* seconds */ long when_ms; /* milliseconds */ aeTimeProc *timeProc; aeEventFinalizerProc *finalizerProc; void *clientData; struct aeTimeEvent *next; } aeTimeEvent;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析7–内存（下）]]></title>
      <url>%2F82%2F</url>
      <content type="text"><![CDATA[上一节提到的used_memory变量保存了redis当前所使用的内存。其值常用来跟server.vm_max_memory、server.maxmemory进行比较。vm_max_memory表示redis vm启动swap的内存阈值，在超过该值后应启动vm的swap操作；maxmemory表示redis允许分配的最大内存，在超过该值后应进行内存的释放。这些比较主要在rdbLoad、loadAppendOnlyFile、serverCron、processCommand、vmThreadedIOCompletedJob等函数中。值得注意的是，尽管redis会尽量将内存使用量降低到server.maxmemory（甚至server.vm_max_memory）之下，但并不对此保证。 接下来的一节我们分析下rdbLoad、loadAppendOnlyFile、serverCron、processCommand、vmThreadedIOCompletedJob的内存检查策略。 在rdbLoad、loadAppendOnlyFile（分别代表以快照、aof方式进行数据的持久化后db的加载）中会检查vm_max_memory。超过vm_max_memory后，会先调用vmSwapOneObjectBlocking swap值到vm中，若一直到vmSwapOneObjectBlocking返回出错时，内存使用量还是超过vm_max_memory，则置swap_all_values为1，这样后面加载的数据都直接使用vmSwapObjectBlocking被swap到vm中，至于vmSwapOneObjectBlocking、vmSwapObjectBlocking怎么实现的（二者都是阻塞方式），我们在vm章节中再做详细分析。当然，在这两个函数中，对vm_max_memory的比较有所放宽，也就是只有比vm_max_memory多32M字节时，才进行swap的操作。从这里也可以看出，加载db时并不和server.maxmemory进行比较。此时，若超过最大内存限制，redis此时不管，也许加载时直接down掉（超过可用内存），或者等到加载完后运行到后面介绍的释放过程再进行释放。当然，检查vm_max_memory，并调用vmSwapOneObjectBlocking等函数是否起作用，还要看是否开启vm机制（server.vm_enabled）。 static int rdbLoad(char *filename) {int swap_all_values = 0; --- while(1) { robj *key, *val; int force_swapout; --- /* Handle swapping while loading big datasets when VM is on */ /* If we detecter we are hopeless about fitting something in memory * we just swap every new key on disk. Directly... * Note that&apos;s important to check for this condition before resorting * to random sampling, otherwise we may try to swap already * swapped keys. */ if (swap_all_values) { dictEntry *de = dictFind(d,key); /* de may be NULL since the key already expired */ if (de) { key = dictGetEntryKey(de); val = dictGetEntryVal(de); if (val-&gt;refcount != 1) continue; /* Unshare the key if needed */ if (key-&gt;refcount != 1) { robj *newkey = dupStringObject(key); decrRefCount(key); key = dictGetEntryKey(de) = newkey; } if (vmSwapObjectBlocking(key,val) == REDIS_OK) dictGetEntryVal(de) = NULL; } continue; } /* Flush data on disk once 32 MB of additional RAM are used... */ force_swapout = 0; if ((zmalloc_used_memory() - server.vm_max_memory) &gt; 1024*1024*32) force_swapout = 1; /* If we have still some hope of having some value fitting memory * then we try random sampling. */ if (!swap_all_values &amp;&amp; server.vm_enabled &amp;&amp; force_swapout) { while (zmalloc_used_memory() &gt; server.vm_max_memory) { if (vmSwapOneObjectBlocking() == REDIS_ERR) break; } if (zmalloc_used_memory() &gt; server.vm_max_memory) swap_all_values = 1; /* We are already using too much mem */ } } --- } int loadAppendOnlyFile(char *filename) { while(1) { force_swapout = 0; if ((zmalloc_used_memory() - server.vm_max_memory) &gt; 1024*1024*32) force_swapout = 1; if (server.vm_enabled &amp;&amp; force_swapout) { while (zmalloc_used_memory() &gt; server.vm_max_memory) { if (vmSwapOneObjectBlocking() == REDIS_ERR) break; } } } --- }接着看看serverCron中的处理策略。 serverCron是redis中的定时循环函数（100ms循环一次），serverCron是先使用tryFreeOneObjectFromFreelist释放些内存，只有在释放内存还是不够时才启用vm的swap操作，并根据是否启用多线程swap调用vmSwapOneObjectBlocking(阻塞方式) 或者vmSwapOneObjectThreaded（多线程）。另外，如果是多线程方式，则只swap一个object（并不是立即swap），因为在此方式下，会将swap的操作作为一个job，插入到工作线程中，而当该工作线程完成后，会自动调用vmThreadedIOCompletedJob，而在vmThreadedIOCompletedJob中，也有内存大小检查的操作（内存大小超过阈值时，也是调用vmSwapOneObjectThreaded）。可以看到，如果serverCron中的这段代码中的retval == REDIS_ERR的话，则一段时间内无法保证使用的内存在指定的范围之内。（swap的策略在后面VM章节中介绍。） static int serverCron(struct aeEventLoop eventLoop, long long id, void clientData) { /* Swap a few keys on disk if we are over the memory limit and VM * is enbled. Try to free objects from the free list first. */ if (vmCanSwapOut()) { while (server.vm_enabled &amp;&amp; zmalloc_used_memory() &gt; server.vm_max_memory) { int retval; if (tryFreeOneObjectFromFreelist() == REDIS_OK) continue; retval = (server.vm_max_threads == 0) ? vmSwapOneObjectBlocking() : vmSwapOneObjectThreaded(); if (retval == REDIS_ERR &amp;&amp; !(loops % 300) &amp;&amp; zmalloc_used_memory() &gt; (server.vm_max_memory+server.vm_max_memory/10)) { redisLog(REDIS_WARNING,&quot;WARNING: vm-max-memory limit exceeded by more than 10%% but unable to swap more objects out!&quot;); } /* Note that when using threade I/O we free just one object, * because anyway when the I/O thread in charge to swap this * object out will finish, the handler of completed jobs * will try to swap more objects if we are still out of memory. */ if (retval == REDIS_ERR || server.vm_max_threads &gt; 0) break; } } --- }而在处理客户端命令的核心函数processCommand中，在超过内存阈值maxmemory时，会先调用freeMemoryIfNeeded释放一些内存；在释放内存后若还是超过了设置的内存大小，则在客户端命令设置了REDIS_CMD_DENYOOM参数时返回内存出错信息， 否则还是会正常处理。 static int processCommand(redisClient *c) {if (server.maxmemory) freeMemoryIfNeeded(); if (server.maxmemory &amp;&amp; (cmd-&gt;flags &amp; REDIS_CMD_DENYOOM) &amp;&amp; zmalloc_used_memory() &gt; server.maxmemory) { addReplySds(c,sdsnew(&quot;-ERR command not allowed when used memory &gt; &apos;maxmemory&apos;\r\n&quot;)); resetClient(c); return 1; } ---- }我们先看一下 REDIS_CMD_DENYOOM参数。该参数其实并不是由客户端设置的，而是在redis解析客户端请求的命令后，取得该命令所对应的结构体，而该结构体早已设置好了该参数。全局命令表cmdTable中保存了所有命令的相关命令字机器处理函数和相关参数。可以看到，对于set等可能会增加内存使用的命令，一律设置了REDIS_CMD_DENYOOM参数，而get等则没有。 static struct redisCommand cmdTable[] = { {“get”,getCommand,2,REDIS_CMD_INLINE,NULL,1,1,1}, {“set”,setCommand,3,REDIS_CMD_BULK|REDIS_CMD_DENYOOM,NULL,0,0,0}, {“setnx”,setnxCommand,3,REDIS_CMD_BULK|REDIS_CMD_DENYOOM,NULL,0,0,0}, {&quot;setex&quot;,setexCommand,4,REDIS_CMD_BULK|REDIS_CMD_DENYOOM,NULL,0,0,0}, --- }再来看看 freeMemoryIfNeeded是怎么释放内存的。释放时先调用tryFreeOneObjectFromFreelist释放内存，在内存仍不够时，会试图释放带 expire标记的key。对于每个db中的expire dict，每次会随机选择3个key，并删除会最先expire的key（此时就很可能丢失带expire标记的数据了）。 static void freeMemoryIfNeeded(void) { while (server.maxmemory &amp;&amp; zmalloc_used_memory() &gt; server.maxmemory) { int j, k, freed = 0; if (tryFreeOneObjectFromFreelist() == REDIS_OK) continue; for (j = 0; j &lt; server.dbnum; j++) { int minttl = -1; robj *minkey = NULL; struct dictEntry *de; if (dictSize(server.db[j].expires)) { freed = 1; /* From a sample of three keys drop the one nearest to * the natural expire */ for (k = 0; k &lt; 3; k++) { time_t t; de = dictGetRandomKey(server.db[j].expires); t = (time_t) dictGetEntryVal(de); if (minttl == -1 || t &lt; minttl) { minkey = dictGetEntryKey(de); minttl = t; } } deleteKey(server.db+j,minkey); server.stat_expiredkeys++; } } if (!freed) return; /* nothing to free... */ } }最后我们来看看tryFreeOneObjectFromFreelist函数。redis会将系统中的无效list node（即该node已解除对其内部value的引用）放到server.objfreelist链表中，平时如果需要list node，可直接从该list中获得一个，但此刻因为内存不够，该释放它们了。 static int tryFreeOneObjectFromFreelist(void) { robj *o; if (server.vm_enabled) pthread_mutex_lock(&amp;server.obj_freelist_mutex); if (listLength(server.objfreelist)) { listNode *head = listFirst(server.objfreelist); o = listNodeValue(head); listDelNode(server.objfreelist,head); if (server.vm_enabled) pthread_mutex_unlock(&amp;server.obj_freelist_mutex); zfree(o); return REDIS_OK; } else { if (server.vm_enabled) pthread_mutex_unlock(&amp;server.obj_freelist_mutex); return REDIS_ERR; } }前面的过程搞清后，就可以回答一个问题了。redis不开启VM时，内存超过maxmemory设置后，是怎么处理的？ 不开启VM，redis并不保证内存使用一定低于maxmemory，只是会尽可能释放。 先看client，对于有些会增加内存使用的命令，比如set，此时会返回出错信息。 释放策略是：因为redis会保存先前已不再使用的object，也就是一个object链表，平时这个链表的作用使得redis可以直接从上面取得一个object，不需要使用zmalloc分配。 当内存超过阈值时，这个链表就会首先被释放了。 若还是超过内存阈值，redis对于每个db，会随机选择3个带expire标记的key， 并释放最先expire的key及其val。 但如果此后还是超过内存阈值（把所有带expire标记的都释放后），我想redis是没办法了。 尽管如此，redis使用的内存&gt;设置的maxmemory，只会出现在一开始加载的数据就超过maxmemroy。这样的话，client调用set等命令会一直返回出错信息。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析6–内存（中）]]></title>
      <url>%2F80%2F</url>
      <content type="text"><![CDATA[在上一节介绍zmalloc/zfree函数时，我们看到redis会调用increment_used_memory/decrement_used_memory，这两个宏其实就是对static变量used_memory进行指定大小的增加/减少，该变量保存了redis所使用的内存大小： // sizeof(long)字节对齐 #define increment_used_memory(__n) do { \ size_t _n = (__n); \ if (_n&(sizeof(long)-1)) _n += sizeof(long)-(_n&(sizeof(long)-1)); \ if (zmalloc_thread_safe) { \ pthread_mutex_lock(&used_memory_mutex); \ used_memory += _n; \ pthread_mutex_unlock(&used_memory_mutex); \ } else { \ used_memory += _n; \ } \ } while(0) 分配内存的线程安全性是由底层系统的malloc系列函数来保证的，而used_memory变量的线程安全性取决于线程锁zmalloc_thread_safe的值。由于redis处理客户端连接是单进程单线程事件多路循环的，那么就有一个疑问，redis在什么情况下，需要多线程保护了？在后续的介绍中，我们会看到，redis的虚拟内存（VM）可能启用多线程。 我们来看看zmalloc_thread_safe 的相关处理。zmalloc_thread_safe 初始值为0，仅在zmalloc_enable_thread_safeness函数中改为1，而zmalloc_enable_thread_safeness仅在全局变量server.vm_max_threads!=0（也即vm启用多线程时），才在vmInit中调用。 void zmalloc_enable_thread_safeness(void) { zmalloc_thread_safe = 1; } static void vmInit(void) { --- if (server.vm_max_threads != 0) zmalloc_enable_thread_safeness(); /* we need thread safe zmalloc() */ --- } 内存相关的函数，除了zmalloc、zremalloc、zfree等直接操作内存外，常用的就是使用zmalloc_used_memory返回已分配的内存大小了，redis对内存的监控仅限于此。 size_t zmalloc_used_memory(void) { size_t um; if (zmalloc_thread_safe) pthread_mutex_lock(&used_memory_mutex); um = used_memory; if (zmalloc_thread_safe) pthread_mutex_unlock(&used_memory_mutex); return um; } redis在某些位置输出log时需要调用该函数输出已分配的内存大小外，比如serverCron中的调用： redisLog(REDIS_VERBOSE,"%d clients connected (%d slaves), %zu bytes in use", listLength(server.clients)-listLength(server.slaves), listLength(server.slaves), zmalloc_used_memory());]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析5–内存（上）]]></title>
      <url>%2F78%2F</url>
      <content type="text"><![CDATA[这一节介绍redis中内存分配相关的api，下一节介绍redis内存使用过程中的一些细节。 redis中所有的内存分配都由自己接管。主要由zmalloc.c和zmalloc.h中的zmalloc、zremalloc、zfree实现。 void zmalloc(size_t size) { void ptr = malloc(size+PREFIX_SIZE); if (!ptr) zmalloc_oom(size); #ifdef HAVE_MALLOC_SIZE // apple系统，额外说明 increment_used_memory(redis_malloc_size(ptr)); return ptr; #else ((size_t)ptr) = size; increment_used_memory(size+PREFIX_SIZE); return (char*)ptr+PREFIX_SIZE; #endif}可以看到，系统中除了分配请求大小的内存外，还在该内存块头部保存了该内存块的大小，这样，释放的时候可以通过该大小找到该内存块的起始位置： void zfree(void *ptr) {realptr = (char*)ptr-PREFIX_SIZE; oldsize = *((size_t*)realptr); decrement_used_memory(oldsize+PREFIX_SIZE); free(realptr); --- }另外对于 apple系统，可以用malloc_size（redis_malloc_size是对它的封装）取得指针所指向的内存块大小，因此就不需要手动保存大小了。（笔者没有用过apple系统，仅看了下apple的相关文档：http://developer.apple.com /library/mac/#documentation/Darwin/Reference/ManPages/man3 /malloc_size.3.html）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析4–字典（下）]]></title>
      <url>%2F75%2F</url>
      <content type="text"><![CDATA[hash表的两个有意思的处理过程为自动调整hash表的大小，在需要时实现增量rehash，其他如查找，删除等不解释。 我们先看看resize的处理逻辑。 dict.c中有个变量dict_can_resize控制着是否允许自动调整hash表的大小，该变量的改变主要靠如下两个函数： void dictEnableResize(void) { dict_can_resize = 1; } void dictDisableResize(void) { dict_can_resize = 0; } 总的说来，在系统运行有后台进程时，不允许自动自动调整大小，这是为了为了使得类linux系统的copy-on-write有更好的性能（没有调整大小， 就没有rehash，这样父进程的db没有改变，子进程就不需要真的copy）。在后台子进程退出后，又会允许resize。这里说到的后台子进程主要跟redis的持久化机制有关，在后面的持久化之快照和持久化之aof中会详细介绍其过程。 接下来我们看看自动调整大小的算法。 主要是由dictExpand完成。该函数在将size转成最接近2的幂的数后（比如size=60，realsize会等于64，_dictNextPower复杂度为O(log(Size)) ），将相关参数保存在dict中的ht[1]中，并将其rehashidx置为0（置为-1，表示没有rehash进行），这样就可以开始rehash了。 int dictExpand(dict *d, unsigned long size) { dictht n; /* the new hashtable */ unsigned long realsize = _dictNextPower(size); -- n.size = realsize; n.sizemask = realsize-1; n.table = _dictAlloc(realsize*sizeof(dictEntry*)); n.used = 0; /* Initialize all the pointers to NULL */ memset(n.table, 0, realsize*sizeof(dictEntry*)); -- /* Prepare a second hash table for incremental rehashing */ d->ht[1] = n; d->rehashidx = 0; return DICT_OK; } 而dictExpand主要被rdbLoadObject、_dictExpandIfNeeded、dictResize调用。 rdbLoadObject表示文件中加载一个object，当加载的是一个set对象时，一开始就将其扩展为set中元素的个数，可以大大减少随后rehash的次数（set用ht实现，而ht的初始值DICT_HT_INITIAL_SIZE = 4） if (type == REDIS_SET && listlen > DICT_HT_INITIAL_SIZE) dictExpand(o->ptr,listlen); _dictExpandIfNeeded最终会被dictAdd调用，也就是说在每次向ht中增加一个元素时，都会判断。_dictExpandIfNeeded的判断策略如下： /* If we reached the 1:1 ratio, and we are allowed to resize the hash * table (global setting) or we should avoid it but the ratio between * elements/buckets is over the "safe" threshold, we resize doubling * the number of buckets. */ if (d->ht[0].used >= d->ht[0].size && (dict_can_resize || d->ht[0].used/d->ht[0].size > dict_force_resize_ratio)) { return dictExpand(d, ((d->ht[0].size > d->ht[0].used) ? d->ht[0].size : d->ht[0].used)*2); } dict_can_resize就是一开始提到的用于避免后台正在持久化时rehash的全局变量。除了这个变量，当ht桶的元素的平均个数多于 dict_force_resize_ratio=5时，也会强制进行rehash。扩展策略是现有值的2倍。 如果说_dictExpandIfNeeded会将ht增大的话，那么可以认为dictResize会将ht减小。其目标是将桶的个数减少到接近ht中所含元素的个数。 int dictResize(dict *d) { int minimal; if (!dict_can_resize || dictIsRehashing(d)) return DICT_ERR; minimal = d->ht[0].used; if (minimal < DICT_HT_INITIAL_SIZE) minimal = DICT_HT_INITIAL_SIZE; return dictExpand(d, minimal); } 跟ht的增大过程是在向dict中增加元素时对应，ht的减小过程主要是在删除元素时进行。而redis中的set和zset都是用ht实现的，因此dictResize主要被sremCommand和spopCommand（当dict用于set时）、zremCommand和 zremrangebyscoreCommand和zremrangebyrankCommand（当dict是zset时）、hashDelete调用。除此之外，serverCron（每隔100ms执行一次）会调用tryResizeHashTables，后者会进一步调用dictResize来判断整个redis哪些db需要调整（每个db又是一个大的ht）。 static int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) { --- if (server.bgsavechildpid == -1 && server.bgrewritechildpid == -1) { if (!(loops % 10)) tryResizeHashTables(); if (server.activerehashing) incrementallyRehash(); } --- } static void tryResizeHashTables(void) { int j; for (j = 0; j < server.dbnum; j++) { if (htNeedsResize(server.db[j].dict)) dictResize(server.db[j].dict); if (htNeedsResize(server.db[j].expires)) dictResize(server.db[j].expires); } } 而所有dictResize调用是否进行，要看htNeedsResize的返回值。htNeedsResize函数最终反映了redis中减小ht的策略。从其代码可以看出，当填充率不到10%时（ REDIS_HT_MINFILL定义为10），会将ht的桶的个数调整到接近ht中元素的个数（参看前面dictResize）。 static int htNeedsResize(dict *dict) { long long size, used; size = dictSlots(dict); used = dictSize(dict); return (size && used && size > DICT_HT_INITIAL_SIZE && (used*100/size < REDIS_HT_MINFILL)); } 接下来看下rehash，主要在dictRehash中完成。先看下什么时候进行rehash。 在如上的serverCron中（每100ms执行一次），当没有后台子进程时，会调用incrementallyRehash，最终调用dictRehash。incrementallyRehash会对每个db，rehash大概1ms的时间，这1ms又进一步划分成多步，每步rehash 100个桶。 static void incrementallyRehash(void) { int j; for (j = 0; j < server.dbnum; j++) { if (dictIsRehashing(server.db[j].dict)) { dictRehashMilliseconds(server.db[j].dict,1); break; /* already used our millisecond for this loop... */ } } } int dictRehashMilliseconds(dict *d, int ms) { long long start = timeInMilliseconds(); int rehashes = 0; while(dictRehash(d,100)) { rehashes += 100; if (timeInMilliseconds()-start > ms) break; } return rehashes; } 另外在_dictRehashStep，也会调用dictRehash，而_dictRehashStep每次会rehash一个桶从ht[0]到 ht1，但由于_dictRehashStep是被dictGetRandomKey、dictFind、 dictGenericDelete、dictAdd调用的，因此在每次dict增删查改时都会被调用，这无疑就加快了rehash过程。 我们再来看看rehash过程。dictRehash每次增量rehash n个桶（dictRehash的参数n表示桶的个数），由于在自动调整大小时已设置好了ht[1]的大小，因此rehash的主要过程就是遍历ht[0]的各个桶，取得key，然后将该key按ht[1]的桶的大小重新rehash，并将相应的dictEntry从ht[0]移动到ht[1]。在rehash完后，会将ht[0]指向ht[1]，然后将ht[1]清空。 int dictRehash(dict *d, int n) { if (!dictIsRehashing(d)) return 0; while(n--) { dictEntry *de, *nextde; /* Check if we already rehashed the whole table... */ if (d->ht[0].used == 0) { _dictFree(d->ht[0].table); d->ht[0] = d->ht[1]; _dictReset(&d->ht[1]); d->rehashidx = -1; return 0; } /* Note that rehashidx can't overflow as we are sure there are more * elements because ht[0].used != 0 */ while(d->ht[0].table[d->rehashidx] == NULL) d->rehashidx++; de = d->ht[0].table[d->rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while(de) { unsigned int h; nextde = de->next; /* Get the index in the new hash table */ h = dictHashKey(d, de->key) & d->ht[1].sizemask; de->next = d->ht[1].table[h]; d->ht[1].table[h] = de; d->ht[0].used--; d->ht[1].used++; de = nextde; } d->ht[0].table[d->rehashidx] = NULL; d->rehashidx++; } return 1; } 前面详细了解了rehash的过程，现在可以看看redis为什么使用两个hash表了（ht[0]与ht[1]）。 通常，设计hash表时，只有一个ht。在进行rehash时，需要先申请更大内存（比如tmp指向该内存）后，然后一次性把ht中所有的数据rehash到tmp中，然后再让ht执行tmp。也就是说，rehash的粒度是整个ht。 采用两个表后，rehash时的粒度最小可降到一个桶（而每个桶中元素的平均个数为前面提到的dict_force_resize_ratio=5）。很显然，rehash被分散处理了。另外，由于rehash只是移动dictEntry从ht[0]到ht[1]，整个使用的内存并没有增加。 最后，简单介绍下在rehash进行时，元素插入、查找、删除的过程（没有rehash时，所有的增删查改都指向ht[0]）。 插入会直接插入到ht[1]中。查找、删除会查找两个表ht[0]、ht[1]。 redis中用到的整数hash、字符串hash算法如下，做个备份： /* Thomas Wang's 32 bit Mix Function */ unsigned int dictIntHashFunction(unsigned int key) { key += ~(key > 10); key += (key > 6); key += ~(key > 16); return key; } /* Generic hash function (a popular one from Bernstein). * I tested a few and this was the best. */ unsigned int dictGenHashFunction(const unsigned char *buf, int len) { unsigned int hash = 5381; while (len--) hash = ((hash]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析3–字典（上）]]></title>
      <url>%2F72%2F</url>
      <content type="text"><![CDATA[字典实现中主要用到如下5个结构体： typedef struct dictEntry { void key; void val; struct dictEntry *next;} dictEntry; typedef struct dictType { unsigned int (hashFunction)(const void key); void (keyDup)(void privdata, const void key); void (valDup)(void privdata, const void obj); int (keyCompare)(void privdata, const void key1, const void key2); void (keyDestructor)(void privdata, void key); void (valDestructor)(void privdata, void obj);} dictType; typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;} dictht; typedef struct dict { dictType type; void privdata; dictht ht[2]; int rehashidx; / rehashing not in progress if rehashidx == -1 / int iterators; / number of iterators currently running /} dict; typedef struct dictIterator { dict d; int table; int index; dictEntry entry, *nextEntry;} dictIterator;dict代表整个字典，内部有两个dictht, 以实现增量hash（将ht[0]中的值rehash到ht[1]中）， rehashidx是下一个需要rehash的项在ht[0]中的索引，不需要rehash时置为-1， iterators记录当前dict中的迭代器数，主要是为了避免在有迭代器时rehash，在有迭代器时rehash可能会造成值的丢失或重复， type的类型dictType是一组函数指针，表示该怎样哈希、复制、释放key，该怎样复制、释放value； dictht中的table是一个数组+指针形式的hash表，size表hash数组(桶)的大小，used表示hash表的元素个数，这两个值与rehash、resize过程密切相关。sizemask等于size-1，这是为了方便将hash值映射到数组中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析2–动态字符串]]></title>
      <url>%2F69%2F</url>
      <content type="text"><![CDATA[redis的字符串号称是二进制安全的，其内部实现其实就是个head+ char*。 typedef char *sds;struct sdshdr { long len; long free; char buf[];};len 表示buf字符数组实际使用的空间大小，free表示buf剩余空间大小，buf所分配的空间大小等于len+free。尽管一开始buf的大小等于 len（当然可以大于），但随着字符串连接、拷贝（C中的字符串函数），buf分配的空间很可能会比len大，此时redis并不会释放多出的内存。 # redis源代码分析2–动态字符串]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis源代码分析1–链表]]></title>
      <url>%2F65%2F</url>
      <content type="text"><![CDATA[先简单介绍下redis中用到的链表，是在文件adlist.c和adlist.h中实现的。 实现中主要用到listNode、list、listIter三个结构，listNode代表链表中的每个节点，list指向整个链表，listIter是为了迭代访问整个list，内部其实就是个listNode指针。 typedef struct listNode { struct listNode prev; struct listNode next; void *value;} listNode; typedef struct listIter { listNode *next; int direction;} listIter; typedef struct list { listNode head; listNode tail; void (dup)(void ptr); void(free)(void ptr); int(match)(void ptr, void key); unsigned int len;} list;listNode提供的prev、next指针将整个list链接成一个双向链表，保存的值是void *类型，对值的复制、释放、匹配操作是用在list中注册的三个函数指针dup、free、match完成的。 在list结构中，除提供对listNode中的值进行操作的三个函数指针外，还提供了head、tail指针，以指向list的头部和尾部。另外list的len保存了整个list的长度，方便对list是否为空的判断(纯属多余吧)。 listIter是为了遍历list，可以从头部、尾部开始遍历。用法可用如下伪代码表示： iter=listGetIterotr(list, );while ((node=listNext(iter)) !=NULL){ DoSomethingWith(listNodeValue(node));}另外注意：在提供的增加、删除节点的api（listAddNodeHead、listAddNodeTail、listDelNode）中，并没有分配、释放节点 内部的value所用内存，需要调用者自己分配或者释放value所占的内存（除了分配和释放节点本身的内存外）。 乍一看没有提供修改某个节点的值的方法，但是由于listSearchKey、listIndex等方法返回了节点指针，故可以直接修改节点的value。&nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis相关资料]]></title>
      <url>%2F14%2F</url>
      <content type="text"><![CDATA[1 官方文档：redis源码doc目录中的所有文档官方网站：http://redis.io/命令集合：http://redis.io/commands2 国外分析文档：redis-from-the-ground-up:http://blog.mjrusso.com/2010/10/17/redis-from-the-ground-up.htmlRedis: under the hoodhttp://pauladamsmith.com/articles/redis-under-the-hood.htmlredis cookbook：http://www.rediscookbook.org/redis tutorial：http://simonwillison.net/static/2010/redis-tutorial/Redis Virtual Memory: the story and the code:http://antirez.com/post/redis-virtual-memory-story.htmlredis性能测试：http://jaksprats.wordpress.com/2010/09/22/12/3 一些中文文档：概述性的介绍了redis的特点和使用：http://www.cnblogs.com/xhan/archive/2011/02/08/1949867.html读Redis有感：http://blog.csdn.net/gpcuster/archive/2010/10/21/5956555.aspxRedis误区http://timyang.net/data/redis-misunderstanding/Redis内存陷阱 http://www.javaeye.com/topic/8082934 友情提示我是从去年11月份开始学习这个代码的，那时的稳定版本还是2.0.4的，没想到，才不到半年时间，就升到了2.2.4，代码结构也有了很大的变化，原来2.0.4一个redis.c文件有11000行代码，像aof、rdb、vm 等功能都集成到这一个文件里，当时看起来真费劲，不过之后作者把这些功能分开了，放到不同的文件里了。接下来的分析主要基于2.0.4，未来有时间会更新 到最新版本。备注：文章最初发表在http://hi.baidu.com/petermao/，不过后来我将一些文章中的代码着色后会使得文章超过baidu空间中的字数限制，这就迫使我寻求其他方法。最后在[redice](http://redicecn.com)的帮助下，利用他的主机搭建了个人blog，在此谢过redice。所有文章仅在个人blog进行后续更新，baidu空间仅作为一个备份。]]></content>
    </entry>

    
  
  
</search>
